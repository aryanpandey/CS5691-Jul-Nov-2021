# -*- coding: utf-8 -*-
"""CE19B110_NA19B030.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ak-4dmEyN48s3oP39uZq262Ih_Ju9yVx

# Code for Question 2E
"""

import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader, random_split
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt

torch.manual_seed(42)

class direhorse(Dataset):
    def __init__(self, file, normalize = True):
        super(direhorse, self).__init__()   # Calling parent class init function
        self.file = file                    
        self._init_dataset()                # Function to load the dataset and split into train and labels
        if normalize:                       # Normalise dataset function
            self._normalize_dataset()
            
    def _init_dataset(self):
        dataset = pd.read_csv(self.file) 
        self.train = torch.tensor(dataset['Ages'].values.astype('float32'))       
        self.labels = torch.tensor(dataset['Weights'].values.astype('float32'))
        
    def _normalize_dataset(self):
        self.train = F.normalize(self.train, dim = 0)
        
    def __getitem__(self, index):                       # Function to get the data point at the ith index
        return (self.train[index], self.labels[index])   
    
    def __len__(self):                                  # Function to get the length of the data
        return len(self.train)

class network(nn.Module):
    def __init__(self, hidden_layers = 1, units_per_layer = [8]):
        super(network, self).__init__()
        
        assert len(units_per_layer) == hidden_layers
        
        layers = []
        
        for i in range(hidden_layers):
            if i == 0:
                layers.append(nn.Linear(1, units_per_layer[i]))   # First layer definition
            else:
                layers.append(nn.Linear(units_per_layer[i-1], units_per_layer[i]))
            layers.append(nn.BatchNorm1d(units_per_layer[i]))     #Every layer is followed by a batchnorm and a relu activation
            layers.append(nn.ReLU())

        layers.append(nn.Linear(units_per_layer[-1], 1))          # Last Layer Definition
        
        self.layers = nn.Sequential(*layers)
        
    def forward(self, x):
        #print(x.shape)
        return self.layers(x)

def train(model, num_epochs, trainloader, valloader, loss_fn, optimiser, device):
    total_step = len(trainloader)
    train_loss_log = []
    val_loss_log = []
    
    for epoch in range(num_epochs):
        running_loss = 0
        model.train()
        print("------------------------------EPOCH: {} ------------------------".format(epoch+1))
        for i, (ages, weights) in enumerate(trainloader):
            ages = ages.to(device)
            weights = weights.to(device)
            
            ages = torch.unsqueeze(ages, dim = 1) 
            
            outputs = model(ages)
            outputs = torch.squeeze(outputs)
            
            train_loss = loss_fn(outputs, weights)
            running_loss += train_loss.item()
            
            optimiser.zero_grad()  # initialising gradients
            train_loss.backward()  # Compute all gradients
            optimiser.step()       # Take a single step of backprop
            
            if (i+1)%2 == 0:
                print('Epoch [{}/{}], Step [{}/{}], train_loss: {:.4f}' 
                   .format(epoch+1, num_epochs, i+1, total_step, train_loss.item()))
        
        train_loss_log.append(running_loss/total_step) 
        
        with torch.no_grad():
            model.eval()
            val_loss = 0
            
            for _, (ages, weights) in enumerate(valloader):
                ages = ages.to(device)
                weights = weights.to(device)
                ages = torch.unsqueeze(ages, dim = 1)
            
                outputs = model(ages)
                outputs = torch.squeeze(outputs)
                
                val_loss += loss_fn(outputs, weights).item()
            print("Val Loss: ", val_loss/len(valloader))
            if epoch!=0:
                if (val_loss/len(valloader)) < min(val_loss_log):
                    torch.save(model.state_dict(), 'direhorse_model.pth')

            val_loss_log.append(val_loss/len(valloader))
            
    return train_loss_log, val_loss_log

def plot_logs(train_logs, val_logs):
    plt.figure(figsize = (7,7))
    plt.plot(train_logs, label = 'Train Loss')
    plt.plot(val_logs, label = 'Validation Loss')
    plt.legend()
    plt.show()

epochs = 2000
hidden_layers = 2
units_per_layer = [8, 8]

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#Create dataloader
data = direhorse(file='direhorse.csv', normalize = False)

train_size = int(0.85*len(data))
val_size = len(data) - train_size

#Create validation split
trainset, valset = random_split(data, [train_size, val_size], torch.Generator().manual_seed(42))

trainloader = DataLoader(trainset, batch_size = 8)
valloader = DataLoader(valset, batch_size = 2)

# Defining model, loss and optimiser
model = network(hidden_layers=hidden_layers, units_per_layer=units_per_layer).to(device)
loss = nn.MSELoss().to(device)
optimiser = optim.Adam(model.parameters(), lr = 0.001)

train_log, val_log = train(model, epochs, trainloader, valloader, loss, optimiser, device)

plot_logs(train_log, val_log)

## Comparison with function
mse = 0
for _, (ages, weights) in enumerate(valloader):
    y = 233.846*(1 - torch.exp(-1*0.006042*ages)) + 0.0001
    mse += nn.functional.mse_loss(y, weights).item()/len(valloader)

print(mse)

