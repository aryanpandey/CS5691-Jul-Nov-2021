{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "CE19B110_NA19B030.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMOuT4kLdlfx"
      },
      "source": [
        "# Code for Question 2E"
      ],
      "id": "UMOuT4kLdlfx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-24T15:38:23.353017Z",
          "start_time": "2021-11-24T15:38:14.304459Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bizarre-attempt",
        "outputId": "413baef0-a5ff-4980-b90d-fd97fff12bca"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(42)"
      ],
      "id": "bizarre-attempt",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f24297cb2f0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-24T15:38:23.393508Z",
          "start_time": "2021-11-24T15:38:23.355871Z"
        },
        "id": "suffering-hospital"
      },
      "source": [
        "class direhorse(Dataset):\n",
        "    def __init__(self, file, normalize = True):\n",
        "        super(direhorse, self).__init__()   # Calling parent class init function\n",
        "        self.file = file                    \n",
        "        self._init_dataset()                # Function to load the dataset and split into train and labels\n",
        "        if normalize:                       # Normalise dataset function\n",
        "            self._normalize_dataset()\n",
        "            \n",
        "    def _init_dataset(self):\n",
        "        dataset = pd.read_csv(self.file) \n",
        "        self.train = torch.tensor(dataset['Ages'].values.astype('float32'))       \n",
        "        self.labels = torch.tensor(dataset['Weights'].values.astype('float32'))\n",
        "        \n",
        "    def _normalize_dataset(self):\n",
        "        self.train = F.normalize(self.train, dim = 0)\n",
        "        \n",
        "    def __getitem__(self, index):                       # Function to get the data point at the ith index\n",
        "        return (self.train[index], self.labels[index])   \n",
        "    \n",
        "    def __len__(self):                                  # Function to get the length of the data\n",
        "        return len(self.train)"
      ],
      "id": "suffering-hospital",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-24T15:38:23.494168Z",
          "start_time": "2021-11-24T15:38:23.395528Z"
        },
        "id": "minimal-knife"
      },
      "source": [
        "class network(nn.Module):\n",
        "    def __init__(self, hidden_layers = 1, units_per_layer = [8]):\n",
        "        super(network, self).__init__()\n",
        "        \n",
        "        assert len(units_per_layer) == hidden_layers\n",
        "        \n",
        "        layers = []\n",
        "        \n",
        "        for i in range(hidden_layers):\n",
        "            if i == 0:\n",
        "                layers.append(nn.Linear(1, units_per_layer[i]))   # First layer definition\n",
        "            else:\n",
        "                layers.append(nn.Linear(units_per_layer[i-1], units_per_layer[i]))\n",
        "            layers.append(nn.BatchNorm1d(units_per_layer[i]))     #Every layer is followed by a batchnorm and a relu activation\n",
        "            layers.append(nn.ReLU())\n",
        "\n",
        "        layers.append(nn.Linear(units_per_layer[-1], 1))          # Last Layer Definition\n",
        "        \n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #print(x.shape)\n",
        "        return self.layers(x)"
      ],
      "id": "minimal-knife",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-24T15:38:23.566342Z",
          "start_time": "2021-11-24T15:38:23.501330Z"
        },
        "id": "suburban-permit"
      },
      "source": [
        "def train(model, num_epochs, trainloader, valloader, loss_fn, optimiser, device):\n",
        "    total_step = len(trainloader)\n",
        "    train_loss_log = []\n",
        "    val_loss_log = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0\n",
        "        model.train()\n",
        "        print(\"------------------------------EPOCH: {} ------------------------\".format(epoch+1))\n",
        "        for i, (ages, weights) in enumerate(trainloader):\n",
        "            ages = ages.to(device)\n",
        "            weights = weights.to(device)\n",
        "            \n",
        "            ages = torch.unsqueeze(ages, dim = 1) \n",
        "            \n",
        "            outputs = model(ages)\n",
        "            outputs = torch.squeeze(outputs)\n",
        "            \n",
        "            train_loss = loss_fn(outputs, weights)\n",
        "            running_loss += train_loss.item()\n",
        "            \n",
        "            optimiser.zero_grad()  # initialising gradients\n",
        "            train_loss.backward()  # Compute all gradients\n",
        "            optimiser.step()       # Take a single step of backprop\n",
        "            \n",
        "            if (i+1)%2 == 0:\n",
        "                print('Epoch [{}/{}], Step [{}/{}], train_loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, train_loss.item()))\n",
        "        \n",
        "        train_loss_log.append(running_loss/total_step) \n",
        "        \n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            \n",
        "            for _, (ages, weights) in enumerate(valloader):\n",
        "                ages = ages.to(device)\n",
        "                weights = weights.to(device)\n",
        "                ages = torch.unsqueeze(ages, dim = 1)\n",
        "            \n",
        "                outputs = model(ages)\n",
        "                outputs = torch.squeeze(outputs)\n",
        "                \n",
        "                val_loss += loss_fn(outputs, weights).item()\n",
        "            print(\"Val Loss: \", val_loss/len(valloader))\n",
        "            if epoch!=0:\n",
        "                if (val_loss/len(valloader)) < min(val_loss_log):\n",
        "                    torch.save(model.state_dict(), 'direhorse_model.pth')\n",
        "\n",
        "            val_loss_log.append(val_loss/len(valloader))\n",
        "            \n",
        "    return train_loss_log, val_loss_log"
      ],
      "id": "suburban-permit",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-24T15:41:27.928682Z",
          "start_time": "2021-11-24T15:41:27.925963Z"
        },
        "id": "eligible-darwin"
      },
      "source": [
        "def plot_logs(train_logs, val_logs):\n",
        "    plt.figure(figsize = (7,7))\n",
        "    plt.plot(train_logs, label = 'Train Loss')\n",
        "    plt.plot(val_logs, label = 'Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "id": "eligible-darwin",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-24T15:41:59.315886Z",
          "start_time": "2021-11-24T15:41:59.292573Z"
        },
        "id": "tired-entity"
      },
      "source": [
        "epochs = 2000\n",
        "hidden_layers = 2\n",
        "units_per_layer = [8, 8]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#Create dataloader\n",
        "data = direhorse(file='direhorse.csv', normalize = False)\n",
        "\n",
        "train_size = int(0.85*len(data))\n",
        "val_size = len(data) - train_size\n",
        "\n",
        "#Create validation split\n",
        "trainset, valset = random_split(data, [train_size, val_size], torch.Generator().manual_seed(42))\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size = 8)\n",
        "valloader = DataLoader(valset, batch_size = 2)\n",
        "\n",
        "# Defining model, loss and optimiser\n",
        "model = network(hidden_layers=hidden_layers, units_per_layer=units_per_layer).to(device)\n",
        "loss = nn.MSELoss().to(device)\n",
        "optimiser = optim.Adam(model.parameters(), lr = 0.001)\n"
      ],
      "id": "tired-entity",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-24T15:42:36.699381Z",
          "start_time": "2021-11-24T15:42:00.068230Z"
        },
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dense-bennett",
        "outputId": "b5447845-0dab-492d-e9af-5aacff8171cc"
      },
      "source": [
        "train_log, val_log = train(model, epochs, trainloader, valloader, loss, optimiser, device)"
      ],
      "id": "dense-bennett",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------EPOCH: 1 ------------------------\n",
            "Epoch [1/2000], Step [2/8], train_loss: 26581.3418\n",
            "Epoch [1/2000], Step [4/8], train_loss: 15491.4434\n",
            "Epoch [1/2000], Step [6/8], train_loss: 23213.9805\n",
            "Epoch [1/2000], Step [8/8], train_loss: 18019.3633\n",
            "Val Loss:  31339.741129557293\n",
            "------------------------------EPOCH: 2 ------------------------\n",
            "Epoch [2/2000], Step [2/8], train_loss: 26562.3750\n",
            "Epoch [2/2000], Step [4/8], train_loss: 15477.5479\n",
            "Epoch [2/2000], Step [6/8], train_loss: 23188.1406\n",
            "Epoch [2/2000], Step [8/8], train_loss: 18003.7305\n",
            "Val Loss:  31311.516031901043\n",
            "------------------------------EPOCH: 3 ------------------------\n",
            "Epoch [3/2000], Step [2/8], train_loss: 26545.6230\n",
            "Epoch [3/2000], Step [4/8], train_loss: 15463.6621\n",
            "Epoch [3/2000], Step [6/8], train_loss: 23158.3730\n",
            "Epoch [3/2000], Step [8/8], train_loss: 17993.4980\n",
            "Val Loss:  31290.935546875\n",
            "------------------------------EPOCH: 4 ------------------------\n",
            "Epoch [4/2000], Step [2/8], train_loss: 26529.4766\n",
            "Epoch [4/2000], Step [4/8], train_loss: 15448.8262\n",
            "Epoch [4/2000], Step [6/8], train_loss: 23130.2207\n",
            "Epoch [4/2000], Step [8/8], train_loss: 17981.1719\n",
            "Val Loss:  31266.880940755207\n",
            "------------------------------EPOCH: 5 ------------------------\n",
            "Epoch [5/2000], Step [2/8], train_loss: 26511.6250\n",
            "Epoch [5/2000], Step [4/8], train_loss: 15433.5400\n",
            "Epoch [5/2000], Step [6/8], train_loss: 23112.4688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch [1167/2000], Step [8/8], train_loss: 280.9968\n",
            "Val Loss:  218.7430183092753\n",
            "------------------------------EPOCH: 1168 ------------------------\n",
            "Epoch [1168/2000], Step [2/8], train_loss: 408.5361\n",
            "Epoch [1168/2000], Step [4/8], train_loss: 604.6132\n",
            "Epoch [1168/2000], Step [6/8], train_loss: 186.2787\n",
            "Epoch [1168/2000], Step [8/8], train_loss: 280.7744\n",
            "Val Loss:  375.63532225290936\n",
            "------------------------------EPOCH: 1169 ------------------------\n",
            "Epoch [1169/2000], Step [2/8], train_loss: 404.3271\n",
            "Epoch [1169/2000], Step [4/8], train_loss: 615.2329\n",
            "Epoch [1169/2000], Step [6/8], train_loss: 185.7355\n",
            "Epoch [1169/2000], Step [8/8], train_loss: 281.0155\n",
            "Val Loss:  257.15618483225506\n",
            "------------------------------EPOCH: 1170 ------------------------\n",
            "Epoch [1170/2000], Step [2/8], train_loss: 408.6881\n",
            "Epoch [1170/2000], Step [4/8], train_loss: 603.0825\n",
            "Epoch [1170/2000], Step [6/8], train_loss: 186.1258\n",
            "Epoch [1170/2000], Step [8/8], train_loss: 280.8799\n",
            "Val Loss:  354.55838203430176\n",
            "------------------------------EPOCH: 1171 ------------------------\n",
            "Epoch [1171/2000], Step [2/8], train_loss: 404.6286\n",
            "Epoch [1171/2000], Step [4/8], train_loss: 611.5272\n",
            "Epoch [1171/2000], Step [6/8], train_loss: 185.9356\n",
            "Epoch [1171/2000], Step [8/8], train_loss: 281.0984\n",
            "Val Loss:  253.33265598615012\n",
            "------------------------------EPOCH: 1172 ------------------------\n",
            "Epoch [1172/2000], Step [2/8], train_loss: 407.8117\n",
            "Epoch [1172/2000], Step [4/8], train_loss: 602.3730\n",
            "Epoch [1172/2000], Step [6/8], train_loss: 186.0829\n",
            "Epoch [1172/2000], Step [8/8], train_loss: 280.9954\n",
            "Val Loss:  300.65644454956055\n",
            "------------------------------EPOCH: 1173 ------------------------\n",
            "Epoch [1173/2000], Step [2/8], train_loss: 404.6818\n",
            "Epoch [1173/2000], Step [4/8], train_loss: 610.8082\n",
            "Epoch [1173/2000], Step [6/8], train_loss: 185.4925\n",
            "Epoch [1173/2000], Step [8/8], train_loss: 281.2396\n",
            "Val Loss:  200.1423842906952\n",
            "------------------------------EPOCH: 1174 ------------------------\n",
            "Epoch [1174/2000], Step [2/8], train_loss: 407.8708\n",
            "Epoch [1174/2000], Step [4/8], train_loss: 611.6946\n",
            "Epoch [1174/2000], Step [6/8], train_loss: 185.9559\n",
            "Epoch [1174/2000], Step [8/8], train_loss: 281.0180\n",
            "Val Loss:  405.42242829004925\n",
            "------------------------------EPOCH: 1175 ------------------------\n",
            "Epoch [1175/2000], Step [2/8], train_loss: 403.7262\n",
            "Epoch [1175/2000], Step [4/8], train_loss: 618.7219\n",
            "Epoch [1175/2000], Step [6/8], train_loss: 185.2355\n",
            "Epoch [1175/2000], Step [8/8], train_loss: 281.4362\n",
            "Val Loss:  147.2657803694407\n",
            "------------------------------EPOCH: 1176 ------------------------\n",
            "Epoch [1176/2000], Step [2/8], train_loss: 412.1366\n",
            "Epoch [1176/2000], Step [4/8], train_loss: 602.7277\n",
            "Epoch [1176/2000], Step [6/8], train_loss: 185.1048\n",
            "Epoch [1176/2000], Step [8/8], train_loss: 280.8509\n",
            "Val Loss:  474.9503779411316\n",
            "------------------------------EPOCH: 1177 ------------------------\n",
            "Epoch [1177/2000], Step [2/8], train_loss: 400.7889\n",
            "Epoch [1177/2000], Step [4/8], train_loss: 626.6899\n",
            "Epoch [1177/2000], Step [6/8], train_loss: 185.8931\n",
            "Epoch [1177/2000], Step [8/8], train_loss: 281.2531\n",
            "Val Loss:  211.03503115971884\n",
            "------------------------------EPOCH: 1178 ------------------------\n",
            "Epoch [1178/2000], Step [2/8], train_loss: 409.1011\n",
            "Epoch [1178/2000], Step [4/8], train_loss: 604.4407\n",
            "Epoch [1178/2000], Step [6/8], train_loss: 186.9373\n",
            "Epoch [1178/2000], Step [8/8], train_loss: 280.8375\n",
            "Val Loss:  302.0516478220622\n",
            "------------------------------EPOCH: 1179 ------------------------\n",
            "Epoch [1179/2000], Step [2/8], train_loss: 401.0897\n",
            "Epoch [1179/2000], Step [4/8], train_loss: 626.0798\n",
            "Epoch [1179/2000], Step [6/8], train_loss: 185.8310\n",
            "Epoch [1179/2000], Step [8/8], train_loss: 281.1996\n",
            "Val Loss:  130.6310005982717\n",
            "------------------------------EPOCH: 1180 ------------------------\n",
            "Epoch [1180/2000], Step [2/8], train_loss: 409.1541\n",
            "Epoch [1180/2000], Step [4/8], train_loss: 612.4205\n",
            "Epoch [1180/2000], Step [6/8], train_loss: 186.0958\n",
            "Epoch [1180/2000], Step [8/8], train_loss: 280.7510\n",
            "Val Loss:  246.01861564318338\n",
            "------------------------------EPOCH: 1181 ------------------------\n",
            "Epoch [1181/2000], Step [2/8], train_loss: 400.2299\n",
            "Epoch [1181/2000], Step [4/8], train_loss: 633.4235\n",
            "Epoch [1181/2000], Step [6/8], train_loss: 185.8921\n",
            "Epoch [1181/2000], Step [8/8], train_loss: 281.1685\n",
            "Val Loss:  125.42700743675232\n",
            "------------------------------EPOCH: 1182 ------------------------\n",
            "Epoch [1182/2000], Step [2/8], train_loss: 409.7542\n",
            "Epoch [1182/2000], Step [4/8], train_loss: 599.8583\n",
            "Epoch [1182/2000], Step [6/8], train_loss: 186.2910\n",
            "Epoch [1182/2000], Step [8/8], train_loss: 280.7564\n",
            "Val Loss:  196.76917910575867\n",
            "------------------------------EPOCH: 1183 ------------------------\n",
            "Epoch [1183/2000], Step [2/8], train_loss: 401.0653\n",
            "Epoch [1183/2000], Step [4/8], train_loss: 627.9603\n",
            "Epoch [1183/2000], Step [6/8], train_loss: 185.9363\n",
            "Epoch [1183/2000], Step [8/8], train_loss: 281.0270\n",
            "Val Loss:  131.76308631896973\n",
            "------------------------------EPOCH: 1184 ------------------------\n",
            "Epoch [1184/2000], Step [2/8], train_loss: 407.4420\n",
            "Epoch [1184/2000], Step [4/8], train_loss: 607.5106\n",
            "Epoch [1184/2000], Step [6/8], train_loss: 186.3579\n",
            "Epoch [1184/2000], Step [8/8], train_loss: 280.7067\n",
            "Val Loss:  263.0899608532588\n",
            "------------------------------EPOCH: 1185 ------------------------\n",
            "Epoch [1185/2000], Step [2/8], train_loss: 400.5504\n",
            "Epoch [1185/2000], Step [4/8], train_loss: 628.4347\n",
            "Epoch [1185/2000], Step [6/8], train_loss: 185.7485\n",
            "Epoch [1185/2000], Step [8/8], train_loss: 281.0683\n",
            "Val Loss:  122.17559729019801\n",
            "------------------------------EPOCH: 1186 ------------------------\n",
            "Epoch [1186/2000], Step [2/8], train_loss: 408.0862\n",
            "Epoch [1186/2000], Step [4/8], train_loss: 600.7827\n",
            "Epoch [1186/2000], Step [6/8], train_loss: 185.6618\n",
            "Epoch [1186/2000], Step [8/8], train_loss: 280.7495\n",
            "Val Loss:  362.0649700164795\n",
            "------------------------------EPOCH: 1187 ------------------------\n",
            "Epoch [1187/2000], Step [2/8], train_loss: 400.6474\n",
            "Epoch [1187/2000], Step [4/8], train_loss: 621.2314\n",
            "Epoch [1187/2000], Step [6/8], train_loss: 185.7933\n",
            "Epoch [1187/2000], Step [8/8], train_loss: 281.0259\n",
            "Val Loss:  250.81185642878214\n",
            "------------------------------EPOCH: 1188 ------------------------\n",
            "Epoch [1188/2000], Step [2/8], train_loss: 406.8246\n",
            "Epoch [1188/2000], Step [4/8], train_loss: 602.2726\n",
            "Epoch [1188/2000], Step [6/8], train_loss: 185.9824\n",
            "Epoch [1188/2000], Step [8/8], train_loss: 280.8710\n",
            "Val Loss:  441.5409469604492\n",
            "------------------------------EPOCH: 1189 ------------------------\n",
            "Epoch [1189/2000], Step [2/8], train_loss: 402.4062\n",
            "Epoch [1189/2000], Step [4/8], train_loss: 617.9606\n",
            "Epoch [1189/2000], Step [6/8], train_loss: 185.4181\n",
            "Epoch [1189/2000], Step [8/8], train_loss: 281.1029\n",
            "Val Loss:  286.9090754191081\n",
            "------------------------------EPOCH: 1190 ------------------------\n",
            "Epoch [1190/2000], Step [2/8], train_loss: 406.5438\n",
            "Epoch [1190/2000], Step [4/8], train_loss: 612.8122\n",
            "Epoch [1190/2000], Step [6/8], train_loss: 185.9513\n",
            "Epoch [1190/2000], Step [8/8], train_loss: 280.8498\n",
            "Val Loss:  394.2203509012858\n",
            "------------------------------EPOCH: 1191 ------------------------\n",
            "Epoch [1191/2000], Step [2/8], train_loss: 400.8650\n",
            "Epoch [1191/2000], Step [4/8], train_loss: 623.1921\n",
            "Epoch [1191/2000], Step [6/8], train_loss: 185.5154\n",
            "Epoch [1191/2000], Step [8/8], train_loss: 281.2576\n",
            "Val Loss:  202.2715212504069\n",
            "------------------------------EPOCH: 1192 ------------------------\n",
            "Epoch [1192/2000], Step [2/8], train_loss: 407.7456\n",
            "Epoch [1192/2000], Step [4/8], train_loss: 603.8807\n",
            "Epoch [1192/2000], Step [6/8], train_loss: 185.3912\n",
            "Epoch [1192/2000], Step [8/8], train_loss: 280.8393\n",
            "Val Loss:  318.17760014533997\n",
            "------------------------------EPOCH: 1193 ------------------------\n",
            "Epoch [1193/2000], Step [2/8], train_loss: 399.3528\n",
            "Epoch [1193/2000], Step [4/8], train_loss: 624.9336\n",
            "Epoch [1193/2000], Step [6/8], train_loss: 185.2144\n",
            "Epoch [1193/2000], Step [8/8], train_loss: 281.1547\n",
            "Val Loss:  177.81803965568542\n",
            "------------------------------EPOCH: 1194 ------------------------\n",
            "Epoch [1194/2000], Step [2/8], train_loss: 406.8930\n",
            "Epoch [1194/2000], Step [4/8], train_loss: 604.8157\n",
            "Epoch [1194/2000], Step [6/8], train_loss: 185.6126\n",
            "Epoch [1194/2000], Step [8/8], train_loss: 280.8925\n",
            "Val Loss:  304.1178141434987\n",
            "------------------------------EPOCH: 1195 ------------------------\n",
            "Epoch [1195/2000], Step [2/8], train_loss: 401.1539\n",
            "Epoch [1195/2000], Step [4/8], train_loss: 621.4502\n",
            "Epoch [1195/2000], Step [6/8], train_loss: 184.9517\n",
            "Epoch [1195/2000], Step [8/8], train_loss: 281.1489\n",
            "Val Loss:  195.2348574002584\n",
            "------------------------------EPOCH: 1196 ------------------------\n",
            "Epoch [1196/2000], Step [2/8], train_loss: 405.9885\n",
            "Epoch [1196/2000], Step [4/8], train_loss: 603.3254\n",
            "Epoch [1196/2000], Step [6/8], train_loss: 184.9649\n",
            "Epoch [1196/2000], Step [8/8], train_loss: 280.9833\n",
            "Val Loss:  316.7830650806427\n",
            "------------------------------EPOCH: 1197 ------------------------\n",
            "Epoch [1197/2000], Step [2/8], train_loss: 400.4084\n",
            "Epoch [1197/2000], Step [4/8], train_loss: 618.1713\n",
            "Epoch [1197/2000], Step [6/8], train_loss: 185.6487\n",
            "Epoch [1197/2000], Step [8/8], train_loss: 281.2418\n",
            "Val Loss:  237.93734248479208\n",
            "------------------------------EPOCH: 1198 ------------------------\n",
            "Epoch [1198/2000], Step [2/8], train_loss: 403.9988\n",
            "Epoch [1198/2000], Step [4/8], train_loss: 610.7106\n",
            "Epoch [1198/2000], Step [6/8], train_loss: 186.3960\n",
            "Epoch [1198/2000], Step [8/8], train_loss: 280.9765\n",
            "Val Loss:  371.7917284965515\n",
            "------------------------------EPOCH: 1199 ------------------------\n",
            "Epoch [1199/2000], Step [2/8], train_loss: 399.7927\n",
            "Epoch [1199/2000], Step [4/8], train_loss: 624.0154\n",
            "Epoch [1199/2000], Step [6/8], train_loss: 184.8803\n",
            "Epoch [1199/2000], Step [8/8], train_loss: 281.3179\n",
            "Val Loss:  221.8168761730194\n",
            "------------------------------EPOCH: 1200 ------------------------\n",
            "Epoch [1200/2000], Step [2/8], train_loss: 407.2059\n",
            "Epoch [1200/2000], Step [4/8], train_loss: 605.4512\n",
            "Epoch [1200/2000], Step [6/8], train_loss: 184.2321\n",
            "Epoch [1200/2000], Step [8/8], train_loss: 280.9171\n",
            "Val Loss:  411.1750925381978\n",
            "------------------------------EPOCH: 1201 ------------------------\n",
            "Epoch [1201/2000], Step [2/8], train_loss: 398.6475\n",
            "Epoch [1201/2000], Step [4/8], train_loss: 625.4069\n",
            "Epoch [1201/2000], Step [6/8], train_loss: 185.4217\n",
            "Epoch [1201/2000], Step [8/8], train_loss: 281.2644\n",
            "Val Loss:  271.93837547302246\n",
            "------------------------------EPOCH: 1202 ------------------------\n",
            "Epoch [1202/2000], Step [2/8], train_loss: 404.3334\n",
            "Epoch [1202/2000], Step [4/8], train_loss: 605.8189\n",
            "Epoch [1202/2000], Step [6/8], train_loss: 186.7026\n",
            "Epoch [1202/2000], Step [8/8], train_loss: 280.9791\n",
            "Val Loss:  514.7603146235148\n",
            "------------------------------EPOCH: 1203 ------------------------\n",
            "Epoch [1203/2000], Step [2/8], train_loss: 398.4514\n",
            "Epoch [1203/2000], Step [4/8], train_loss: 622.3464\n",
            "Epoch [1203/2000], Step [6/8], train_loss: 185.4832\n",
            "Epoch [1203/2000], Step [8/8], train_loss: 281.2223\n",
            "Val Loss:  319.8666548728943\n",
            "------------------------------EPOCH: 1204 ------------------------\n",
            "Epoch [1204/2000], Step [2/8], train_loss: 403.8160\n",
            "Epoch [1204/2000], Step [4/8], train_loss: 605.4703\n",
            "Epoch [1204/2000], Step [6/8], train_loss: 184.7675\n",
            "Epoch [1204/2000], Step [8/8], train_loss: 281.0557\n",
            "Val Loss:  435.0320043563843\n",
            "------------------------------EPOCH: 1205 ------------------------\n",
            "Epoch [1205/2000], Step [2/8], train_loss: 399.2386\n",
            "Epoch [1205/2000], Step [4/8], train_loss: 618.9252\n",
            "Epoch [1205/2000], Step [6/8], train_loss: 185.1011\n",
            "Epoch [1205/2000], Step [8/8], train_loss: 281.2942\n",
            "Val Loss:  296.99543110529584\n",
            "------------------------------EPOCH: 1206 ------------------------\n",
            "Epoch [1206/2000], Step [2/8], train_loss: 403.2766\n",
            "Epoch [1206/2000], Step [4/8], train_loss: 611.8943\n",
            "Epoch [1206/2000], Step [6/8], train_loss: 186.0366\n",
            "Epoch [1206/2000], Step [8/8], train_loss: 281.0345\n",
            "Val Loss:  463.4591040611267\n",
            "------------------------------EPOCH: 1207 ------------------------\n",
            "Epoch [1207/2000], Step [2/8], train_loss: 398.6812\n",
            "Epoch [1207/2000], Step [4/8], train_loss: 623.1474\n",
            "Epoch [1207/2000], Step [6/8], train_loss: 184.6931\n",
            "Epoch [1207/2000], Step [8/8], train_loss: 281.4081\n",
            "Val Loss:  247.63753350575766\n",
            "------------------------------EPOCH: 1208 ------------------------\n",
            "Epoch [1208/2000], Step [2/8], train_loss: 405.8503\n",
            "Epoch [1208/2000], Step [4/8], train_loss: 607.7999\n",
            "Epoch [1208/2000], Step [6/8], train_loss: 184.3989\n",
            "Epoch [1208/2000], Step [8/8], train_loss: 281.0409\n",
            "Val Loss:  456.2572078704834\n",
            "------------------------------EPOCH: 1209 ------------------------\n",
            "Epoch [1209/2000], Step [2/8], train_loss: 398.1260\n",
            "Epoch [1209/2000], Step [4/8], train_loss: 626.3365\n",
            "Epoch [1209/2000], Step [6/8], train_loss: 184.6877\n",
            "Epoch [1209/2000], Step [8/8], train_loss: 281.4100\n",
            "Val Loss:  255.59955088297525\n",
            "------------------------------EPOCH: 1210 ------------------------\n",
            "Epoch [1210/2000], Step [2/8], train_loss: 404.6512\n",
            "Epoch [1210/2000], Step [4/8], train_loss: 607.1041\n",
            "Epoch [1210/2000], Step [6/8], train_loss: 185.3597\n",
            "Epoch [1210/2000], Step [8/8], train_loss: 281.1185\n",
            "Val Loss:  440.49008623758954\n",
            "------------------------------EPOCH: 1211 ------------------------\n",
            "Epoch [1211/2000], Step [2/8], train_loss: 398.5414\n",
            "Epoch [1211/2000], Step [4/8], train_loss: 622.8938\n",
            "Epoch [1211/2000], Step [6/8], train_loss: 184.8749\n",
            "Epoch [1211/2000], Step [8/8], train_loss: 281.3413\n",
            "Val Loss:  322.6968644460042\n",
            "------------------------------EPOCH: 1212 ------------------------\n",
            "Epoch [1212/2000], Step [2/8], train_loss: 403.4796\n",
            "Epoch [1212/2000], Step [4/8], train_loss: 605.5062\n",
            "Epoch [1212/2000], Step [6/8], train_loss: 185.1535\n",
            "Epoch [1212/2000], Step [8/8], train_loss: 281.1629\n",
            "Val Loss:  389.90375328063965\n",
            "------------------------------EPOCH: 1213 ------------------------\n",
            "Epoch [1213/2000], Step [2/8], train_loss: 399.1551\n",
            "Epoch [1213/2000], Step [4/8], train_loss: 619.5862\n",
            "Epoch [1213/2000], Step [6/8], train_loss: 184.8492\n",
            "Epoch [1213/2000], Step [8/8], train_loss: 281.3812\n",
            "Val Loss:  278.11671272913617\n",
            "------------------------------EPOCH: 1214 ------------------------\n",
            "Epoch [1214/2000], Step [2/8], train_loss: 402.7715\n",
            "Epoch [1214/2000], Step [4/8], train_loss: 610.6198\n",
            "Epoch [1214/2000], Step [6/8], train_loss: 184.7506\n",
            "Epoch [1214/2000], Step [8/8], train_loss: 281.1354\n",
            "Val Loss:  397.9938809076945\n",
            "------------------------------EPOCH: 1215 ------------------------\n",
            "Epoch [1215/2000], Step [2/8], train_loss: 398.0019\n",
            "Epoch [1215/2000], Step [4/8], train_loss: 621.8965\n",
            "Epoch [1215/2000], Step [6/8], train_loss: 184.6441\n",
            "Epoch [1215/2000], Step [8/8], train_loss: 281.3744\n",
            "Val Loss:  230.18332330385843\n",
            "------------------------------EPOCH: 1216 ------------------------\n",
            "Epoch [1216/2000], Step [2/8], train_loss: 403.4571\n",
            "Epoch [1216/2000], Step [4/8], train_loss: 611.3350\n",
            "Epoch [1216/2000], Step [6/8], train_loss: 184.6812\n",
            "Epoch [1216/2000], Step [8/8], train_loss: 281.2073\n",
            "Val Loss:  460.17766857147217\n",
            "------------------------------EPOCH: 1217 ------------------------\n",
            "Epoch [1217/2000], Step [2/8], train_loss: 399.3904\n",
            "Epoch [1217/2000], Step [4/8], train_loss: 621.2432\n",
            "Epoch [1217/2000], Step [6/8], train_loss: 184.5011\n",
            "Epoch [1217/2000], Step [8/8], train_loss: 281.4132\n",
            "Val Loss:  340.8256769180298\n",
            "------------------------------EPOCH: 1218 ------------------------\n",
            "Epoch [1218/2000], Step [2/8], train_loss: 401.9232\n",
            "Epoch [1218/2000], Step [4/8], train_loss: 609.9944\n",
            "Epoch [1218/2000], Step [6/8], train_loss: 185.3553\n",
            "Epoch [1218/2000], Step [8/8], train_loss: 281.3289\n",
            "Val Loss:  330.3443075815837\n",
            "------------------------------EPOCH: 1219 ------------------------\n",
            "Epoch [1219/2000], Step [2/8], train_loss: 397.8430\n",
            "Epoch [1219/2000], Step [4/8], train_loss: 617.1620\n",
            "Epoch [1219/2000], Step [6/8], train_loss: 185.2758\n",
            "Epoch [1219/2000], Step [8/8], train_loss: 281.5215\n",
            "Val Loss:  223.04850419362387\n",
            "------------------------------EPOCH: 1220 ------------------------\n",
            "Epoch [1220/2000], Step [2/8], train_loss: 400.7595\n",
            "Epoch [1220/2000], Step [4/8], train_loss: 608.4310\n",
            "Epoch [1220/2000], Step [6/8], train_loss: 185.2491\n",
            "Epoch [1220/2000], Step [8/8], train_loss: 281.4343\n",
            "Val Loss:  290.29925807317096\n",
            "------------------------------EPOCH: 1221 ------------------------\n",
            "Epoch [1221/2000], Step [2/8], train_loss: 398.8043\n",
            "Epoch [1221/2000], Step [4/8], train_loss: 615.1932\n",
            "Epoch [1221/2000], Step [6/8], train_loss: 184.3897\n",
            "Epoch [1221/2000], Step [8/8], train_loss: 281.6192\n",
            "Val Loss:  211.79764318466187\n",
            "------------------------------EPOCH: 1222 ------------------------\n",
            "Epoch [1222/2000], Step [2/8], train_loss: 401.9062\n",
            "Epoch [1222/2000], Step [4/8], train_loss: 618.1426\n",
            "Epoch [1222/2000], Step [6/8], train_loss: 184.9698\n",
            "Epoch [1222/2000], Step [8/8], train_loss: 281.3510\n",
            "Val Loss:  349.87858295440674\n",
            "------------------------------EPOCH: 1223 ------------------------\n",
            "Epoch [1223/2000], Step [2/8], train_loss: 397.6903\n",
            "Epoch [1223/2000], Step [4/8], train_loss: 622.3801\n",
            "Epoch [1223/2000], Step [6/8], train_loss: 184.3965\n",
            "Epoch [1223/2000], Step [8/8], train_loss: 281.7141\n",
            "Val Loss:  222.1838935216268\n",
            "------------------------------EPOCH: 1224 ------------------------\n",
            "Epoch [1224/2000], Step [2/8], train_loss: 404.0089\n",
            "Epoch [1224/2000], Step [4/8], train_loss: 602.8470\n",
            "Epoch [1224/2000], Step [6/8], train_loss: 184.6346\n",
            "Epoch [1224/2000], Step [8/8], train_loss: 281.4380\n",
            "Val Loss:  284.32254870732623\n",
            "------------------------------EPOCH: 1225 ------------------------\n",
            "Epoch [1225/2000], Step [2/8], train_loss: 397.4169\n",
            "Epoch [1225/2000], Step [4/8], train_loss: 627.2212\n",
            "Epoch [1225/2000], Step [6/8], train_loss: 184.3197\n",
            "Epoch [1225/2000], Step [8/8], train_loss: 281.8328\n",
            "Val Loss:  142.47304844856262\n",
            "------------------------------EPOCH: 1226 ------------------------\n",
            "Epoch [1226/2000], Step [2/8], train_loss: 403.2571\n",
            "Epoch [1226/2000], Step [4/8], train_loss: 608.3482\n",
            "Epoch [1226/2000], Step [6/8], train_loss: 184.5139\n",
            "Epoch [1226/2000], Step [8/8], train_loss: 281.5841\n",
            "Val Loss:  323.4801781972249\n",
            "------------------------------EPOCH: 1227 ------------------------\n",
            "Epoch [1227/2000], Step [2/8], train_loss: 397.4598\n",
            "Epoch [1227/2000], Step [4/8], train_loss: 623.6390\n",
            "Epoch [1227/2000], Step [6/8], train_loss: 183.8824\n",
            "Epoch [1227/2000], Step [8/8], train_loss: 281.8110\n",
            "Val Loss:  203.60880255699158\n",
            "------------------------------EPOCH: 1228 ------------------------\n",
            "Epoch [1228/2000], Step [2/8], train_loss: 401.6219\n",
            "Epoch [1228/2000], Step [4/8], train_loss: 612.3232\n",
            "Epoch [1228/2000], Step [6/8], train_loss: 184.1451\n",
            "Epoch [1228/2000], Step [8/8], train_loss: 281.5554\n",
            "Val Loss:  281.67970911661786\n",
            "------------------------------EPOCH: 1229 ------------------------\n",
            "Epoch [1229/2000], Step [2/8], train_loss: 396.4489\n",
            "Epoch [1229/2000], Step [4/8], train_loss: 622.4131\n",
            "Epoch [1229/2000], Step [6/8], train_loss: 184.4633\n",
            "Epoch [1229/2000], Step [8/8], train_loss: 281.7721\n",
            "Val Loss:  193.73969467480978\n",
            "------------------------------EPOCH: 1230 ------------------------\n",
            "Epoch [1230/2000], Step [2/8], train_loss: 400.6106\n",
            "Epoch [1230/2000], Step [4/8], train_loss: 610.5883\n",
            "Epoch [1230/2000], Step [6/8], train_loss: 185.2366\n",
            "Epoch [1230/2000], Step [8/8], train_loss: 281.5876\n",
            "Val Loss:  250.79500301678976\n",
            "------------------------------EPOCH: 1231 ------------------------\n",
            "Epoch [1231/2000], Step [2/8], train_loss: 396.8886\n",
            "Epoch [1231/2000], Step [4/8], train_loss: 619.6032\n",
            "Epoch [1231/2000], Step [6/8], train_loss: 184.4595\n",
            "Epoch [1231/2000], Step [8/8], train_loss: 281.8272\n",
            "Val Loss:  174.2368016242981\n",
            "------------------------------EPOCH: 1232 ------------------------\n",
            "Epoch [1232/2000], Step [2/8], train_loss: 400.8203\n",
            "Epoch [1232/2000], Step [4/8], train_loss: 621.1498\n",
            "Epoch [1232/2000], Step [6/8], train_loss: 185.1390\n",
            "Epoch [1232/2000], Step [8/8], train_loss: 281.5110\n",
            "Val Loss:  340.2622979482015\n",
            "------------------------------EPOCH: 1233 ------------------------\n",
            "Epoch [1233/2000], Step [2/8], train_loss: 395.3344\n",
            "Epoch [1233/2000], Step [4/8], train_loss: 626.8223\n",
            "Epoch [1233/2000], Step [6/8], train_loss: 184.5802\n",
            "Epoch [1233/2000], Step [8/8], train_loss: 281.9835\n",
            "Val Loss:  142.24611568450928\n",
            "------------------------------EPOCH: 1234 ------------------------\n",
            "Epoch [1234/2000], Step [2/8], train_loss: 403.0469\n",
            "Epoch [1234/2000], Step [4/8], train_loss: 609.7446\n",
            "Epoch [1234/2000], Step [6/8], train_loss: 184.7661\n",
            "Epoch [1234/2000], Step [8/8], train_loss: 281.4832\n",
            "Val Loss:  371.3449861208598\n",
            "------------------------------EPOCH: 1235 ------------------------\n",
            "Epoch [1235/2000], Step [2/8], train_loss: 394.3855\n",
            "Epoch [1235/2000], Step [4/8], train_loss: 632.5741\n",
            "Epoch [1235/2000], Step [6/8], train_loss: 184.1432\n",
            "Epoch [1235/2000], Step [8/8], train_loss: 281.8465\n",
            "Val Loss:  171.08600536982217\n",
            "------------------------------EPOCH: 1236 ------------------------\n",
            "Epoch [1236/2000], Step [2/8], train_loss: 402.5258\n",
            "Epoch [1236/2000], Step [4/8], train_loss: 609.5842\n",
            "Epoch [1236/2000], Step [6/8], train_loss: 184.8861\n",
            "Epoch [1236/2000], Step [8/8], train_loss: 281.4540\n",
            "Val Loss:  291.09096749623615\n",
            "------------------------------EPOCH: 1237 ------------------------\n",
            "Epoch [1237/2000], Step [2/8], train_loss: 394.7054\n",
            "Epoch [1237/2000], Step [4/8], train_loss: 629.8837\n",
            "Epoch [1237/2000], Step [6/8], train_loss: 184.3850\n",
            "Epoch [1237/2000], Step [8/8], train_loss: 281.7544\n",
            "Val Loss:  161.1856997013092\n",
            "------------------------------EPOCH: 1238 ------------------------\n",
            "Epoch [1238/2000], Step [2/8], train_loss: 400.5552\n",
            "Epoch [1238/2000], Step [4/8], train_loss: 617.9312\n",
            "Epoch [1238/2000], Step [6/8], train_loss: 185.0098\n",
            "Epoch [1238/2000], Step [8/8], train_loss: 281.3804\n",
            "Val Loss:  301.5092649459839\n",
            "------------------------------EPOCH: 1239 ------------------------\n",
            "Epoch [1239/2000], Step [2/8], train_loss: 393.9121\n",
            "Epoch [1239/2000], Step [4/8], train_loss: 637.9414\n",
            "Epoch [1239/2000], Step [6/8], train_loss: 184.4090\n",
            "Epoch [1239/2000], Step [8/8], train_loss: 281.7941\n",
            "Val Loss:  189.14724842707315\n",
            "------------------------------EPOCH: 1240 ------------------------\n",
            "Epoch [1240/2000], Step [2/8], train_loss: 402.7489\n",
            "Epoch [1240/2000], Step [4/8], train_loss: 605.1151\n",
            "Epoch [1240/2000], Step [6/8], train_loss: 184.5091\n",
            "Epoch [1240/2000], Step [8/8], train_loss: 281.3680\n",
            "Val Loss:  382.2884777386983\n",
            "------------------------------EPOCH: 1241 ------------------------\n",
            "Epoch [1241/2000], Step [2/8], train_loss: 393.4977\n",
            "Epoch [1241/2000], Step [4/8], train_loss: 634.2009\n",
            "Epoch [1241/2000], Step [6/8], train_loss: 184.7082\n",
            "Epoch [1241/2000], Step [8/8], train_loss: 281.6177\n",
            "Val Loss:  222.96587467193604\n",
            "------------------------------EPOCH: 1242 ------------------------\n",
            "Epoch [1242/2000], Step [2/8], train_loss: 398.7852\n",
            "Epoch [1242/2000], Step [4/8], train_loss: 614.6286\n",
            "Epoch [1242/2000], Step [6/8], train_loss: 185.5330\n",
            "Epoch [1242/2000], Step [8/8], train_loss: 281.2891\n",
            "Val Loss:  359.3350806236267\n",
            "------------------------------EPOCH: 1243 ------------------------\n",
            "Epoch [1243/2000], Step [2/8], train_loss: 392.6328\n",
            "Epoch [1243/2000], Step [4/8], train_loss: 635.9507\n",
            "Epoch [1243/2000], Step [6/8], train_loss: 185.0157\n",
            "Epoch [1243/2000], Step [8/8], train_loss: 281.6818\n",
            "Val Loss:  230.28489271799722\n",
            "------------------------------EPOCH: 1244 ------------------------\n",
            "Epoch [1244/2000], Step [2/8], train_loss: 400.7794\n",
            "Epoch [1244/2000], Step [4/8], train_loss: 607.7623\n",
            "Epoch [1244/2000], Step [6/8], train_loss: 185.0161\n",
            "Epoch [1244/2000], Step [8/8], train_loss: 281.3677\n",
            "Val Loss:  423.0232760111491\n",
            "------------------------------EPOCH: 1245 ------------------------\n",
            "Epoch [1245/2000], Step [2/8], train_loss: 393.7438\n",
            "Epoch [1245/2000], Step [4/8], train_loss: 629.8416\n",
            "Epoch [1245/2000], Step [6/8], train_loss: 184.4587\n",
            "Epoch [1245/2000], Step [8/8], train_loss: 281.6452\n",
            "Val Loss:  254.3958657582601\n",
            "------------------------------EPOCH: 1246 ------------------------\n",
            "Epoch [1246/2000], Step [2/8], train_loss: 399.1555\n",
            "Epoch [1246/2000], Step [4/8], train_loss: 619.1562\n",
            "Epoch [1246/2000], Step [6/8], train_loss: 185.1658\n",
            "Epoch [1246/2000], Step [8/8], train_loss: 281.3206\n",
            "Val Loss:  338.86865520477295\n",
            "------------------------------EPOCH: 1247 ------------------------\n",
            "Epoch [1247/2000], Step [2/8], train_loss: 392.6887\n",
            "Epoch [1247/2000], Step [4/8], train_loss: 633.6666\n",
            "Epoch [1247/2000], Step [6/8], train_loss: 184.3769\n",
            "Epoch [1247/2000], Step [8/8], train_loss: 281.7097\n",
            "Val Loss:  163.77003407478333\n",
            "------------------------------EPOCH: 1248 ------------------------\n",
            "Epoch [1248/2000], Step [2/8], train_loss: 400.3499\n",
            "Epoch [1248/2000], Step [4/8], train_loss: 609.9472\n",
            "Epoch [1248/2000], Step [6/8], train_loss: 184.5062\n",
            "Epoch [1248/2000], Step [8/8], train_loss: 281.3217\n",
            "Val Loss:  390.7466033299764\n",
            "------------------------------EPOCH: 1249 ------------------------\n",
            "Epoch [1249/2000], Step [2/8], train_loss: 392.1577\n",
            "Epoch [1249/2000], Step [4/8], train_loss: 631.7731\n",
            "Epoch [1249/2000], Step [6/8], train_loss: 184.6465\n",
            "Epoch [1249/2000], Step [8/8], train_loss: 281.6676\n",
            "Val Loss:  236.4430820941925\n",
            "------------------------------EPOCH: 1250 ------------------------\n",
            "Epoch [1250/2000], Step [2/8], train_loss: 398.9052\n",
            "Epoch [1250/2000], Step [4/8], train_loss: 609.6987\n",
            "Epoch [1250/2000], Step [6/8], train_loss: 184.7024\n",
            "Epoch [1250/2000], Step [8/8], train_loss: 281.4683\n",
            "Val Loss:  300.1744966506958\n",
            "------------------------------EPOCH: 1251 ------------------------\n",
            "Epoch [1251/2000], Step [2/8], train_loss: 394.1637\n",
            "Epoch [1251/2000], Step [4/8], train_loss: 626.7137\n",
            "Epoch [1251/2000], Step [6/8], train_loss: 184.0473\n",
            "Epoch [1251/2000], Step [8/8], train_loss: 281.6947\n",
            "Val Loss:  199.67026805877686\n",
            "------------------------------EPOCH: 1252 ------------------------\n",
            "Epoch [1252/2000], Step [2/8], train_loss: 398.6709\n",
            "Epoch [1252/2000], Step [4/8], train_loss: 617.6902\n",
            "Epoch [1252/2000], Step [6/8], train_loss: 184.6656\n",
            "Epoch [1252/2000], Step [8/8], train_loss: 281.4177\n",
            "Val Loss:  434.07931391398114\n",
            "------------------------------EPOCH: 1253 ------------------------\n",
            "Epoch [1253/2000], Step [2/8], train_loss: 393.1762\n",
            "Epoch [1253/2000], Step [4/8], train_loss: 631.5035\n",
            "Epoch [1253/2000], Step [6/8], train_loss: 183.9888\n",
            "Epoch [1253/2000], Step [8/8], train_loss: 281.7678\n",
            "Val Loss:  191.5684404373169\n",
            "------------------------------EPOCH: 1254 ------------------------\n",
            "Epoch [1254/2000], Step [2/8], train_loss: 399.7480\n",
            "Epoch [1254/2000], Step [4/8], train_loss: 609.9202\n",
            "Epoch [1254/2000], Step [6/8], train_loss: 183.9324\n",
            "Epoch [1254/2000], Step [8/8], train_loss: 281.4307\n",
            "Val Loss:  446.9501856168111\n",
            "------------------------------EPOCH: 1255 ------------------------\n",
            "Epoch [1255/2000], Step [2/8], train_loss: 392.6581\n",
            "Epoch [1255/2000], Step [4/8], train_loss: 629.9071\n",
            "Epoch [1255/2000], Step [6/8], train_loss: 183.9118\n",
            "Epoch [1255/2000], Step [8/8], train_loss: 281.7180\n",
            "Val Loss:  299.2423159281413\n",
            "------------------------------EPOCH: 1256 ------------------------\n",
            "Epoch [1256/2000], Step [2/8], train_loss: 398.1651\n",
            "Epoch [1256/2000], Step [4/8], train_loss: 611.1328\n",
            "Epoch [1256/2000], Step [6/8], train_loss: 184.5066\n",
            "Epoch [1256/2000], Step [8/8], train_loss: 281.4960\n",
            "Val Loss:  340.38260555267334\n",
            "------------------------------EPOCH: 1257 ------------------------\n",
            "Epoch [1257/2000], Step [2/8], train_loss: 393.0465\n",
            "Epoch [1257/2000], Step [4/8], train_loss: 625.6052\n",
            "Epoch [1257/2000], Step [6/8], train_loss: 184.0656\n",
            "Epoch [1257/2000], Step [8/8], train_loss: 281.7275\n",
            "Val Loss:  222.8862819671631\n",
            "------------------------------EPOCH: 1258 ------------------------\n",
            "Epoch [1258/2000], Step [2/8], train_loss: 397.2708\n",
            "Epoch [1258/2000], Step [4/8], train_loss: 619.3024\n",
            "Epoch [1258/2000], Step [6/8], train_loss: 184.7457\n",
            "Epoch [1258/2000], Step [8/8], train_loss: 281.4277\n",
            "Val Loss:  448.9296735127767\n",
            "------------------------------EPOCH: 1259 ------------------------\n",
            "Epoch [1259/2000], Step [2/8], train_loss: 391.8080\n",
            "Epoch [1259/2000], Step [4/8], train_loss: 630.2140\n",
            "Epoch [1259/2000], Step [6/8], train_loss: 183.8566\n",
            "Epoch [1259/2000], Step [8/8], train_loss: 281.8110\n",
            "Val Loss:  229.70794550577799\n",
            "------------------------------EPOCH: 1260 ------------------------\n",
            "Epoch [1260/2000], Step [2/8], train_loss: 399.0611\n",
            "Epoch [1260/2000], Step [4/8], train_loss: 612.2117\n",
            "Epoch [1260/2000], Step [6/8], train_loss: 184.0364\n",
            "Epoch [1260/2000], Step [8/8], train_loss: 281.4393\n",
            "Val Loss:  371.16023174921673\n",
            "------------------------------EPOCH: 1261 ------------------------\n",
            "Epoch [1261/2000], Step [2/8], train_loss: 391.6721\n",
            "Epoch [1261/2000], Step [4/8], train_loss: 632.9438\n",
            "Epoch [1261/2000], Step [6/8], train_loss: 183.7945\n",
            "Epoch [1261/2000], Step [8/8], train_loss: 281.7794\n",
            "Val Loss:  220.65318298339844\n",
            "------------------------------EPOCH: 1262 ------------------------\n",
            "Epoch [1262/2000], Step [2/8], train_loss: 397.6146\n",
            "Epoch [1262/2000], Step [4/8], train_loss: 610.7094\n",
            "Epoch [1262/2000], Step [6/8], train_loss: 184.0572\n",
            "Epoch [1262/2000], Step [8/8], train_loss: 281.4837\n",
            "Val Loss:  393.70980230967206\n",
            "------------------------------EPOCH: 1263 ------------------------\n",
            "Epoch [1263/2000], Step [2/8], train_loss: 391.1583\n",
            "Epoch [1263/2000], Step [4/8], train_loss: 628.2151\n",
            "Epoch [1263/2000], Step [6/8], train_loss: 184.6382\n",
            "Epoch [1263/2000], Step [8/8], train_loss: 281.6859\n",
            "Val Loss:  242.73264837265015\n",
            "------------------------------EPOCH: 1264 ------------------------\n",
            "Epoch [1264/2000], Step [2/8], train_loss: 395.4355\n",
            "Epoch [1264/2000], Step [4/8], train_loss: 618.0262\n",
            "Epoch [1264/2000], Step [6/8], train_loss: 185.9722\n",
            "Epoch [1264/2000], Step [8/8], train_loss: 281.3670\n",
            "Val Loss:  372.10824664433795\n",
            "------------------------------EPOCH: 1265 ------------------------\n",
            "Epoch [1265/2000], Step [2/8], train_loss: 390.3680\n",
            "Epoch [1265/2000], Step [4/8], train_loss: 635.1526\n",
            "Epoch [1265/2000], Step [6/8], train_loss: 184.6871\n",
            "Epoch [1265/2000], Step [8/8], train_loss: 281.7449\n",
            "Val Loss:  191.1479864915212\n",
            "------------------------------EPOCH: 1266 ------------------------\n",
            "Epoch [1266/2000], Step [2/8], train_loss: 397.9949\n",
            "Epoch [1266/2000], Step [4/8], train_loss: 610.3495\n",
            "Epoch [1266/2000], Step [6/8], train_loss: 184.0285\n",
            "Epoch [1266/2000], Step [8/8], train_loss: 281.4364\n",
            "Val Loss:  275.9502749443054\n",
            "------------------------------EPOCH: 1267 ------------------------\n",
            "Epoch [1267/2000], Step [2/8], train_loss: 391.2763\n",
            "Epoch [1267/2000], Step [4/8], train_loss: 631.1607\n",
            "Epoch [1267/2000], Step [6/8], train_loss: 183.6887\n",
            "Epoch [1267/2000], Step [8/8], train_loss: 281.7574\n",
            "Val Loss:  166.48283910751343\n",
            "------------------------------EPOCH: 1268 ------------------------\n",
            "Epoch [1268/2000], Step [2/8], train_loss: 396.8694\n",
            "Epoch [1268/2000], Step [4/8], train_loss: 611.0754\n",
            "Epoch [1268/2000], Step [6/8], train_loss: 184.3138\n",
            "Epoch [1268/2000], Step [8/8], train_loss: 281.5737\n",
            "Val Loss:  242.22599403063455\n",
            "------------------------------EPOCH: 1269 ------------------------\n",
            "Epoch [1269/2000], Step [2/8], train_loss: 391.5709\n",
            "Epoch [1269/2000], Step [4/8], train_loss: 626.4479\n",
            "Epoch [1269/2000], Step [6/8], train_loss: 183.8943\n",
            "Epoch [1269/2000], Step [8/8], train_loss: 281.8102\n",
            "Val Loss:  169.54159053166708\n",
            "------------------------------EPOCH: 1270 ------------------------\n",
            "Epoch [1270/2000], Step [2/8], train_loss: 395.1869\n",
            "Epoch [1270/2000], Step [4/8], train_loss: 621.4727\n",
            "Epoch [1270/2000], Step [6/8], train_loss: 184.1857\n",
            "Epoch [1270/2000], Step [8/8], train_loss: 281.5629\n",
            "Val Loss:  309.0731658935547\n",
            "------------------------------EPOCH: 1271 ------------------------\n",
            "Epoch [1271/2000], Step [2/8], train_loss: 390.8985\n",
            "Epoch [1271/2000], Step [4/8], train_loss: 632.7081\n",
            "Epoch [1271/2000], Step [6/8], train_loss: 183.4303\n",
            "Epoch [1271/2000], Step [8/8], train_loss: 281.9495\n",
            "Val Loss:  168.2549091974894\n",
            "------------------------------EPOCH: 1272 ------------------------\n",
            "Epoch [1272/2000], Step [2/8], train_loss: 398.2886\n",
            "Epoch [1272/2000], Step [4/8], train_loss: 611.4403\n",
            "Epoch [1272/2000], Step [6/8], train_loss: 183.7244\n",
            "Epoch [1272/2000], Step [8/8], train_loss: 281.5456\n",
            "Val Loss:  301.9828640619914\n",
            "------------------------------EPOCH: 1273 ------------------------\n",
            "Epoch [1273/2000], Step [2/8], train_loss: 389.5277\n",
            "Epoch [1273/2000], Step [4/8], train_loss: 634.1726\n",
            "Epoch [1273/2000], Step [6/8], train_loss: 184.0911\n",
            "Epoch [1273/2000], Step [8/8], train_loss: 281.8456\n",
            "Val Loss:  166.8654444217682\n",
            "------------------------------EPOCH: 1274 ------------------------\n",
            "Epoch [1274/2000], Step [2/8], train_loss: 394.4158\n",
            "Epoch [1274/2000], Step [4/8], train_loss: 611.6815\n",
            "Epoch [1274/2000], Step [6/8], train_loss: 184.5169\n",
            "Epoch [1274/2000], Step [8/8], train_loss: 281.6246\n",
            "Val Loss:  293.74270582199097\n",
            "------------------------------EPOCH: 1275 ------------------------\n",
            "Epoch [1275/2000], Step [2/8], train_loss: 389.7831\n",
            "Epoch [1275/2000], Step [4/8], train_loss: 627.7704\n",
            "Epoch [1275/2000], Step [6/8], train_loss: 184.2210\n",
            "Epoch [1275/2000], Step [8/8], train_loss: 281.8880\n",
            "Val Loss:  202.52027519543967\n",
            "------------------------------EPOCH: 1276 ------------------------\n",
            "Epoch [1276/2000], Step [2/8], train_loss: 394.9650\n",
            "Epoch [1276/2000], Step [4/8], train_loss: 619.3383\n",
            "Epoch [1276/2000], Step [6/8], train_loss: 184.7564\n",
            "Epoch [1276/2000], Step [8/8], train_loss: 281.6141\n",
            "Val Loss:  300.96270449956256\n",
            "------------------------------EPOCH: 1277 ------------------------\n",
            "Epoch [1277/2000], Step [2/8], train_loss: 389.8771\n",
            "Epoch [1277/2000], Step [4/8], train_loss: 633.2858\n",
            "Epoch [1277/2000], Step [6/8], train_loss: 183.3601\n",
            "Epoch [1277/2000], Step [8/8], train_loss: 281.9943\n",
            "Val Loss:  188.5652674039205\n",
            "------------------------------EPOCH: 1278 ------------------------\n",
            "Epoch [1278/2000], Step [2/8], train_loss: 396.8099\n",
            "Epoch [1278/2000], Step [4/8], train_loss: 614.0654\n",
            "Epoch [1278/2000], Step [6/8], train_loss: 183.3757\n",
            "Epoch [1278/2000], Step [8/8], train_loss: 281.6290\n",
            "Val Loss:  330.10128768285114\n",
            "------------------------------EPOCH: 1279 ------------------------\n",
            "Epoch [1279/2000], Step [2/8], train_loss: 389.0431\n",
            "Epoch [1279/2000], Step [4/8], train_loss: 633.4938\n",
            "Epoch [1279/2000], Step [6/8], train_loss: 183.4681\n",
            "Epoch [1279/2000], Step [8/8], train_loss: 281.9400\n",
            "Val Loss:  184.28068709373474\n",
            "------------------------------EPOCH: 1280 ------------------------\n",
            "Epoch [1280/2000], Step [2/8], train_loss: 395.6367\n",
            "Epoch [1280/2000], Step [4/8], train_loss: 613.6201\n",
            "Epoch [1280/2000], Step [6/8], train_loss: 183.8681\n",
            "Epoch [1280/2000], Step [8/8], train_loss: 281.7423\n",
            "Val Loss:  321.9897720019023\n",
            "------------------------------EPOCH: 1281 ------------------------\n",
            "Epoch [1281/2000], Step [2/8], train_loss: 390.8464\n",
            "Epoch [1281/2000], Step [4/8], train_loss: 627.9458\n",
            "Epoch [1281/2000], Step [6/8], train_loss: 183.6526\n",
            "Epoch [1281/2000], Step [8/8], train_loss: 281.9400\n",
            "Val Loss:  244.41182057062784\n",
            "------------------------------EPOCH: 1282 ------------------------\n",
            "Epoch [1282/2000], Step [2/8], train_loss: 394.3413\n",
            "Epoch [1282/2000], Step [4/8], train_loss: 613.0399\n",
            "Epoch [1282/2000], Step [6/8], train_loss: 183.8288\n",
            "Epoch [1282/2000], Step [8/8], train_loss: 281.8398\n",
            "Val Loss:  321.7405071258545\n",
            "------------------------------EPOCH: 1283 ------------------------\n",
            "Epoch [1283/2000], Step [2/8], train_loss: 391.0335\n",
            "Epoch [1283/2000], Step [4/8], train_loss: 624.5548\n",
            "Epoch [1283/2000], Step [6/8], train_loss: 183.7980\n",
            "Epoch [1283/2000], Step [8/8], train_loss: 282.0132\n",
            "Val Loss:  278.498170375824\n",
            "------------------------------EPOCH: 1284 ------------------------\n",
            "Epoch [1284/2000], Step [2/8], train_loss: 393.4845\n",
            "Epoch [1284/2000], Step [4/8], train_loss: 618.7896\n",
            "Epoch [1284/2000], Step [6/8], train_loss: 183.8821\n",
            "Epoch [1284/2000], Step [8/8], train_loss: 281.8322\n",
            "Val Loss:  320.14122319221497\n",
            "------------------------------EPOCH: 1285 ------------------------\n",
            "Epoch [1285/2000], Step [2/8], train_loss: 389.9225\n",
            "Epoch [1285/2000], Step [4/8], train_loss: 627.6028\n",
            "Epoch [1285/2000], Step [6/8], train_loss: 183.3241\n",
            "Epoch [1285/2000], Step [8/8], train_loss: 281.9916\n",
            "Val Loss:  283.9376409848531\n",
            "------------------------------EPOCH: 1286 ------------------------\n",
            "Epoch [1286/2000], Step [2/8], train_loss: 393.1996\n",
            "Epoch [1286/2000], Step [4/8], train_loss: 620.2117\n",
            "Epoch [1286/2000], Step [6/8], train_loss: 183.4844\n",
            "Epoch [1286/2000], Step [8/8], train_loss: 281.8651\n",
            "Val Loss:  339.80069096883136\n",
            "------------------------------EPOCH: 1287 ------------------------\n",
            "Epoch [1287/2000], Step [2/8], train_loss: 390.4825\n",
            "Epoch [1287/2000], Step [4/8], train_loss: 626.1753\n",
            "Epoch [1287/2000], Step [6/8], train_loss: 183.2503\n",
            "Epoch [1287/2000], Step [8/8], train_loss: 282.0345\n",
            "Val Loss:  273.414141813914\n",
            "------------------------------EPOCH: 1288 ------------------------\n",
            "Epoch [1288/2000], Step [2/8], train_loss: 392.4077\n",
            "Epoch [1288/2000], Step [4/8], train_loss: 617.8131\n",
            "Epoch [1288/2000], Step [6/8], train_loss: 183.5782\n",
            "Epoch [1288/2000], Step [8/8], train_loss: 282.0253\n",
            "Val Loss:  313.4199860890706\n",
            "------------------------------EPOCH: 1289 ------------------------\n",
            "Epoch [1289/2000], Step [2/8], train_loss: 390.6765\n",
            "Epoch [1289/2000], Step [4/8], train_loss: 623.0298\n",
            "Epoch [1289/2000], Step [6/8], train_loss: 183.5176\n",
            "Epoch [1289/2000], Step [8/8], train_loss: 282.2238\n",
            "Val Loss:  234.2433501879374\n",
            "------------------------------EPOCH: 1290 ------------------------\n",
            "Epoch [1290/2000], Step [2/8], train_loss: 393.3255\n",
            "Epoch [1290/2000], Step [4/8], train_loss: 615.4608\n",
            "Epoch [1290/2000], Step [6/8], train_loss: 183.4182\n",
            "Epoch [1290/2000], Step [8/8], train_loss: 282.1622\n",
            "Val Loss:  266.6385142008464\n",
            "------------------------------EPOCH: 1291 ------------------------\n",
            "Epoch [1291/2000], Step [2/8], train_loss: 391.1759\n",
            "Epoch [1291/2000], Step [4/8], train_loss: 622.8590\n",
            "Epoch [1291/2000], Step [6/8], train_loss: 182.8482\n",
            "Epoch [1291/2000], Step [8/8], train_loss: 282.3412\n",
            "Val Loss:  221.32676347096762\n",
            "------------------------------EPOCH: 1292 ------------------------\n",
            "Epoch [1292/2000], Step [2/8], train_loss: 393.1187\n",
            "Epoch [1292/2000], Step [4/8], train_loss: 622.9144\n",
            "Epoch [1292/2000], Step [6/8], train_loss: 183.7114\n",
            "Epoch [1292/2000], Step [8/8], train_loss: 282.1206\n",
            "Val Loss:  336.65522718429565\n",
            "------------------------------EPOCH: 1293 ------------------------\n",
            "Epoch [1293/2000], Step [2/8], train_loss: 389.2260\n",
            "Epoch [1293/2000], Step [4/8], train_loss: 627.1285\n",
            "Epoch [1293/2000], Step [6/8], train_loss: 182.8931\n",
            "Epoch [1293/2000], Step [8/8], train_loss: 282.4444\n",
            "Val Loss:  217.61638514200845\n",
            "------------------------------EPOCH: 1294 ------------------------\n",
            "Epoch [1294/2000], Step [2/8], train_loss: 394.4079\n",
            "Epoch [1294/2000], Step [4/8], train_loss: 611.7313\n",
            "Epoch [1294/2000], Step [6/8], train_loss: 183.2214\n",
            "Epoch [1294/2000], Step [8/8], train_loss: 282.1951\n",
            "Val Loss:  318.3880439599355\n",
            "------------------------------EPOCH: 1295 ------------------------\n",
            "Epoch [1295/2000], Step [2/8], train_loss: 389.4467\n",
            "Epoch [1295/2000], Step [4/8], train_loss: 631.4468\n",
            "Epoch [1295/2000], Step [6/8], train_loss: 182.8593\n",
            "Epoch [1295/2000], Step [8/8], train_loss: 282.4926\n",
            "Val Loss:  206.79260635375977\n",
            "------------------------------EPOCH: 1296 ------------------------\n",
            "Epoch [1296/2000], Step [2/8], train_loss: 394.0395\n",
            "Epoch [1296/2000], Step [4/8], train_loss: 616.9902\n",
            "Epoch [1296/2000], Step [6/8], train_loss: 183.4461\n",
            "Epoch [1296/2000], Step [8/8], train_loss: 282.2070\n",
            "Val Loss:  383.8947992324829\n",
            "------------------------------EPOCH: 1297 ------------------------\n",
            "Epoch [1297/2000], Step [2/8], train_loss: 388.7648\n",
            "Epoch [1297/2000], Step [4/8], train_loss: 629.5800\n",
            "Epoch [1297/2000], Step [6/8], train_loss: 182.9203\n",
            "Epoch [1297/2000], Step [8/8], train_loss: 282.4125\n",
            "Val Loss:  252.3884553114573\n",
            "------------------------------EPOCH: 1298 ------------------------\n",
            "Epoch [1298/2000], Step [2/8], train_loss: 392.6318\n",
            "Epoch [1298/2000], Step [4/8], train_loss: 619.9721\n",
            "Epoch [1298/2000], Step [6/8], train_loss: 183.1947\n",
            "Epoch [1298/2000], Step [8/8], train_loss: 282.2299\n",
            "Val Loss:  314.8041598002116\n",
            "------------------------------EPOCH: 1299 ------------------------\n",
            "Epoch [1299/2000], Step [2/8], train_loss: 389.2237\n",
            "Epoch [1299/2000], Step [4/8], train_loss: 628.9496\n",
            "Epoch [1299/2000], Step [6/8], train_loss: 182.9828\n",
            "Epoch [1299/2000], Step [8/8], train_loss: 282.5385\n",
            "Val Loss:  208.88160141309103\n",
            "------------------------------EPOCH: 1300 ------------------------\n",
            "Epoch [1300/2000], Step [2/8], train_loss: 392.9601\n",
            "Epoch [1300/2000], Step [4/8], train_loss: 615.9069\n",
            "Epoch [1300/2000], Step [6/8], train_loss: 183.1781\n",
            "Epoch [1300/2000], Step [8/8], train_loss: 282.3886\n",
            "Val Loss:  362.92818387349445\n",
            "------------------------------EPOCH: 1301 ------------------------\n",
            "Epoch [1301/2000], Step [2/8], train_loss: 388.7182\n",
            "Epoch [1301/2000], Step [4/8], train_loss: 626.1653\n",
            "Epoch [1301/2000], Step [6/8], train_loss: 183.0837\n",
            "Epoch [1301/2000], Step [8/8], train_loss: 282.6092\n",
            "Val Loss:  271.1644039154053\n",
            "------------------------------EPOCH: 1302 ------------------------\n",
            "Epoch [1302/2000], Step [2/8], train_loss: 391.8816\n",
            "Epoch [1302/2000], Step [4/8], train_loss: 623.6348\n",
            "Epoch [1302/2000], Step [6/8], train_loss: 184.2711\n",
            "Epoch [1302/2000], Step [8/8], train_loss: 282.2892\n",
            "Val Loss:  353.59265359242755\n",
            "------------------------------EPOCH: 1303 ------------------------\n",
            "Epoch [1303/2000], Step [2/8], train_loss: 387.1536\n",
            "Epoch [1303/2000], Step [4/8], train_loss: 632.0023\n",
            "Epoch [1303/2000], Step [6/8], train_loss: 182.9050\n",
            "Epoch [1303/2000], Step [8/8], train_loss: 282.6772\n",
            "Val Loss:  163.589893023173\n",
            "------------------------------EPOCH: 1304 ------------------------\n",
            "Epoch [1304/2000], Step [2/8], train_loss: 394.1664\n",
            "Epoch [1304/2000], Step [4/8], train_loss: 618.4858\n",
            "Epoch [1304/2000], Step [6/8], train_loss: 183.2688\n",
            "Epoch [1304/2000], Step [8/8], train_loss: 282.2065\n",
            "Val Loss:  337.2428708076477\n",
            "------------------------------EPOCH: 1305 ------------------------\n",
            "Epoch [1305/2000], Step [2/8], train_loss: 386.5382\n",
            "Epoch [1305/2000], Step [4/8], train_loss: 637.4626\n",
            "Epoch [1305/2000], Step [6/8], train_loss: 182.6029\n",
            "Epoch [1305/2000], Step [8/8], train_loss: 282.5555\n",
            "Val Loss:  195.82077010472617\n",
            "------------------------------EPOCH: 1306 ------------------------\n",
            "Epoch [1306/2000], Step [2/8], train_loss: 393.5586\n",
            "Epoch [1306/2000], Step [4/8], train_loss: 616.1395\n",
            "Epoch [1306/2000], Step [6/8], train_loss: 183.3290\n",
            "Epoch [1306/2000], Step [8/8], train_loss: 282.1523\n",
            "Val Loss:  357.47379144032794\n",
            "------------------------------EPOCH: 1307 ------------------------\n",
            "Epoch [1307/2000], Step [2/8], train_loss: 385.8231\n",
            "Epoch [1307/2000], Step [4/8], train_loss: 635.7948\n",
            "Epoch [1307/2000], Step [6/8], train_loss: 183.8610\n",
            "Epoch [1307/2000], Step [8/8], train_loss: 282.4138\n",
            "Val Loss:  221.34614896774292\n",
            "------------------------------EPOCH: 1308 ------------------------\n",
            "Epoch [1308/2000], Step [2/8], train_loss: 390.7738\n",
            "Epoch [1308/2000], Step [4/8], train_loss: 625.4858\n",
            "Epoch [1308/2000], Step [6/8], train_loss: 185.0833\n",
            "Epoch [1308/2000], Step [8/8], train_loss: 282.0302\n",
            "Val Loss:  498.33050537109375\n",
            "------------------------------EPOCH: 1309 ------------------------\n",
            "Epoch [1309/2000], Step [2/8], train_loss: 385.1046\n",
            "Epoch [1309/2000], Step [4/8], train_loss: 643.4469\n",
            "Epoch [1309/2000], Step [6/8], train_loss: 183.7670\n",
            "Epoch [1309/2000], Step [8/8], train_loss: 282.4547\n",
            "Val Loss:  253.17546955744425\n",
            "------------------------------EPOCH: 1310 ------------------------\n",
            "Epoch [1310/2000], Step [2/8], train_loss: 394.0278\n",
            "Epoch [1310/2000], Step [4/8], train_loss: 612.7173\n",
            "Epoch [1310/2000], Step [6/8], train_loss: 183.3823\n",
            "Epoch [1310/2000], Step [8/8], train_loss: 282.0876\n",
            "Val Loss:  325.5673745473226\n",
            "------------------------------EPOCH: 1311 ------------------------\n",
            "Epoch [1311/2000], Step [2/8], train_loss: 386.0287\n",
            "Epoch [1311/2000], Step [4/8], train_loss: 638.6521\n",
            "Epoch [1311/2000], Step [6/8], train_loss: 183.2739\n",
            "Epoch [1311/2000], Step [8/8], train_loss: 282.3679\n",
            "Val Loss:  177.06406191984811\n",
            "------------------------------EPOCH: 1312 ------------------------\n",
            "Epoch [1312/2000], Step [2/8], train_loss: 391.1311\n",
            "Epoch [1312/2000], Step [4/8], train_loss: 623.1888\n",
            "Epoch [1312/2000], Step [6/8], train_loss: 184.2933\n",
            "Epoch [1312/2000], Step [8/8], train_loss: 282.0477\n",
            "Val Loss:  344.9087775548299\n",
            "------------------------------EPOCH: 1313 ------------------------\n",
            "Epoch [1313/2000], Step [2/8], train_loss: 385.0941\n",
            "Epoch [1313/2000], Step [4/8], train_loss: 640.6270\n",
            "Epoch [1313/2000], Step [6/8], train_loss: 183.4865\n",
            "Epoch [1313/2000], Step [8/8], train_loss: 282.4005\n",
            "Val Loss:  197.4364274342855\n",
            "------------------------------EPOCH: 1314 ------------------------\n",
            "Epoch [1314/2000], Step [2/8], train_loss: 391.8721\n",
            "Epoch [1314/2000], Step [4/8], train_loss: 615.9349\n",
            "Epoch [1314/2000], Step [6/8], train_loss: 183.4295\n",
            "Epoch [1314/2000], Step [8/8], train_loss: 282.1070\n",
            "Val Loss:  376.8873167037964\n",
            "------------------------------EPOCH: 1315 ------------------------\n",
            "Epoch [1315/2000], Step [2/8], train_loss: 386.1496\n",
            "Epoch [1315/2000], Step [4/8], train_loss: 634.5363\n",
            "Epoch [1315/2000], Step [6/8], train_loss: 182.9472\n",
            "Epoch [1315/2000], Step [8/8], train_loss: 282.3887\n",
            "Val Loss:  248.218985080719\n",
            "------------------------------EPOCH: 1316 ------------------------\n",
            "Epoch [1316/2000], Step [2/8], train_loss: 391.3871\n",
            "Epoch [1316/2000], Step [4/8], train_loss: 615.8528\n",
            "Epoch [1316/2000], Step [6/8], train_loss: 183.2847\n",
            "Epoch [1316/2000], Step [8/8], train_loss: 282.2258\n",
            "Val Loss:  345.4753112792969\n",
            "------------------------------EPOCH: 1317 ------------------------\n",
            "Epoch [1317/2000], Step [2/8], train_loss: 386.7450\n",
            "Epoch [1317/2000], Step [4/8], train_loss: 629.2499\n",
            "Epoch [1317/2000], Step [6/8], train_loss: 183.2105\n",
            "Epoch [1317/2000], Step [8/8], train_loss: 282.4232\n",
            "Val Loss:  240.56183687845865\n",
            "------------------------------EPOCH: 1318 ------------------------\n",
            "Epoch [1318/2000], Step [2/8], train_loss: 389.8009\n",
            "Epoch [1318/2000], Step [4/8], train_loss: 626.1222\n",
            "Epoch [1318/2000], Step [6/8], train_loss: 183.9865\n",
            "Epoch [1318/2000], Step [8/8], train_loss: 282.1605\n",
            "Val Loss:  419.7978232701619\n",
            "------------------------------EPOCH: 1319 ------------------------\n",
            "Epoch [1319/2000], Step [2/8], train_loss: 385.5821\n",
            "Epoch [1319/2000], Step [4/8], train_loss: 634.7543\n",
            "Epoch [1319/2000], Step [6/8], train_loss: 182.6504\n",
            "Epoch [1319/2000], Step [8/8], train_loss: 282.4976\n",
            "Val Loss:  239.1792639096578\n",
            "------------------------------EPOCH: 1320 ------------------------\n",
            "Epoch [1320/2000], Step [2/8], train_loss: 391.8516\n",
            "Epoch [1320/2000], Step [4/8], train_loss: 618.1425\n",
            "Epoch [1320/2000], Step [6/8], train_loss: 182.4572\n",
            "Epoch [1320/2000], Step [8/8], train_loss: 282.1013\n",
            "Val Loss:  373.44223801294964\n",
            "------------------------------EPOCH: 1321 ------------------------\n",
            "Epoch [1321/2000], Step [2/8], train_loss: 384.1729\n",
            "Epoch [1321/2000], Step [4/8], train_loss: 637.5117\n",
            "Epoch [1321/2000], Step [6/8], train_loss: 183.2578\n",
            "Epoch [1321/2000], Step [8/8], train_loss: 282.4668\n",
            "Val Loss:  220.86250233650208\n",
            "------------------------------EPOCH: 1322 ------------------------\n",
            "Epoch [1322/2000], Step [2/8], train_loss: 390.2877\n",
            "Epoch [1322/2000], Step [4/8], train_loss: 617.7440\n",
            "Epoch [1322/2000], Step [6/8], train_loss: 183.9645\n",
            "Epoch [1322/2000], Step [8/8], train_loss: 282.2505\n",
            "Val Loss:  357.5154905319214\n",
            "------------------------------EPOCH: 1323 ------------------------\n",
            "Epoch [1323/2000], Step [2/8], train_loss: 385.2875\n",
            "Epoch [1323/2000], Step [4/8], train_loss: 632.0900\n",
            "Epoch [1323/2000], Step [6/8], train_loss: 183.1235\n",
            "Epoch [1323/2000], Step [8/8], train_loss: 282.4578\n",
            "Val Loss:  289.00311549504596\n",
            "------------------------------EPOCH: 1324 ------------------------\n",
            "Epoch [1324/2000], Step [2/8], train_loss: 389.1579\n",
            "Epoch [1324/2000], Step [4/8], train_loss: 617.0217\n",
            "Epoch [1324/2000], Step [6/8], train_loss: 183.2572\n",
            "Epoch [1324/2000], Step [8/8], train_loss: 282.3611\n",
            "Val Loss:  414.931902885437\n",
            "------------------------------EPOCH: 1325 ------------------------\n",
            "Epoch [1325/2000], Step [2/8], train_loss: 385.8700\n",
            "Epoch [1325/2000], Step [4/8], train_loss: 629.0815\n",
            "Epoch [1325/2000], Step [6/8], train_loss: 183.1690\n",
            "Epoch [1325/2000], Step [8/8], train_loss: 282.5442\n",
            "Val Loss:  293.61561648050946\n",
            "------------------------------EPOCH: 1326 ------------------------\n",
            "Epoch [1326/2000], Step [2/8], train_loss: 388.4054\n",
            "Epoch [1326/2000], Step [4/8], train_loss: 625.6438\n",
            "Epoch [1326/2000], Step [6/8], train_loss: 183.7702\n",
            "Epoch [1326/2000], Step [8/8], train_loss: 282.3409\n",
            "Val Loss:  346.42499748865765\n",
            "------------------------------EPOCH: 1327 ------------------------\n",
            "Epoch [1327/2000], Step [2/8], train_loss: 385.1841\n",
            "Epoch [1327/2000], Step [4/8], train_loss: 632.9897\n",
            "Epoch [1327/2000], Step [6/8], train_loss: 182.3387\n",
            "Epoch [1327/2000], Step [8/8], train_loss: 282.6690\n",
            "Val Loss:  185.74368619918823\n",
            "------------------------------EPOCH: 1328 ------------------------\n",
            "Epoch [1328/2000], Step [2/8], train_loss: 390.7883\n",
            "Epoch [1328/2000], Step [4/8], train_loss: 619.9939\n",
            "Epoch [1328/2000], Step [6/8], train_loss: 182.1994\n",
            "Epoch [1328/2000], Step [8/8], train_loss: 282.2785\n",
            "Val Loss:  381.23129145304364\n",
            "------------------------------EPOCH: 1329 ------------------------\n",
            "Epoch [1329/2000], Step [2/8], train_loss: 383.5607\n",
            "Epoch [1329/2000], Step [4/8], train_loss: 637.0027\n",
            "Epoch [1329/2000], Step [6/8], train_loss: 182.7579\n",
            "Epoch [1329/2000], Step [8/8], train_loss: 282.6036\n",
            "Val Loss:  255.74728441238403\n",
            "------------------------------EPOCH: 1330 ------------------------\n",
            "Epoch [1330/2000], Step [2/8], train_loss: 389.7659\n",
            "Epoch [1330/2000], Step [4/8], train_loss: 619.3357\n",
            "Epoch [1330/2000], Step [6/8], train_loss: 183.7039\n",
            "Epoch [1330/2000], Step [8/8], train_loss: 282.3651\n",
            "Val Loss:  408.74197006225586\n",
            "------------------------------EPOCH: 1331 ------------------------\n",
            "Epoch [1331/2000], Step [2/8], train_loss: 384.4787\n",
            "Epoch [1331/2000], Step [4/8], train_loss: 632.8898\n",
            "Epoch [1331/2000], Step [6/8], train_loss: 183.0042\n",
            "Epoch [1331/2000], Step [8/8], train_loss: 282.5862\n",
            "Val Loss:  317.15688101450604\n",
            "------------------------------EPOCH: 1332 ------------------------\n",
            "Epoch [1332/2000], Step [2/8], train_loss: 388.2455\n",
            "Epoch [1332/2000], Step [4/8], train_loss: 618.2772\n",
            "Epoch [1332/2000], Step [6/8], train_loss: 183.3243\n",
            "Epoch [1332/2000], Step [8/8], train_loss: 282.4730\n",
            "Val Loss:  422.5227138201396\n",
            "------------------------------EPOCH: 1333 ------------------------\n",
            "Epoch [1333/2000], Step [2/8], train_loss: 384.6989\n",
            "Epoch [1333/2000], Step [4/8], train_loss: 629.9047\n",
            "Epoch [1333/2000], Step [6/8], train_loss: 183.1553\n",
            "Epoch [1333/2000], Step [8/8], train_loss: 282.6760\n",
            "Val Loss:  301.75249497095746\n",
            "------------------------------EPOCH: 1334 ------------------------\n",
            "Epoch [1334/2000], Step [2/8], train_loss: 387.5422\n",
            "Epoch [1334/2000], Step [4/8], train_loss: 627.1213\n",
            "Epoch [1334/2000], Step [6/8], train_loss: 183.7982\n",
            "Epoch [1334/2000], Step [8/8], train_loss: 282.4468\n",
            "Val Loss:  436.02206802368164\n",
            "------------------------------EPOCH: 1335 ------------------------\n",
            "Epoch [1335/2000], Step [2/8], train_loss: 384.1513\n",
            "Epoch [1335/2000], Step [4/8], train_loss: 634.5292\n",
            "Epoch [1335/2000], Step [6/8], train_loss: 182.4308\n",
            "Epoch [1335/2000], Step [8/8], train_loss: 282.7998\n",
            "Val Loss:  233.68984492619833\n",
            "------------------------------EPOCH: 1336 ------------------------\n",
            "Epoch [1336/2000], Step [2/8], train_loss: 390.2362\n",
            "Epoch [1336/2000], Step [4/8], train_loss: 619.8419\n",
            "Epoch [1336/2000], Step [6/8], train_loss: 182.5132\n",
            "Epoch [1336/2000], Step [8/8], train_loss: 282.4256\n",
            "Val Loss:  390.3617474238078\n",
            "------------------------------EPOCH: 1337 ------------------------\n",
            "Epoch [1337/2000], Step [2/8], train_loss: 383.5150\n",
            "Epoch [1337/2000], Step [4/8], train_loss: 637.2991\n",
            "Epoch [1337/2000], Step [6/8], train_loss: 182.1023\n",
            "Epoch [1337/2000], Step [8/8], train_loss: 282.7801\n",
            "Val Loss:  244.56991704305014\n",
            "------------------------------EPOCH: 1338 ------------------------\n",
            "Epoch [1338/2000], Step [2/8], train_loss: 390.1165\n",
            "Epoch [1338/2000], Step [4/8], train_loss: 619.9980\n",
            "Epoch [1338/2000], Step [6/8], train_loss: 182.7693\n",
            "Epoch [1338/2000], Step [8/8], train_loss: 282.5363\n",
            "Val Loss:  456.2750957806905\n",
            "------------------------------EPOCH: 1339 ------------------------\n",
            "Epoch [1339/2000], Step [2/8], train_loss: 384.5142\n",
            "Epoch [1339/2000], Step [4/8], train_loss: 634.8093\n",
            "Epoch [1339/2000], Step [6/8], train_loss: 182.1313\n",
            "Epoch [1339/2000], Step [8/8], train_loss: 282.7781\n",
            "Val Loss:  324.2349262237549\n",
            "------------------------------EPOCH: 1340 ------------------------\n",
            "Epoch [1340/2000], Step [2/8], train_loss: 388.3405\n",
            "Epoch [1340/2000], Step [4/8], train_loss: 619.6561\n",
            "Epoch [1340/2000], Step [6/8], train_loss: 182.3478\n",
            "Epoch [1340/2000], Step [8/8], train_loss: 282.5869\n",
            "Val Loss:  457.74576171239215\n",
            "------------------------------EPOCH: 1341 ------------------------\n",
            "Epoch [1341/2000], Step [2/8], train_loss: 383.6793\n",
            "Epoch [1341/2000], Step [4/8], train_loss: 632.6266\n",
            "Epoch [1341/2000], Step [6/8], train_loss: 182.8171\n",
            "Epoch [1341/2000], Step [8/8], train_loss: 282.7961\n",
            "Val Loss:  277.2450467745463\n",
            "------------------------------EPOCH: 1342 ------------------------\n",
            "Epoch [1342/2000], Step [2/8], train_loss: 387.0578\n",
            "Epoch [1342/2000], Step [4/8], train_loss: 624.5990\n",
            "Epoch [1342/2000], Step [6/8], train_loss: 183.3439\n",
            "Epoch [1342/2000], Step [8/8], train_loss: 282.5488\n",
            "Val Loss:  359.5964641571045\n",
            "------------------------------EPOCH: 1343 ------------------------\n",
            "Epoch [1343/2000], Step [2/8], train_loss: 383.2463\n",
            "Epoch [1343/2000], Step [4/8], train_loss: 635.4910\n",
            "Epoch [1343/2000], Step [6/8], train_loss: 182.0697\n",
            "Epoch [1343/2000], Step [8/8], train_loss: 282.7574\n",
            "Val Loss:  276.0994002024333\n",
            "------------------------------EPOCH: 1344 ------------------------\n",
            "Epoch [1344/2000], Step [2/8], train_loss: 387.8621\n",
            "Epoch [1344/2000], Step [4/8], train_loss: 625.0480\n",
            "Epoch [1344/2000], Step [6/8], train_loss: 181.8495\n",
            "Epoch [1344/2000], Step [8/8], train_loss: 282.5705\n",
            "Val Loss:  413.0836504300435\n",
            "------------------------------EPOCH: 1345 ------------------------\n",
            "Epoch [1345/2000], Step [2/8], train_loss: 383.9681\n",
            "Epoch [1345/2000], Step [4/8], train_loss: 633.0844\n",
            "Epoch [1345/2000], Step [6/8], train_loss: 182.6166\n",
            "Epoch [1345/2000], Step [8/8], train_loss: 282.7400\n",
            "Val Loss:  307.7917634646098\n",
            "------------------------------EPOCH: 1346 ------------------------\n",
            "Epoch [1346/2000], Step [2/8], train_loss: 386.1043\n",
            "Epoch [1346/2000], Step [4/8], train_loss: 621.2637\n",
            "Epoch [1346/2000], Step [6/8], train_loss: 183.4907\n",
            "Epoch [1346/2000], Step [8/8], train_loss: 282.6173\n",
            "Val Loss:  426.66875489552814\n",
            "------------------------------EPOCH: 1347 ------------------------\n",
            "Epoch [1347/2000], Step [2/8], train_loss: 383.1461\n",
            "Epoch [1347/2000], Step [4/8], train_loss: 630.0273\n",
            "Epoch [1347/2000], Step [6/8], train_loss: 183.0291\n",
            "Epoch [1347/2000], Step [8/8], train_loss: 282.8809\n",
            "Val Loss:  261.93263880411786\n",
            "------------------------------EPOCH: 1348 ------------------------\n",
            "Epoch [1348/2000], Step [2/8], train_loss: 386.8226\n",
            "Epoch [1348/2000], Step [4/8], train_loss: 628.9142\n",
            "Epoch [1348/2000], Step [6/8], train_loss: 183.7935\n",
            "Epoch [1348/2000], Step [8/8], train_loss: 282.6241\n",
            "Val Loss:  387.8591005007426\n",
            "------------------------------EPOCH: 1349 ------------------------\n",
            "Epoch [1349/2000], Step [2/8], train_loss: 382.5952\n",
            "Epoch [1349/2000], Step [4/8], train_loss: 635.8758\n",
            "Epoch [1349/2000], Step [6/8], train_loss: 182.0772\n",
            "Epoch [1349/2000], Step [8/8], train_loss: 283.0182\n",
            "Val Loss:  202.45668093363443\n",
            "------------------------------EPOCH: 1350 ------------------------\n",
            "Epoch [1350/2000], Step [2/8], train_loss: 389.0623\n",
            "Epoch [1350/2000], Step [4/8], train_loss: 622.7728\n",
            "Epoch [1350/2000], Step [6/8], train_loss: 182.5435\n",
            "Epoch [1350/2000], Step [8/8], train_loss: 282.5805\n",
            "Val Loss:  374.7042478720347\n",
            "------------------------------EPOCH: 1351 ------------------------\n",
            "Epoch [1351/2000], Step [2/8], train_loss: 381.6966\n",
            "Epoch [1351/2000], Step [4/8], train_loss: 641.3875\n",
            "Epoch [1351/2000], Step [6/8], train_loss: 182.1053\n",
            "Epoch [1351/2000], Step [8/8], train_loss: 282.9315\n",
            "Val Loss:  217.10202407836914\n",
            "------------------------------EPOCH: 1352 ------------------------\n",
            "Epoch [1352/2000], Step [2/8], train_loss: 388.4059\n",
            "Epoch [1352/2000], Step [4/8], train_loss: 621.9611\n",
            "Epoch [1352/2000], Step [6/8], train_loss: 182.9817\n",
            "Epoch [1352/2000], Step [8/8], train_loss: 282.5959\n",
            "Val Loss:  418.2561233838399\n",
            "------------------------------EPOCH: 1353 ------------------------\n",
            "Epoch [1353/2000], Step [2/8], train_loss: 382.2198\n",
            "Epoch [1353/2000], Step [4/8], train_loss: 637.4150\n",
            "Epoch [1353/2000], Step [6/8], train_loss: 182.6586\n",
            "Epoch [1353/2000], Step [8/8], train_loss: 282.8380\n",
            "Val Loss:  277.20859241485596\n",
            "------------------------------EPOCH: 1354 ------------------------\n",
            "Epoch [1354/2000], Step [2/8], train_loss: 386.7037\n",
            "Epoch [1354/2000], Step [4/8], train_loss: 618.8907\n",
            "Epoch [1354/2000], Step [6/8], train_loss: 183.0508\n",
            "Epoch [1354/2000], Step [8/8], train_loss: 282.6494\n",
            "Val Loss:  371.79782342910767\n",
            "------------------------------EPOCH: 1355 ------------------------\n",
            "Epoch [1355/2000], Step [2/8], train_loss: 382.3323\n",
            "Epoch [1355/2000], Step [4/8], train_loss: 634.0336\n",
            "Epoch [1355/2000], Step [6/8], train_loss: 182.8179\n",
            "Epoch [1355/2000], Step [8/8], train_loss: 282.8676\n",
            "Val Loss:  229.75427118937174\n",
            "------------------------------EPOCH: 1356 ------------------------\n",
            "Epoch [1356/2000], Step [2/8], train_loss: 385.6461\n",
            "Epoch [1356/2000], Step [4/8], train_loss: 627.4310\n",
            "Epoch [1356/2000], Step [6/8], train_loss: 183.5311\n",
            "Epoch [1356/2000], Step [8/8], train_loss: 282.6259\n",
            "Val Loss:  360.4376862843831\n",
            "------------------------------EPOCH: 1357 ------------------------\n",
            "Epoch [1357/2000], Step [2/8], train_loss: 381.9633\n",
            "Epoch [1357/2000], Step [4/8], train_loss: 638.1752\n",
            "Epoch [1357/2000], Step [6/8], train_loss: 182.2415\n",
            "Epoch [1357/2000], Step [8/8], train_loss: 282.9758\n",
            "Val Loss:  196.86728191375732\n",
            "------------------------------EPOCH: 1358 ------------------------\n",
            "Epoch [1358/2000], Step [2/8], train_loss: 387.9222\n",
            "Epoch [1358/2000], Step [4/8], train_loss: 620.3314\n",
            "Epoch [1358/2000], Step [6/8], train_loss: 182.2432\n",
            "Epoch [1358/2000], Step [8/8], train_loss: 282.6411\n",
            "Val Loss:  341.40722672144574\n",
            "------------------------------EPOCH: 1359 ------------------------\n",
            "Epoch [1359/2000], Step [2/8], train_loss: 381.4712\n",
            "Epoch [1359/2000], Step [4/8], train_loss: 637.8515\n",
            "Epoch [1359/2000], Step [6/8], train_loss: 181.9896\n",
            "Epoch [1359/2000], Step [8/8], train_loss: 283.0312\n",
            "Val Loss:  203.99275143941244\n",
            "------------------------------EPOCH: 1360 ------------------------\n",
            "Epoch [1360/2000], Step [2/8], train_loss: 387.3956\n",
            "Epoch [1360/2000], Step [4/8], train_loss: 620.9784\n",
            "Epoch [1360/2000], Step [6/8], train_loss: 182.5319\n",
            "Epoch [1360/2000], Step [8/8], train_loss: 282.8262\n",
            "Val Loss:  330.7771665255229\n",
            "------------------------------EPOCH: 1361 ------------------------\n",
            "Epoch [1361/2000], Step [2/8], train_loss: 382.3506\n",
            "Epoch [1361/2000], Step [4/8], train_loss: 635.1825\n",
            "Epoch [1361/2000], Step [6/8], train_loss: 181.8491\n",
            "Epoch [1361/2000], Step [8/8], train_loss: 283.0799\n",
            "Val Loss:  227.74581217765808\n",
            "------------------------------EPOCH: 1362 ------------------------\n",
            "Epoch [1362/2000], Step [2/8], train_loss: 386.0983\n",
            "Epoch [1362/2000], Step [4/8], train_loss: 632.9878\n",
            "Epoch [1362/2000], Step [6/8], train_loss: 182.6991\n",
            "Epoch [1362/2000], Step [8/8], train_loss: 282.7219\n",
            "Val Loss:  289.9679028193156\n",
            "------------------------------EPOCH: 1363 ------------------------\n",
            "Epoch [1363/2000], Step [2/8], train_loss: 380.3761\n",
            "Epoch [1363/2000], Step [4/8], train_loss: 642.0433\n",
            "Epoch [1363/2000], Step [6/8], train_loss: 181.9580\n",
            "Epoch [1363/2000], Step [8/8], train_loss: 283.0720\n",
            "Val Loss:  148.05326890945435\n",
            "------------------------------EPOCH: 1364 ------------------------\n",
            "Epoch [1364/2000], Step [2/8], train_loss: 387.2156\n",
            "Epoch [1364/2000], Step [4/8], train_loss: 619.8810\n",
            "Epoch [1364/2000], Step [6/8], train_loss: 182.4456\n",
            "Epoch [1364/2000], Step [8/8], train_loss: 282.6226\n",
            "Val Loss:  331.42151141166687\n",
            "------------------------------EPOCH: 1365 ------------------------\n",
            "Epoch [1365/2000], Step [2/8], train_loss: 379.8052\n",
            "Epoch [1365/2000], Step [4/8], train_loss: 643.0162\n",
            "Epoch [1365/2000], Step [6/8], train_loss: 182.2979\n",
            "Epoch [1365/2000], Step [8/8], train_loss: 282.9471\n",
            "Val Loss:  213.83756987253824\n",
            "------------------------------EPOCH: 1366 ------------------------\n",
            "Epoch [1366/2000], Step [2/8], train_loss: 386.1642\n",
            "Epoch [1366/2000], Step [4/8], train_loss: 619.0607\n",
            "Epoch [1366/2000], Step [6/8], train_loss: 182.8362\n",
            "Epoch [1366/2000], Step [8/8], train_loss: 282.7079\n",
            "Val Loss:  381.0455107688904\n",
            "------------------------------EPOCH: 1367 ------------------------\n",
            "Epoch [1367/2000], Step [2/8], train_loss: 380.3918\n",
            "Epoch [1367/2000], Step [4/8], train_loss: 637.9481\n",
            "Epoch [1367/2000], Step [6/8], train_loss: 182.8336\n",
            "Epoch [1367/2000], Step [8/8], train_loss: 282.9561\n",
            "Val Loss:  255.1398696899414\n",
            "------------------------------EPOCH: 1368 ------------------------\n",
            "Epoch [1368/2000], Step [2/8], train_loss: 384.5460\n",
            "Epoch [1368/2000], Step [4/8], train_loss: 627.8162\n",
            "Epoch [1368/2000], Step [6/8], train_loss: 183.6387\n",
            "Epoch [1368/2000], Step [8/8], train_loss: 282.6890\n",
            "Val Loss:  430.86661593119305\n",
            "------------------------------EPOCH: 1369 ------------------------\n",
            "Epoch [1369/2000], Step [2/8], train_loss: 380.0334\n",
            "Epoch [1369/2000], Step [4/8], train_loss: 643.3259\n",
            "Epoch [1369/2000], Step [6/8], train_loss: 182.2257\n",
            "Epoch [1369/2000], Step [8/8], train_loss: 283.0356\n",
            "Val Loss:  244.23257207870483\n",
            "------------------------------EPOCH: 1370 ------------------------\n",
            "Epoch [1370/2000], Step [2/8], train_loss: 386.2155\n",
            "Epoch [1370/2000], Step [4/8], train_loss: 620.3855\n",
            "Epoch [1370/2000], Step [6/8], train_loss: 181.9544\n",
            "Epoch [1370/2000], Step [8/8], train_loss: 282.7344\n",
            "Val Loss:  327.13828802108765\n",
            "------------------------------EPOCH: 1371 ------------------------\n",
            "Epoch [1371/2000], Step [2/8], train_loss: 380.1375\n",
            "Epoch [1371/2000], Step [4/8], train_loss: 638.6470\n",
            "Epoch [1371/2000], Step [6/8], train_loss: 181.7782\n",
            "Epoch [1371/2000], Step [8/8], train_loss: 283.0468\n",
            "Val Loss:  197.76971673965454\n",
            "------------------------------EPOCH: 1372 ------------------------\n",
            "Epoch [1372/2000], Step [2/8], train_loss: 385.6051\n",
            "Epoch [1372/2000], Step [4/8], train_loss: 621.6754\n",
            "Epoch [1372/2000], Step [6/8], train_loss: 182.3261\n",
            "Epoch [1372/2000], Step [8/8], train_loss: 282.8919\n",
            "Val Loss:  257.61077276865643\n",
            "------------------------------EPOCH: 1373 ------------------------\n",
            "Epoch [1373/2000], Step [2/8], train_loss: 381.2772\n",
            "Epoch [1373/2000], Step [4/8], train_loss: 635.7040\n",
            "Epoch [1373/2000], Step [6/8], train_loss: 181.9212\n",
            "Epoch [1373/2000], Step [8/8], train_loss: 283.1099\n",
            "Val Loss:  206.19879142443338\n",
            "------------------------------EPOCH: 1374 ------------------------\n",
            "Epoch [1374/2000], Step [2/8], train_loss: 384.4231\n",
            "Epoch [1374/2000], Step [4/8], train_loss: 631.4258\n",
            "Epoch [1374/2000], Step [6/8], train_loss: 182.8420\n",
            "Epoch [1374/2000], Step [8/8], train_loss: 282.8299\n",
            "Val Loss:  368.65624992052716\n",
            "------------------------------EPOCH: 1375 ------------------------\n",
            "Epoch [1375/2000], Step [2/8], train_loss: 379.7645\n",
            "Epoch [1375/2000], Step [4/8], train_loss: 640.4741\n",
            "Epoch [1375/2000], Step [6/8], train_loss: 181.5975\n",
            "Epoch [1375/2000], Step [8/8], train_loss: 283.1627\n",
            "Val Loss:  218.60233958562216\n",
            "------------------------------EPOCH: 1376 ------------------------\n",
            "Epoch [1376/2000], Step [2/8], train_loss: 385.6864\n",
            "Epoch [1376/2000], Step [4/8], train_loss: 622.8774\n",
            "Epoch [1376/2000], Step [6/8], train_loss: 181.6670\n",
            "Epoch [1376/2000], Step [8/8], train_loss: 282.7401\n",
            "Val Loss:  414.9339057604472\n",
            "------------------------------EPOCH: 1377 ------------------------\n",
            "Epoch [1377/2000], Step [2/8], train_loss: 378.3327\n",
            "Epoch [1377/2000], Step [4/8], train_loss: 641.9144\n",
            "Epoch [1377/2000], Step [6/8], train_loss: 182.1705\n",
            "Epoch [1377/2000], Step [8/8], train_loss: 283.0461\n",
            "Val Loss:  254.24172655741373\n",
            "------------------------------EPOCH: 1378 ------------------------\n",
            "Epoch [1378/2000], Step [2/8], train_loss: 384.4478\n",
            "Epoch [1378/2000], Step [4/8], train_loss: 621.9401\n",
            "Epoch [1378/2000], Step [6/8], train_loss: 183.2823\n",
            "Epoch [1378/2000], Step [8/8], train_loss: 282.8099\n",
            "Val Loss:  419.58250459035236\n",
            "------------------------------EPOCH: 1379 ------------------------\n",
            "Epoch [1379/2000], Step [2/8], train_loss: 379.4836\n",
            "Epoch [1379/2000], Step [4/8], train_loss: 638.4240\n",
            "Epoch [1379/2000], Step [6/8], train_loss: 182.4788\n",
            "Epoch [1379/2000], Step [8/8], train_loss: 283.0565\n",
            "Val Loss:  295.26267608006793\n",
            "------------------------------EPOCH: 1380 ------------------------\n",
            "Epoch [1380/2000], Step [2/8], train_loss: 383.3949\n",
            "Epoch [1380/2000], Step [4/8], train_loss: 631.8152\n",
            "Epoch [1380/2000], Step [6/8], train_loss: 182.8559\n",
            "Epoch [1380/2000], Step [8/8], train_loss: 282.7585\n",
            "Val Loss:  447.57719866434735\n",
            "------------------------------EPOCH: 1381 ------------------------\n",
            "Epoch [1381/2000], Step [2/8], train_loss: 378.1944\n",
            "Epoch [1381/2000], Step [4/8], train_loss: 643.1279\n",
            "Epoch [1381/2000], Step [6/8], train_loss: 182.1020\n",
            "Epoch [1381/2000], Step [8/8], train_loss: 283.1263\n",
            "Val Loss:  230.39296261469522\n",
            "------------------------------EPOCH: 1382 ------------------------\n",
            "Epoch [1382/2000], Step [2/8], train_loss: 384.7374\n",
            "Epoch [1382/2000], Step [4/8], train_loss: 621.6861\n",
            "Epoch [1382/2000], Step [6/8], train_loss: 182.7042\n",
            "Epoch [1382/2000], Step [8/8], train_loss: 282.7602\n",
            "Val Loss:  412.7194010416667\n",
            "------------------------------EPOCH: 1383 ------------------------\n",
            "Epoch [1383/2000], Step [2/8], train_loss: 378.2835\n",
            "Epoch [1383/2000], Step [4/8], train_loss: 641.6967\n",
            "Epoch [1383/2000], Step [6/8], train_loss: 181.9175\n",
            "Epoch [1383/2000], Step [8/8], train_loss: 283.0600\n",
            "Val Loss:  255.018981218338\n",
            "------------------------------EPOCH: 1384 ------------------------\n",
            "Epoch [1384/2000], Step [2/8], train_loss: 384.0560\n",
            "Epoch [1384/2000], Step [4/8], train_loss: 621.4730\n",
            "Epoch [1384/2000], Step [6/8], train_loss: 181.9524\n",
            "Epoch [1384/2000], Step [8/8], train_loss: 282.9007\n",
            "Val Loss:  338.3002781867981\n",
            "------------------------------EPOCH: 1385 ------------------------\n",
            "Epoch [1385/2000], Step [2/8], train_loss: 379.6204\n",
            "Epoch [1385/2000], Step [4/8], train_loss: 636.2220\n",
            "Epoch [1385/2000], Step [6/8], train_loss: 181.8843\n",
            "Epoch [1385/2000], Step [8/8], train_loss: 283.1226\n",
            "Val Loss:  249.02607027689615\n",
            "------------------------------EPOCH: 1386 ------------------------\n",
            "Epoch [1386/2000], Step [2/8], train_loss: 382.5742\n",
            "Epoch [1386/2000], Step [4/8], train_loss: 629.9589\n",
            "Epoch [1386/2000], Step [6/8], train_loss: 182.9420\n",
            "Epoch [1386/2000], Step [8/8], train_loss: 282.8672\n",
            "Val Loss:  428.7162609100342\n",
            "------------------------------EPOCH: 1387 ------------------------\n",
            "Epoch [1387/2000], Step [2/8], train_loss: 378.0776\n",
            "Epoch [1387/2000], Step [4/8], train_loss: 639.8186\n",
            "Epoch [1387/2000], Step [6/8], train_loss: 181.7655\n",
            "Epoch [1387/2000], Step [8/8], train_loss: 283.2216\n",
            "Val Loss:  261.0920666058858\n",
            "------------------------------EPOCH: 1388 ------------------------\n",
            "Epoch [1388/2000], Step [2/8], train_loss: 383.9459\n",
            "Epoch [1388/2000], Step [4/8], train_loss: 623.4482\n",
            "Epoch [1388/2000], Step [6/8], train_loss: 182.0300\n",
            "Epoch [1388/2000], Step [8/8], train_loss: 282.8755\n",
            "Val Loss:  375.6452563603719\n",
            "------------------------------EPOCH: 1389 ------------------------\n",
            "Epoch [1389/2000], Step [2/8], train_loss: 378.1277\n",
            "Epoch [1389/2000], Step [4/8], train_loss: 641.4615\n",
            "Epoch [1389/2000], Step [6/8], train_loss: 181.3278\n",
            "Epoch [1389/2000], Step [8/8], train_loss: 283.1490\n",
            "Val Loss:  250.52814928690592\n",
            "------------------------------EPOCH: 1390 ------------------------\n",
            "Epoch [1390/2000], Step [2/8], train_loss: 383.3230\n",
            "Epoch [1390/2000], Step [4/8], train_loss: 622.9193\n",
            "Epoch [1390/2000], Step [6/8], train_loss: 181.7202\n",
            "Epoch [1390/2000], Step [8/8], train_loss: 282.8937\n",
            "Val Loss:  450.7852412859599\n",
            "------------------------------EPOCH: 1391 ------------------------\n",
            "Epoch [1391/2000], Step [2/8], train_loss: 378.5187\n",
            "Epoch [1391/2000], Step [4/8], train_loss: 639.9079\n",
            "Epoch [1391/2000], Step [6/8], train_loss: 182.0221\n",
            "Epoch [1391/2000], Step [8/8], train_loss: 283.1244\n",
            "Val Loss:  288.26211373011273\n",
            "------------------------------EPOCH: 1392 ------------------------\n",
            "Epoch [1392/2000], Step [2/8], train_loss: 381.9908\n",
            "Epoch [1392/2000], Step [4/8], train_loss: 629.0825\n",
            "Epoch [1392/2000], Step [6/8], train_loss: 183.3375\n",
            "Epoch [1392/2000], Step [8/8], train_loss: 282.7983\n",
            "Val Loss:  466.5243983268738\n",
            "------------------------------EPOCH: 1393 ------------------------\n",
            "Epoch [1393/2000], Step [2/8], train_loss: 376.5895\n",
            "Epoch [1393/2000], Step [4/8], train_loss: 644.6846\n",
            "Epoch [1393/2000], Step [6/8], train_loss: 182.1382\n",
            "Epoch [1393/2000], Step [8/8], train_loss: 283.1847\n",
            "Val Loss:  277.6387364069621\n",
            "------------------------------EPOCH: 1394 ------------------------\n",
            "Epoch [1394/2000], Step [2/8], train_loss: 383.3397\n",
            "Epoch [1394/2000], Step [4/8], train_loss: 621.4002\n",
            "Epoch [1394/2000], Step [6/8], train_loss: 182.0266\n",
            "Epoch [1394/2000], Step [8/8], train_loss: 282.8734\n",
            "Val Loss:  460.3900736172994\n",
            "------------------------------EPOCH: 1395 ------------------------\n",
            "Epoch [1395/2000], Step [2/8], train_loss: 377.3575\n",
            "Epoch [1395/2000], Step [4/8], train_loss: 641.9973\n",
            "Epoch [1395/2000], Step [6/8], train_loss: 181.3990\n",
            "Epoch [1395/2000], Step [8/8], train_loss: 283.2230\n",
            "Val Loss:  276.05481656392413\n",
            "------------------------------EPOCH: 1396 ------------------------\n",
            "Epoch [1396/2000], Step [2/8], train_loss: 383.3777\n",
            "Epoch [1396/2000], Step [4/8], train_loss: 632.7862\n",
            "Epoch [1396/2000], Step [6/8], train_loss: 182.3094\n",
            "Epoch [1396/2000], Step [8/8], train_loss: 282.9017\n",
            "Val Loss:  552.4953295389811\n",
            "------------------------------EPOCH: 1397 ------------------------\n",
            "Epoch [1397/2000], Step [2/8], train_loss: 377.2398\n",
            "Epoch [1397/2000], Step [4/8], train_loss: 648.5026\n",
            "Epoch [1397/2000], Step [6/8], train_loss: 180.9523\n",
            "Epoch [1397/2000], Step [8/8], train_loss: 283.2693\n",
            "Val Loss:  340.93766562143963\n",
            "------------------------------EPOCH: 1398 ------------------------\n",
            "Epoch [1398/2000], Step [2/8], train_loss: 383.7547\n",
            "Epoch [1398/2000], Step [4/8], train_loss: 624.5386\n",
            "Epoch [1398/2000], Step [6/8], train_loss: 181.0589\n",
            "Epoch [1398/2000], Step [8/8], train_loss: 282.8573\n",
            "Val Loss:  502.09431171417236\n",
            "------------------------------EPOCH: 1399 ------------------------\n",
            "Epoch [1399/2000], Step [2/8], train_loss: 375.7757\n",
            "Epoch [1399/2000], Step [4/8], train_loss: 644.9592\n",
            "Epoch [1399/2000], Step [6/8], train_loss: 182.0892\n",
            "Epoch [1399/2000], Step [8/8], train_loss: 283.1043\n",
            "Val Loss:  285.86928113301593\n",
            "------------------------------EPOCH: 1400 ------------------------\n",
            "Epoch [1400/2000], Step [2/8], train_loss: 380.7450\n",
            "Epoch [1400/2000], Step [4/8], train_loss: 622.3135\n",
            "Epoch [1400/2000], Step [6/8], train_loss: 183.3285\n",
            "Epoch [1400/2000], Step [8/8], train_loss: 282.8807\n",
            "Val Loss:  383.84998718897504\n",
            "------------------------------EPOCH: 1401 ------------------------\n",
            "Epoch [1401/2000], Step [2/8], train_loss: 376.5774\n",
            "Epoch [1401/2000], Step [4/8], train_loss: 639.0875\n",
            "Epoch [1401/2000], Step [6/8], train_loss: 182.4376\n",
            "Epoch [1401/2000], Step [8/8], train_loss: 283.0803\n",
            "Val Loss:  260.02490933736163\n",
            "------------------------------EPOCH: 1402 ------------------------\n",
            "Epoch [1402/2000], Step [2/8], train_loss: 380.2546\n",
            "Epoch [1402/2000], Step [4/8], train_loss: 631.6179\n",
            "Epoch [1402/2000], Step [6/8], train_loss: 182.2029\n",
            "Epoch [1402/2000], Step [8/8], train_loss: 282.8599\n",
            "Val Loss:  351.5933316548665\n",
            "------------------------------EPOCH: 1403 ------------------------\n",
            "Epoch [1403/2000], Step [2/8], train_loss: 376.5049\n",
            "Epoch [1403/2000], Step [4/8], train_loss: 641.8431\n",
            "Epoch [1403/2000], Step [6/8], train_loss: 181.6112\n",
            "Epoch [1403/2000], Step [8/8], train_loss: 283.1909\n",
            "Val Loss:  214.1923291683197\n",
            "------------------------------EPOCH: 1404 ------------------------\n",
            "Epoch [1404/2000], Step [2/8], train_loss: 381.9081\n",
            "Epoch [1404/2000], Step [4/8], train_loss: 622.8943\n",
            "Epoch [1404/2000], Step [6/8], train_loss: 182.2516\n",
            "Epoch [1404/2000], Step [8/8], train_loss: 282.9146\n",
            "Val Loss:  279.210853099823\n",
            "------------------------------EPOCH: 1405 ------------------------\n",
            "Epoch [1405/2000], Step [2/8], train_loss: 376.4868\n",
            "Epoch [1405/2000], Step [4/8], train_loss: 639.9460\n",
            "Epoch [1405/2000], Step [6/8], train_loss: 181.6444\n",
            "Epoch [1405/2000], Step [8/8], train_loss: 283.1706\n",
            "Val Loss:  188.79602094491324\n",
            "------------------------------EPOCH: 1406 ------------------------\n",
            "Epoch [1406/2000], Step [2/8], train_loss: 380.9648\n",
            "Epoch [1406/2000], Step [4/8], train_loss: 623.8536\n",
            "Epoch [1406/2000], Step [6/8], train_loss: 181.5583\n",
            "Epoch [1406/2000], Step [8/8], train_loss: 283.0619\n",
            "Val Loss:  288.89366857210797\n",
            "------------------------------EPOCH: 1407 ------------------------\n",
            "Epoch [1407/2000], Step [2/8], train_loss: 377.5653\n",
            "Epoch [1407/2000], Step [4/8], train_loss: 634.7662\n",
            "Epoch [1407/2000], Step [6/8], train_loss: 181.3651\n",
            "Epoch [1407/2000], Step [8/8], train_loss: 283.2758\n",
            "Val Loss:  219.8622668584188\n",
            "------------------------------EPOCH: 1408 ------------------------\n",
            "Epoch [1408/2000], Step [2/8], train_loss: 380.1617\n",
            "Epoch [1408/2000], Step [4/8], train_loss: 633.2101\n",
            "Epoch [1408/2000], Step [6/8], train_loss: 182.5141\n",
            "Epoch [1408/2000], Step [8/8], train_loss: 283.0255\n",
            "Val Loss:  310.5936466058095\n",
            "------------------------------EPOCH: 1409 ------------------------\n",
            "Epoch [1409/2000], Step [2/8], train_loss: 376.2616\n",
            "Epoch [1409/2000], Step [4/8], train_loss: 641.5645\n",
            "Epoch [1409/2000], Step [6/8], train_loss: 181.0505\n",
            "Epoch [1409/2000], Step [8/8], train_loss: 283.3679\n",
            "Val Loss:  156.5827238559723\n",
            "------------------------------EPOCH: 1410 ------------------------\n",
            "Epoch [1410/2000], Step [2/8], train_loss: 381.9859\n",
            "Epoch [1410/2000], Step [4/8], train_loss: 625.0404\n",
            "Epoch [1410/2000], Step [6/8], train_loss: 181.0631\n",
            "Epoch [1410/2000], Step [8/8], train_loss: 282.9448\n",
            "Val Loss:  285.7137420972188\n",
            "------------------------------EPOCH: 1411 ------------------------\n",
            "Epoch [1411/2000], Step [2/8], train_loss: 374.8987\n",
            "Epoch [1411/2000], Step [4/8], train_loss: 644.3165\n",
            "Epoch [1411/2000], Step [6/8], train_loss: 181.4726\n",
            "Epoch [1411/2000], Step [8/8], train_loss: 283.2621\n",
            "Val Loss:  198.47507866223654\n",
            "------------------------------EPOCH: 1412 ------------------------\n",
            "Epoch [1412/2000], Step [2/8], train_loss: 380.8588\n",
            "Epoch [1412/2000], Step [4/8], train_loss: 624.3202\n",
            "Epoch [1412/2000], Step [6/8], train_loss: 182.6184\n",
            "Epoch [1412/2000], Step [8/8], train_loss: 283.0295\n",
            "Val Loss:  337.52471892038983\n",
            "------------------------------EPOCH: 1413 ------------------------\n",
            "Epoch [1413/2000], Step [2/8], train_loss: 375.9756\n",
            "Epoch [1413/2000], Step [4/8], train_loss: 640.4758\n",
            "Epoch [1413/2000], Step [6/8], train_loss: 181.8185\n",
            "Epoch [1413/2000], Step [8/8], train_loss: 283.2803\n",
            "Val Loss:  211.29688827196756\n",
            "------------------------------EPOCH: 1414 ------------------------\n",
            "Epoch [1414/2000], Step [2/8], train_loss: 379.7133\n",
            "Epoch [1414/2000], Step [4/8], train_loss: 634.7496\n",
            "Epoch [1414/2000], Step [6/8], train_loss: 182.0496\n",
            "Epoch [1414/2000], Step [8/8], train_loss: 283.0134\n",
            "Val Loss:  298.96847677230835\n",
            "------------------------------EPOCH: 1415 ------------------------\n",
            "Epoch [1415/2000], Step [2/8], train_loss: 375.2640\n",
            "Epoch [1415/2000], Step [4/8], train_loss: 645.4149\n",
            "Epoch [1415/2000], Step [6/8], train_loss: 180.8363\n",
            "Epoch [1415/2000], Step [8/8], train_loss: 283.4071\n",
            "Val Loss:  163.72175526618958\n",
            "------------------------------EPOCH: 1416 ------------------------\n",
            "Epoch [1416/2000], Step [2/8], train_loss: 381.9208\n",
            "Epoch [1416/2000], Step [4/8], train_loss: 622.3809\n",
            "Epoch [1416/2000], Step [6/8], train_loss: 181.3717\n",
            "Epoch [1416/2000], Step [8/8], train_loss: 283.0167\n",
            "Val Loss:  317.1969175338745\n",
            "------------------------------EPOCH: 1417 ------------------------\n",
            "Epoch [1417/2000], Step [2/8], train_loss: 374.3177\n",
            "Epoch [1417/2000], Step [4/8], train_loss: 645.3440\n",
            "Epoch [1417/2000], Step [6/8], train_loss: 181.6350\n",
            "Epoch [1417/2000], Step [8/8], train_loss: 283.3014\n",
            "Val Loss:  195.21481521924338\n",
            "------------------------------EPOCH: 1418 ------------------------\n",
            "Epoch [1418/2000], Step [2/8], train_loss: 379.2886\n",
            "Epoch [1418/2000], Step [4/8], train_loss: 633.5262\n",
            "Epoch [1418/2000], Step [6/8], train_loss: 182.7599\n",
            "Epoch [1418/2000], Step [8/8], train_loss: 282.9506\n",
            "Val Loss:  438.4368019104004\n",
            "------------------------------EPOCH: 1419 ------------------------\n",
            "Epoch [1419/2000], Step [2/8], train_loss: 373.4683\n",
            "Epoch [1419/2000], Step [4/8], train_loss: 648.6533\n",
            "Epoch [1419/2000], Step [6/8], train_loss: 181.8377\n",
            "Epoch [1419/2000], Step [8/8], train_loss: 283.3249\n",
            "Val Loss:  234.85533173878989\n",
            "------------------------------EPOCH: 1420 ------------------------\n",
            "Epoch [1420/2000], Step [2/8], train_loss: 380.5308\n",
            "Epoch [1420/2000], Step [4/8], train_loss: 623.5811\n",
            "Epoch [1420/2000], Step [6/8], train_loss: 182.0584\n",
            "Epoch [1420/2000], Step [8/8], train_loss: 282.9831\n",
            "Val Loss:  436.68369881312054\n",
            "------------------------------EPOCH: 1421 ------------------------\n",
            "Epoch [1421/2000], Step [2/8], train_loss: 374.2201\n",
            "Epoch [1421/2000], Step [4/8], train_loss: 644.0161\n",
            "Epoch [1421/2000], Step [6/8], train_loss: 181.3382\n",
            "Epoch [1421/2000], Step [8/8], train_loss: 283.2870\n",
            "Val Loss:  299.3546509742737\n",
            "------------------------------EPOCH: 1422 ------------------------\n",
            "Epoch [1422/2000], Step [2/8], train_loss: 379.8838\n",
            "Epoch [1422/2000], Step [4/8], train_loss: 633.6407\n",
            "Epoch [1422/2000], Step [6/8], train_loss: 181.9174\n",
            "Epoch [1422/2000], Step [8/8], train_loss: 283.0103\n",
            "Val Loss:  460.229553381602\n",
            "------------------------------EPOCH: 1423 ------------------------\n",
            "Epoch [1423/2000], Step [2/8], train_loss: 374.7855\n",
            "Epoch [1423/2000], Step [4/8], train_loss: 649.4666\n",
            "Epoch [1423/2000], Step [6/8], train_loss: 180.8103\n",
            "Epoch [1423/2000], Step [8/8], train_loss: 283.3565\n",
            "Val Loss:  246.93310944239298\n",
            "------------------------------EPOCH: 1424 ------------------------\n",
            "Epoch [1424/2000], Step [2/8], train_loss: 380.7326\n",
            "Epoch [1424/2000], Step [4/8], train_loss: 622.5540\n",
            "Epoch [1424/2000], Step [6/8], train_loss: 180.7986\n",
            "Epoch [1424/2000], Step [8/8], train_loss: 283.0193\n",
            "Val Loss:  357.95971488952637\n",
            "------------------------------EPOCH: 1425 ------------------------\n",
            "Epoch [1425/2000], Step [2/8], train_loss: 373.6398\n",
            "Epoch [1425/2000], Step [4/8], train_loss: 643.7582\n",
            "Epoch [1425/2000], Step [6/8], train_loss: 181.6927\n",
            "Epoch [1425/2000], Step [8/8], train_loss: 283.2504\n",
            "Val Loss:  233.51759147644043\n",
            "------------------------------EPOCH: 1426 ------------------------\n",
            "Epoch [1426/2000], Step [2/8], train_loss: 377.5059\n",
            "Epoch [1426/2000], Step [4/8], train_loss: 631.3967\n",
            "Epoch [1426/2000], Step [6/8], train_loss: 183.4929\n",
            "Epoch [1426/2000], Step [8/8], train_loss: 282.9323\n",
            "Val Loss:  431.6856026649475\n",
            "------------------------------EPOCH: 1427 ------------------------\n",
            "Epoch [1427/2000], Step [2/8], train_loss: 372.5220\n",
            "Epoch [1427/2000], Step [4/8], train_loss: 648.1320\n",
            "Epoch [1427/2000], Step [6/8], train_loss: 181.9778\n",
            "Epoch [1427/2000], Step [8/8], train_loss: 283.2807\n",
            "Val Loss:  243.1309143702189\n",
            "------------------------------EPOCH: 1428 ------------------------\n",
            "Epoch [1428/2000], Step [2/8], train_loss: 378.8735\n",
            "Epoch [1428/2000], Step [4/8], train_loss: 625.1376\n",
            "Epoch [1428/2000], Step [6/8], train_loss: 181.4290\n",
            "Epoch [1428/2000], Step [8/8], train_loss: 283.0091\n",
            "Val Loss:  355.6399923960368\n",
            "------------------------------EPOCH: 1429 ------------------------\n",
            "Epoch [1429/2000], Step [2/8], train_loss: 373.6698\n",
            "Epoch [1429/2000], Step [4/8], train_loss: 643.3650\n",
            "Epoch [1429/2000], Step [6/8], train_loss: 180.9439\n",
            "Epoch [1429/2000], Step [8/8], train_loss: 283.2981\n",
            "Val Loss:  219.01054406166077\n",
            "------------------------------EPOCH: 1430 ------------------------\n",
            "Epoch [1430/2000], Step [2/8], train_loss: 378.4935\n",
            "Epoch [1430/2000], Step [4/8], train_loss: 624.6111\n",
            "Epoch [1430/2000], Step [6/8], train_loss: 181.3641\n",
            "Epoch [1430/2000], Step [8/8], train_loss: 283.1689\n",
            "Val Loss:  341.2407983144124\n",
            "------------------------------EPOCH: 1431 ------------------------\n",
            "Epoch [1431/2000], Step [2/8], train_loss: 374.6812\n",
            "Epoch [1431/2000], Step [4/8], train_loss: 637.7245\n",
            "Epoch [1431/2000], Step [6/8], train_loss: 181.2885\n",
            "Epoch [1431/2000], Step [8/8], train_loss: 283.3559\n",
            "Val Loss:  270.22033103307086\n",
            "------------------------------EPOCH: 1432 ------------------------\n",
            "Epoch [1432/2000], Step [2/8], train_loss: 377.1015\n",
            "Epoch [1432/2000], Step [4/8], train_loss: 633.5806\n",
            "Epoch [1432/2000], Step [6/8], train_loss: 181.9507\n",
            "Epoch [1432/2000], Step [8/8], train_loss: 283.1393\n",
            "Val Loss:  407.6383948326111\n",
            "------------------------------EPOCH: 1433 ------------------------\n",
            "Epoch [1433/2000], Step [2/8], train_loss: 373.7284\n",
            "Epoch [1433/2000], Step [4/8], train_loss: 641.6525\n",
            "Epoch [1433/2000], Step [6/8], train_loss: 180.5861\n",
            "Epoch [1433/2000], Step [8/8], train_loss: 283.4590\n",
            "Val Loss:  231.27648210525513\n",
            "------------------------------EPOCH: 1434 ------------------------\n",
            "Epoch [1434/2000], Step [2/8], train_loss: 378.8319\n",
            "Epoch [1434/2000], Step [4/8], train_loss: 625.4623\n",
            "Epoch [1434/2000], Step [6/8], train_loss: 180.6637\n",
            "Epoch [1434/2000], Step [8/8], train_loss: 283.1208\n",
            "Val Loss:  376.9015239079793\n",
            "------------------------------EPOCH: 1435 ------------------------\n",
            "Epoch [1435/2000], Step [2/8], train_loss: 372.7128\n",
            "Epoch [1435/2000], Step [4/8], train_loss: 643.3728\n",
            "Epoch [1435/2000], Step [6/8], train_loss: 181.2040\n",
            "Epoch [1435/2000], Step [8/8], train_loss: 283.4006\n",
            "Val Loss:  237.40152088801065\n",
            "------------------------------EPOCH: 1436 ------------------------\n",
            "Epoch [1436/2000], Step [2/8], train_loss: 377.6587\n",
            "Epoch [1436/2000], Step [4/8], train_loss: 625.7341\n",
            "Epoch [1436/2000], Step [6/8], train_loss: 182.1754\n",
            "Epoch [1436/2000], Step [8/8], train_loss: 283.2321\n",
            "Val Loss:  418.0202371279399\n",
            "------------------------------EPOCH: 1437 ------------------------\n",
            "Epoch [1437/2000], Step [2/8], train_loss: 373.7600\n",
            "Epoch [1437/2000], Step [4/8], train_loss: 639.4032\n",
            "Epoch [1437/2000], Step [6/8], train_loss: 181.4744\n",
            "Epoch [1437/2000], Step [8/8], train_loss: 283.4398\n",
            "Val Loss:  307.0055301984151\n",
            "------------------------------EPOCH: 1438 ------------------------\n",
            "Epoch [1438/2000], Step [2/8], train_loss: 376.7001\n",
            "Epoch [1438/2000], Step [4/8], train_loss: 626.2147\n",
            "Epoch [1438/2000], Step [6/8], train_loss: 181.0157\n",
            "Epoch [1438/2000], Step [8/8], train_loss: 283.3656\n",
            "Val Loss:  336.2167552312215\n",
            "------------------------------EPOCH: 1439 ------------------------\n",
            "Epoch [1439/2000], Step [2/8], train_loss: 373.8526\n",
            "Epoch [1439/2000], Step [4/8], train_loss: 636.3614\n",
            "Epoch [1439/2000], Step [6/8], train_loss: 181.0378\n",
            "Epoch [1439/2000], Step [8/8], train_loss: 283.5891\n",
            "Val Loss:  261.0863890647888\n",
            "------------------------------EPOCH: 1440 ------------------------\n",
            "Epoch [1440/2000], Step [2/8], train_loss: 376.8106\n",
            "Epoch [1440/2000], Step [4/8], train_loss: 631.4156\n",
            "Epoch [1440/2000], Step [6/8], train_loss: 181.0271\n",
            "Epoch [1440/2000], Step [8/8], train_loss: 283.4451\n",
            "Val Loss:  433.69721762339276\n",
            "------------------------------EPOCH: 1441 ------------------------\n",
            "Epoch [1441/2000], Step [2/8], train_loss: 373.7435\n",
            "Epoch [1441/2000], Step [4/8], train_loss: 639.3688\n",
            "Epoch [1441/2000], Step [6/8], train_loss: 180.5607\n",
            "Epoch [1441/2000], Step [8/8], train_loss: 283.5486\n",
            "Val Loss:  366.1757771174113\n",
            "------------------------------EPOCH: 1442 ------------------------\n",
            "Epoch [1442/2000], Step [2/8], train_loss: 375.6187\n",
            "Epoch [1442/2000], Step [4/8], train_loss: 631.7642\n",
            "Epoch [1442/2000], Step [6/8], train_loss: 181.3046\n",
            "Epoch [1442/2000], Step [8/8], train_loss: 283.4484\n",
            "Val Loss:  434.4746147791545\n",
            "------------------------------EPOCH: 1443 ------------------------\n",
            "Epoch [1443/2000], Step [2/8], train_loss: 373.2183\n",
            "Epoch [1443/2000], Step [4/8], train_loss: 635.6230\n",
            "Epoch [1443/2000], Step [6/8], train_loss: 181.5561\n",
            "Epoch [1443/2000], Step [8/8], train_loss: 283.5781\n",
            "Val Loss:  328.92647584279376\n",
            "------------------------------EPOCH: 1444 ------------------------\n",
            "Epoch [1444/2000], Step [2/8], train_loss: 374.8969\n",
            "Epoch [1444/2000], Step [4/8], train_loss: 629.9789\n",
            "Epoch [1444/2000], Step [6/8], train_loss: 181.6958\n",
            "Epoch [1444/2000], Step [8/8], train_loss: 283.6143\n",
            "Val Loss:  333.9601454734802\n",
            "------------------------------EPOCH: 1445 ------------------------\n",
            "Epoch [1445/2000], Step [2/8], train_loss: 374.2592\n",
            "Epoch [1445/2000], Step [4/8], train_loss: 632.7141\n",
            "Epoch [1445/2000], Step [6/8], train_loss: 181.2473\n",
            "Epoch [1445/2000], Step [8/8], train_loss: 283.7545\n",
            "Val Loss:  258.2525445620219\n",
            "------------------------------EPOCH: 1446 ------------------------\n",
            "Epoch [1446/2000], Step [2/8], train_loss: 375.3181\n",
            "Epoch [1446/2000], Step [4/8], train_loss: 629.1309\n",
            "Epoch [1446/2000], Step [6/8], train_loss: 181.0462\n",
            "Epoch [1446/2000], Step [8/8], train_loss: 283.8133\n",
            "Val Loss:  291.66475407282513\n",
            "------------------------------EPOCH: 1447 ------------------------\n",
            "Epoch [1447/2000], Step [2/8], train_loss: 374.6953\n",
            "Epoch [1447/2000], Step [4/8], train_loss: 632.1118\n",
            "Epoch [1447/2000], Step [6/8], train_loss: 180.6195\n",
            "Epoch [1447/2000], Step [8/8], train_loss: 283.9796\n",
            "Val Loss:  250.82911324501038\n",
            "------------------------------EPOCH: 1448 ------------------------\n",
            "Epoch [1448/2000], Step [2/8], train_loss: 376.1019\n",
            "Epoch [1448/2000], Step [4/8], train_loss: 636.0892\n",
            "Epoch [1448/2000], Step [6/8], train_loss: 180.9395\n",
            "Epoch [1448/2000], Step [8/8], train_loss: 283.8552\n",
            "Val Loss:  383.69216934839886\n",
            "------------------------------EPOCH: 1449 ------------------------\n",
            "Epoch [1449/2000], Step [2/8], train_loss: 374.1423\n",
            "Epoch [1449/2000], Step [4/8], train_loss: 634.3304\n",
            "Epoch [1449/2000], Step [6/8], train_loss: 179.8851\n",
            "Epoch [1449/2000], Step [8/8], train_loss: 284.1128\n",
            "Val Loss:  287.78144613901776\n",
            "------------------------------EPOCH: 1450 ------------------------\n",
            "Epoch [1450/2000], Step [2/8], train_loss: 377.5070\n",
            "Epoch [1450/2000], Step [4/8], train_loss: 627.3602\n",
            "Epoch [1450/2000], Step [6/8], train_loss: 180.4464\n",
            "Epoch [1450/2000], Step [8/8], train_loss: 283.9666\n",
            "Val Loss:  341.1584555308024\n",
            "------------------------------EPOCH: 1451 ------------------------\n",
            "Epoch [1451/2000], Step [2/8], train_loss: 373.9800\n",
            "Epoch [1451/2000], Step [4/8], train_loss: 637.5269\n",
            "Epoch [1451/2000], Step [6/8], train_loss: 180.3212\n",
            "Epoch [1451/2000], Step [8/8], train_loss: 284.0613\n",
            "Val Loss:  301.38874594370526\n",
            "------------------------------EPOCH: 1452 ------------------------\n",
            "Epoch [1452/2000], Step [2/8], train_loss: 374.9492\n",
            "Epoch [1452/2000], Step [4/8], train_loss: 637.3930\n",
            "Epoch [1452/2000], Step [6/8], train_loss: 180.4102\n",
            "Epoch [1452/2000], Step [8/8], train_loss: 283.9932\n",
            "Val Loss:  427.3060185114543\n",
            "------------------------------EPOCH: 1453 ------------------------\n",
            "Epoch [1453/2000], Step [2/8], train_loss: 373.4959\n",
            "Epoch [1453/2000], Step [4/8], train_loss: 641.0143\n",
            "Epoch [1453/2000], Step [6/8], train_loss: 180.4654\n",
            "Epoch [1453/2000], Step [8/8], train_loss: 284.2466\n",
            "Val Loss:  299.3928578694661\n",
            "------------------------------EPOCH: 1454 ------------------------\n",
            "Epoch [1454/2000], Step [2/8], train_loss: 376.0438\n",
            "Epoch [1454/2000], Step [4/8], train_loss: 628.8585\n",
            "Epoch [1454/2000], Step [6/8], train_loss: 182.2181\n",
            "Epoch [1454/2000], Step [8/8], train_loss: 283.9286\n",
            "Val Loss:  517.7970736821493\n",
            "------------------------------EPOCH: 1455 ------------------------\n",
            "Epoch [1455/2000], Step [2/8], train_loss: 370.4855\n",
            "Epoch [1455/2000], Step [4/8], train_loss: 639.5448\n",
            "Epoch [1455/2000], Step [6/8], train_loss: 181.5634\n",
            "Epoch [1455/2000], Step [8/8], train_loss: 284.1858\n",
            "Val Loss:  298.1779308319092\n",
            "------------------------------EPOCH: 1456 ------------------------\n",
            "Epoch [1456/2000], Step [2/8], train_loss: 374.6240\n",
            "Epoch [1456/2000], Step [4/8], train_loss: 629.6938\n",
            "Epoch [1456/2000], Step [6/8], train_loss: 181.8038\n",
            "Epoch [1456/2000], Step [8/8], train_loss: 283.9785\n",
            "Val Loss:  417.5033696492513\n",
            "------------------------------EPOCH: 1457 ------------------------\n",
            "Epoch [1457/2000], Step [2/8], train_loss: 371.8734\n",
            "Epoch [1457/2000], Step [4/8], train_loss: 639.3654\n",
            "Epoch [1457/2000], Step [6/8], train_loss: 180.3268\n",
            "Epoch [1457/2000], Step [8/8], train_loss: 284.2700\n",
            "Val Loss:  265.717072168986\n",
            "------------------------------EPOCH: 1458 ------------------------\n",
            "Epoch [1458/2000], Step [2/8], train_loss: 375.9131\n",
            "Epoch [1458/2000], Step [4/8], train_loss: 627.0798\n",
            "Epoch [1458/2000], Step [6/8], train_loss: 180.5912\n",
            "Epoch [1458/2000], Step [8/8], train_loss: 284.1061\n",
            "Val Loss:  312.0719617207845\n",
            "------------------------------EPOCH: 1459 ------------------------\n",
            "Epoch [1459/2000], Step [2/8], train_loss: 372.1118\n",
            "Epoch [1459/2000], Step [4/8], train_loss: 638.8408\n",
            "Epoch [1459/2000], Step [6/8], train_loss: 180.4833\n",
            "Epoch [1459/2000], Step [8/8], train_loss: 284.3419\n",
            "Val Loss:  197.74541958173117\n",
            "------------------------------EPOCH: 1460 ------------------------\n",
            "Epoch [1460/2000], Step [2/8], train_loss: 374.9930\n",
            "Epoch [1460/2000], Step [4/8], train_loss: 633.9773\n",
            "Epoch [1460/2000], Step [6/8], train_loss: 181.9281\n",
            "Epoch [1460/2000], Step [8/8], train_loss: 284.0409\n",
            "Val Loss:  338.9467840194702\n",
            "------------------------------EPOCH: 1461 ------------------------\n",
            "Epoch [1461/2000], Step [2/8], train_loss: 371.2905\n",
            "Epoch [1461/2000], Step [4/8], train_loss: 646.0311\n",
            "Epoch [1461/2000], Step [6/8], train_loss: 180.0917\n",
            "Epoch [1461/2000], Step [8/8], train_loss: 284.3977\n",
            "Val Loss:  200.56076097488403\n",
            "------------------------------EPOCH: 1462 ------------------------\n",
            "Epoch [1462/2000], Step [2/8], train_loss: 377.0314\n",
            "Epoch [1462/2000], Step [4/8], train_loss: 627.6937\n",
            "Epoch [1462/2000], Step [6/8], train_loss: 180.3196\n",
            "Epoch [1462/2000], Step [8/8], train_loss: 283.9368\n",
            "Val Loss:  329.1788489818573\n",
            "------------------------------EPOCH: 1463 ------------------------\n",
            "Epoch [1463/2000], Step [2/8], train_loss: 369.7862\n",
            "Epoch [1463/2000], Step [4/8], train_loss: 646.1786\n",
            "Epoch [1463/2000], Step [6/8], train_loss: 180.7578\n",
            "Epoch [1463/2000], Step [8/8], train_loss: 284.2715\n",
            "Val Loss:  185.6288352807363\n",
            "------------------------------EPOCH: 1464 ------------------------\n",
            "Epoch [1464/2000], Step [2/8], train_loss: 375.5975\n",
            "Epoch [1464/2000], Step [4/8], train_loss: 626.9558\n",
            "Epoch [1464/2000], Step [6/8], train_loss: 182.1105\n",
            "Epoch [1464/2000], Step [8/8], train_loss: 284.0235\n",
            "Val Loss:  309.6578489144643\n",
            "------------------------------EPOCH: 1465 ------------------------\n",
            "Epoch [1465/2000], Step [2/8], train_loss: 369.9347\n",
            "Epoch [1465/2000], Step [4/8], train_loss: 642.3158\n",
            "Epoch [1465/2000], Step [6/8], train_loss: 181.6350\n",
            "Epoch [1465/2000], Step [8/8], train_loss: 284.3198\n",
            "Val Loss:  226.52400088310242\n",
            "------------------------------EPOCH: 1466 ------------------------\n",
            "Epoch [1466/2000], Step [2/8], train_loss: 373.1497\n",
            "Epoch [1466/2000], Step [4/8], train_loss: 634.9829\n",
            "Epoch [1466/2000], Step [6/8], train_loss: 182.7023\n",
            "Epoch [1466/2000], Step [8/8], train_loss: 284.0709\n",
            "Val Loss:  364.52815437316895\n",
            "------------------------------EPOCH: 1467 ------------------------\n",
            "Epoch [1467/2000], Step [2/8], train_loss: 368.8889\n",
            "Epoch [1467/2000], Step [4/8], train_loss: 648.3975\n",
            "Epoch [1467/2000], Step [6/8], train_loss: 181.1147\n",
            "Epoch [1467/2000], Step [8/8], train_loss: 284.4764\n",
            "Val Loss:  210.7668687502543\n",
            "------------------------------EPOCH: 1468 ------------------------\n",
            "Epoch [1468/2000], Step [2/8], train_loss: 374.8571\n",
            "Epoch [1468/2000], Step [4/8], train_loss: 625.9089\n",
            "Epoch [1468/2000], Step [6/8], train_loss: 181.1276\n",
            "Epoch [1468/2000], Step [8/8], train_loss: 284.1580\n",
            "Val Loss:  363.5985875924428\n",
            "------------------------------EPOCH: 1469 ------------------------\n",
            "Epoch [1469/2000], Step [2/8], train_loss: 369.1607\n",
            "Epoch [1469/2000], Step [4/8], train_loss: 644.8165\n",
            "Epoch [1469/2000], Step [6/8], train_loss: 180.6689\n",
            "Epoch [1469/2000], Step [8/8], train_loss: 284.4761\n",
            "Val Loss:  247.51072311401367\n",
            "------------------------------EPOCH: 1470 ------------------------\n",
            "Epoch [1470/2000], Step [2/8], train_loss: 373.8706\n",
            "Epoch [1470/2000], Step [4/8], train_loss: 627.1435\n",
            "Epoch [1470/2000], Step [6/8], train_loss: 181.3419\n",
            "Epoch [1470/2000], Step [8/8], train_loss: 284.2880\n",
            "Val Loss:  358.7289938926697\n",
            "------------------------------EPOCH: 1471 ------------------------\n",
            "Epoch [1471/2000], Step [2/8], train_loss: 369.5894\n",
            "Epoch [1471/2000], Step [4/8], train_loss: 642.4515\n",
            "Epoch [1471/2000], Step [6/8], train_loss: 180.8845\n",
            "Epoch [1471/2000], Step [8/8], train_loss: 284.5280\n",
            "Val Loss:  227.92817632357279\n",
            "------------------------------EPOCH: 1472 ------------------------\n",
            "Epoch [1472/2000], Step [2/8], train_loss: 372.7288\n",
            "Epoch [1472/2000], Step [4/8], train_loss: 634.9383\n",
            "Epoch [1472/2000], Step [6/8], train_loss: 181.7215\n",
            "Epoch [1472/2000], Step [8/8], train_loss: 284.2757\n",
            "Val Loss:  359.86380259195965\n",
            "------------------------------EPOCH: 1473 ------------------------\n",
            "Epoch [1473/2000], Step [2/8], train_loss: 368.9651\n",
            "Epoch [1473/2000], Step [4/8], train_loss: 646.4419\n",
            "Epoch [1473/2000], Step [6/8], train_loss: 180.1665\n",
            "Epoch [1473/2000], Step [8/8], train_loss: 284.6296\n",
            "Val Loss:  205.13242061932883\n",
            "------------------------------EPOCH: 1474 ------------------------\n",
            "Epoch [1474/2000], Step [2/8], train_loss: 374.1860\n",
            "Epoch [1474/2000], Step [4/8], train_loss: 627.6215\n",
            "Epoch [1474/2000], Step [6/8], train_loss: 180.2912\n",
            "Epoch [1474/2000], Step [8/8], train_loss: 284.2505\n",
            "Val Loss:  444.87328481674194\n",
            "------------------------------EPOCH: 1475 ------------------------\n",
            "Epoch [1475/2000], Step [2/8], train_loss: 367.6472\n",
            "Epoch [1475/2000], Step [4/8], train_loss: 645.0757\n",
            "Epoch [1475/2000], Step [6/8], train_loss: 180.8022\n",
            "Epoch [1475/2000], Step [8/8], train_loss: 284.5590\n",
            "Val Loss:  254.91615676879883\n",
            "------------------------------EPOCH: 1476 ------------------------\n",
            "Epoch [1476/2000], Step [2/8], train_loss: 372.8064\n",
            "Epoch [1476/2000], Step [4/8], train_loss: 627.2297\n",
            "Epoch [1476/2000], Step [6/8], train_loss: 181.9887\n",
            "Epoch [1476/2000], Step [8/8], train_loss: 284.3604\n",
            "Val Loss:  360.9867908159892\n",
            "------------------------------EPOCH: 1477 ------------------------\n",
            "Epoch [1477/2000], Step [2/8], train_loss: 368.5325\n",
            "Epoch [1477/2000], Step [4/8], train_loss: 643.3441\n",
            "Epoch [1477/2000], Step [6/8], train_loss: 181.1106\n",
            "Epoch [1477/2000], Step [8/8], train_loss: 284.6057\n",
            "Val Loss:  219.20317522684732\n",
            "------------------------------EPOCH: 1478 ------------------------\n",
            "Epoch [1478/2000], Step [2/8], train_loss: 371.7088\n",
            "Epoch [1478/2000], Step [4/8], train_loss: 637.5779\n",
            "Epoch [1478/2000], Step [6/8], train_loss: 181.4192\n",
            "Epoch [1478/2000], Step [8/8], train_loss: 284.3319\n",
            "Val Loss:  383.49784739812213\n",
            "------------------------------EPOCH: 1479 ------------------------\n",
            "Epoch [1479/2000], Step [2/8], train_loss: 367.3481\n",
            "Epoch [1479/2000], Step [4/8], train_loss: 646.9871\n",
            "Epoch [1479/2000], Step [6/8], train_loss: 180.4242\n",
            "Epoch [1479/2000], Step [8/8], train_loss: 284.7103\n",
            "Val Loss:  223.5540944735209\n",
            "------------------------------EPOCH: 1480 ------------------------\n",
            "Epoch [1480/2000], Step [2/8], train_loss: 373.2888\n",
            "Epoch [1480/2000], Step [4/8], train_loss: 627.3934\n",
            "Epoch [1480/2000], Step [6/8], train_loss: 181.2717\n",
            "Epoch [1480/2000], Step [8/8], train_loss: 284.3685\n",
            "Val Loss:  405.06588649749756\n",
            "------------------------------EPOCH: 1481 ------------------------\n",
            "Epoch [1481/2000], Step [2/8], train_loss: 367.5668\n",
            "Epoch [1481/2000], Step [4/8], train_loss: 645.3264\n",
            "Epoch [1481/2000], Step [6/8], train_loss: 180.5661\n",
            "Epoch [1481/2000], Step [8/8], train_loss: 284.6202\n",
            "Val Loss:  283.20374425252277\n",
            "------------------------------EPOCH: 1482 ------------------------\n",
            "Epoch [1482/2000], Step [2/8], train_loss: 371.5630\n",
            "Epoch [1482/2000], Step [4/8], train_loss: 627.2034\n",
            "Epoch [1482/2000], Step [6/8], train_loss: 180.6346\n",
            "Epoch [1482/2000], Step [8/8], train_loss: 284.4700\n",
            "Val Loss:  386.0579164822896\n",
            "------------------------------EPOCH: 1483 ------------------------\n",
            "Epoch [1483/2000], Step [2/8], train_loss: 368.3683\n",
            "Epoch [1483/2000], Step [4/8], train_loss: 640.9325\n",
            "Epoch [1483/2000], Step [6/8], train_loss: 180.2230\n",
            "Epoch [1483/2000], Step [8/8], train_loss: 284.7125\n",
            "Val Loss:  253.6095986366272\n",
            "------------------------------EPOCH: 1484 ------------------------\n",
            "Epoch [1484/2000], Step [2/8], train_loss: 371.4265\n",
            "Epoch [1484/2000], Step [4/8], train_loss: 634.5294\n",
            "Epoch [1484/2000], Step [6/8], train_loss: 181.3679\n",
            "Epoch [1484/2000], Step [8/8], train_loss: 284.4780\n",
            "Val Loss:  402.80403566360474\n",
            "------------------------------EPOCH: 1485 ------------------------\n",
            "Epoch [1485/2000], Step [2/8], train_loss: 367.6928\n",
            "Epoch [1485/2000], Step [4/8], train_loss: 646.6036\n",
            "Epoch [1485/2000], Step [6/8], train_loss: 179.8136\n",
            "Epoch [1485/2000], Step [8/8], train_loss: 284.8180\n",
            "Val Loss:  257.17076444625854\n",
            "------------------------------EPOCH: 1486 ------------------------\n",
            "Epoch [1486/2000], Step [2/8], train_loss: 372.6252\n",
            "Epoch [1486/2000], Step [4/8], train_loss: 628.0222\n",
            "Epoch [1486/2000], Step [6/8], train_loss: 179.9839\n",
            "Epoch [1486/2000], Step [8/8], train_loss: 284.4281\n",
            "Val Loss:  454.64770158131915\n",
            "------------------------------EPOCH: 1487 ------------------------\n",
            "Epoch [1487/2000], Step [2/8], train_loss: 366.0036\n",
            "Epoch [1487/2000], Step [4/8], train_loss: 645.4661\n",
            "Epoch [1487/2000], Step [6/8], train_loss: 180.4203\n",
            "Epoch [1487/2000], Step [8/8], train_loss: 284.7246\n",
            "Val Loss:  236.31662424405417\n",
            "------------------------------EPOCH: 1488 ------------------------\n",
            "Epoch [1488/2000], Step [2/8], train_loss: 371.1553\n",
            "Epoch [1488/2000], Step [4/8], train_loss: 627.7145\n",
            "Epoch [1488/2000], Step [6/8], train_loss: 181.6382\n",
            "Epoch [1488/2000], Step [8/8], train_loss: 284.5102\n",
            "Val Loss:  362.7307898203532\n",
            "------------------------------EPOCH: 1489 ------------------------\n",
            "Epoch [1489/2000], Step [2/8], train_loss: 366.8813\n",
            "Epoch [1489/2000], Step [4/8], train_loss: 643.7543\n",
            "Epoch [1489/2000], Step [6/8], train_loss: 180.7983\n",
            "Epoch [1489/2000], Step [8/8], train_loss: 284.7524\n",
            "Val Loss:  255.63932037353516\n",
            "------------------------------EPOCH: 1490 ------------------------\n",
            "Epoch [1490/2000], Step [2/8], train_loss: 370.1041\n",
            "Epoch [1490/2000], Step [4/8], train_loss: 638.1608\n",
            "Epoch [1490/2000], Step [6/8], train_loss: 181.1865\n",
            "Epoch [1490/2000], Step [8/8], train_loss: 284.4674\n",
            "Val Loss:  432.4735387166341\n",
            "------------------------------EPOCH: 1491 ------------------------\n",
            "Epoch [1491/2000], Step [2/8], train_loss: 365.7726\n",
            "Epoch [1491/2000], Step [4/8], train_loss: 647.2789\n",
            "Epoch [1491/2000], Step [6/8], train_loss: 180.0466\n",
            "Epoch [1491/2000], Step [8/8], train_loss: 284.8462\n",
            "Val Loss:  258.04279438654584\n",
            "------------------------------EPOCH: 1492 ------------------------\n",
            "Epoch [1492/2000], Step [2/8], train_loss: 371.7778\n",
            "Epoch [1492/2000], Step [4/8], train_loss: 627.3333\n",
            "Epoch [1492/2000], Step [6/8], train_loss: 180.8762\n",
            "Epoch [1492/2000], Step [8/8], train_loss: 284.4966\n",
            "Val Loss:  455.49779923756915\n",
            "------------------------------EPOCH: 1493 ------------------------\n",
            "Epoch [1493/2000], Step [2/8], train_loss: 366.0369\n",
            "Epoch [1493/2000], Step [4/8], train_loss: 645.7516\n",
            "Epoch [1493/2000], Step [6/8], train_loss: 179.8819\n",
            "Epoch [1493/2000], Step [8/8], train_loss: 284.7852\n",
            "Val Loss:  249.14397970835367\n",
            "------------------------------EPOCH: 1494 ------------------------\n",
            "Epoch [1494/2000], Step [2/8], train_loss: 370.7790\n",
            "Epoch [1494/2000], Step [4/8], train_loss: 626.9481\n",
            "Epoch [1494/2000], Step [6/8], train_loss: 179.7267\n",
            "Epoch [1494/2000], Step [8/8], train_loss: 284.6085\n",
            "Val Loss:  270.1816299756368\n",
            "------------------------------EPOCH: 1495 ------------------------\n",
            "Epoch [1495/2000], Step [2/8], train_loss: 366.5665\n",
            "Epoch [1495/2000], Step [4/8], train_loss: 642.1873\n",
            "Epoch [1495/2000], Step [6/8], train_loss: 179.9086\n",
            "Epoch [1495/2000], Step [8/8], train_loss: 284.8257\n",
            "Val Loss:  201.3215266863505\n",
            "------------------------------EPOCH: 1496 ------------------------\n",
            "Epoch [1496/2000], Step [2/8], train_loss: 369.4672\n",
            "Epoch [1496/2000], Step [4/8], train_loss: 634.9125\n",
            "Epoch [1496/2000], Step [6/8], train_loss: 181.2101\n",
            "Epoch [1496/2000], Step [8/8], train_loss: 284.5643\n",
            "Val Loss:  384.78110218048096\n",
            "------------------------------EPOCH: 1497 ------------------------\n",
            "Epoch [1497/2000], Step [2/8], train_loss: 365.9850\n",
            "Epoch [1497/2000], Step [4/8], train_loss: 648.5721\n",
            "Epoch [1497/2000], Step [6/8], train_loss: 179.4095\n",
            "Epoch [1497/2000], Step [8/8], train_loss: 284.8953\n",
            "Val Loss:  237.0158290863037\n",
            "------------------------------EPOCH: 1498 ------------------------\n",
            "Epoch [1498/2000], Step [2/8], train_loss: 371.1335\n",
            "Epoch [1498/2000], Step [4/8], train_loss: 628.1693\n",
            "Epoch [1498/2000], Step [6/8], train_loss: 179.3125\n",
            "Epoch [1498/2000], Step [8/8], train_loss: 284.5148\n",
            "Val Loss:  413.82544199625653\n",
            "------------------------------EPOCH: 1499 ------------------------\n",
            "Epoch [1499/2000], Step [2/8], train_loss: 364.7961\n",
            "Epoch [1499/2000], Step [4/8], train_loss: 645.6470\n",
            "Epoch [1499/2000], Step [6/8], train_loss: 179.8960\n",
            "Epoch [1499/2000], Step [8/8], train_loss: 284.7953\n",
            "Val Loss:  261.4300168355306\n",
            "------------------------------EPOCH: 1500 ------------------------\n",
            "Epoch [1500/2000], Step [2/8], train_loss: 369.6415\n",
            "Epoch [1500/2000], Step [4/8], train_loss: 628.0081\n",
            "Epoch [1500/2000], Step [6/8], train_loss: 181.1770\n",
            "Epoch [1500/2000], Step [8/8], train_loss: 284.5968\n",
            "Val Loss:  367.01139799753827\n",
            "------------------------------EPOCH: 1501 ------------------------\n",
            "Epoch [1501/2000], Step [2/8], train_loss: 365.5919\n",
            "Epoch [1501/2000], Step [4/8], train_loss: 643.1299\n",
            "Epoch [1501/2000], Step [6/8], train_loss: 180.4052\n",
            "Epoch [1501/2000], Step [8/8], train_loss: 284.8222\n",
            "Val Loss:  271.99247694015503\n",
            "------------------------------EPOCH: 1502 ------------------------\n",
            "Epoch [1502/2000], Step [2/8], train_loss: 368.5630\n",
            "Epoch [1502/2000], Step [4/8], train_loss: 637.9191\n",
            "Epoch [1502/2000], Step [6/8], train_loss: 180.6854\n",
            "Epoch [1502/2000], Step [8/8], train_loss: 284.5406\n",
            "Val Loss:  504.17859776814777\n",
            "------------------------------EPOCH: 1503 ------------------------\n",
            "Epoch [1503/2000], Step [2/8], train_loss: 364.4778\n",
            "Epoch [1503/2000], Step [4/8], train_loss: 646.4590\n",
            "Epoch [1503/2000], Step [6/8], train_loss: 179.5753\n",
            "Epoch [1503/2000], Step [8/8], train_loss: 284.8994\n",
            "Val Loss:  298.71735858917236\n",
            "------------------------------EPOCH: 1504 ------------------------\n",
            "Epoch [1504/2000], Step [2/8], train_loss: 370.2584\n",
            "Epoch [1504/2000], Step [4/8], train_loss: 627.4558\n",
            "Epoch [1504/2000], Step [6/8], train_loss: 180.4208\n",
            "Epoch [1504/2000], Step [8/8], train_loss: 284.5446\n",
            "Val Loss:  499.4206082026164\n",
            "------------------------------EPOCH: 1505 ------------------------\n",
            "Epoch [1505/2000], Step [2/8], train_loss: 364.7555\n",
            "Epoch [1505/2000], Step [4/8], train_loss: 645.6121\n",
            "Epoch [1505/2000], Step [6/8], train_loss: 179.4361\n",
            "Epoch [1505/2000], Step [8/8], train_loss: 284.8117\n",
            "Val Loss:  292.8448254267375\n",
            "------------------------------EPOCH: 1506 ------------------------\n",
            "Epoch [1506/2000], Step [2/8], train_loss: 369.5429\n",
            "Epoch [1506/2000], Step [4/8], train_loss: 627.4503\n",
            "Epoch [1506/2000], Step [6/8], train_loss: 179.4067\n",
            "Epoch [1506/2000], Step [8/8], train_loss: 284.6410\n",
            "Val Loss:  372.27104727427167\n",
            "------------------------------EPOCH: 1507 ------------------------\n",
            "Epoch [1507/2000], Step [2/8], train_loss: 365.4733\n",
            "Epoch [1507/2000], Step [4/8], train_loss: 640.8930\n",
            "Epoch [1507/2000], Step [6/8], train_loss: 179.5543\n",
            "Epoch [1507/2000], Step [8/8], train_loss: 284.8514\n",
            "Val Loss:  273.55755790074664\n",
            "------------------------------EPOCH: 1508 ------------------------\n",
            "Epoch [1508/2000], Step [2/8], train_loss: 367.9650\n",
            "Epoch [1508/2000], Step [4/8], train_loss: 635.2987\n",
            "Epoch [1508/2000], Step [6/8], train_loss: 180.8487\n",
            "Epoch [1508/2000], Step [8/8], train_loss: 284.5948\n",
            "Val Loss:  303.9773879845937\n",
            "------------------------------EPOCH: 1509 ------------------------\n",
            "Epoch [1509/2000], Step [2/8], train_loss: 364.2299\n",
            "Epoch [1509/2000], Step [4/8], train_loss: 644.9384\n",
            "Epoch [1509/2000], Step [6/8], train_loss: 179.1322\n",
            "Epoch [1509/2000], Step [8/8], train_loss: 284.9388\n",
            "Val Loss:  141.4698448975881\n",
            "------------------------------EPOCH: 1510 ------------------------\n",
            "Epoch [1510/2000], Step [2/8], train_loss: 369.3374\n",
            "Epoch [1510/2000], Step [4/8], train_loss: 628.9851\n",
            "Epoch [1510/2000], Step [6/8], train_loss: 179.3143\n",
            "Epoch [1510/2000], Step [8/8], train_loss: 284.5339\n",
            "Val Loss:  364.5349203745524\n",
            "------------------------------EPOCH: 1511 ------------------------\n",
            "Epoch [1511/2000], Step [2/8], train_loss: 363.5204\n",
            "Epoch [1511/2000], Step [4/8], train_loss: 646.4706\n",
            "Epoch [1511/2000], Step [6/8], train_loss: 179.5631\n",
            "Epoch [1511/2000], Step [8/8], train_loss: 284.8134\n",
            "Val Loss:  256.4640245437622\n",
            "------------------------------EPOCH: 1512 ------------------------\n",
            "Epoch [1512/2000], Step [2/8], train_loss: 368.8340\n",
            "Epoch [1512/2000], Step [4/8], train_loss: 627.2927\n",
            "Epoch [1512/2000], Step [6/8], train_loss: 180.5340\n",
            "Epoch [1512/2000], Step [8/8], train_loss: 284.5758\n",
            "Val Loss:  423.58372990290326\n",
            "------------------------------EPOCH: 1513 ------------------------\n",
            "Epoch [1513/2000], Step [2/8], train_loss: 364.1858\n",
            "Epoch [1513/2000], Step [4/8], train_loss: 643.3575\n",
            "Epoch [1513/2000], Step [6/8], train_loss: 180.0034\n",
            "Epoch [1513/2000], Step [8/8], train_loss: 284.8167\n",
            "Val Loss:  293.9621148109436\n",
            "------------------------------EPOCH: 1514 ------------------------\n",
            "Epoch [1514/2000], Step [2/8], train_loss: 367.2168\n",
            "Epoch [1514/2000], Step [4/8], train_loss: 636.0450\n",
            "Epoch [1514/2000], Step [6/8], train_loss: 180.4470\n",
            "Epoch [1514/2000], Step [8/8], train_loss: 284.5732\n",
            "Val Loss:  491.9843986829122\n",
            "------------------------------EPOCH: 1515 ------------------------\n",
            "Epoch [1515/2000], Step [2/8], train_loss: 363.6460\n",
            "Epoch [1515/2000], Step [4/8], train_loss: 647.9079\n",
            "Epoch [1515/2000], Step [6/8], train_loss: 178.8429\n",
            "Epoch [1515/2000], Step [8/8], train_loss: 284.9463\n",
            "Val Loss:  265.68244965871173\n",
            "------------------------------EPOCH: 1516 ------------------------\n",
            "Epoch [1516/2000], Step [2/8], train_loss: 369.4178\n",
            "Epoch [1516/2000], Step [4/8], train_loss: 626.3118\n",
            "Epoch [1516/2000], Step [6/8], train_loss: 179.4003\n",
            "Epoch [1516/2000], Step [8/8], train_loss: 284.5551\n",
            "Val Loss:  321.862722714742\n",
            "------------------------------EPOCH: 1517 ------------------------\n",
            "Epoch [1517/2000], Step [2/8], train_loss: 362.4855\n",
            "Epoch [1517/2000], Step [4/8], train_loss: 647.0344\n",
            "Epoch [1517/2000], Step [6/8], train_loss: 179.7533\n",
            "Epoch [1517/2000], Step [8/8], train_loss: 284.8241\n",
            "Val Loss:  178.770015001297\n",
            "------------------------------EPOCH: 1518 ------------------------\n",
            "Epoch [1518/2000], Step [2/8], train_loss: 366.7306\n",
            "Epoch [1518/2000], Step [4/8], train_loss: 637.7537\n",
            "Epoch [1518/2000], Step [6/8], train_loss: 181.1918\n",
            "Epoch [1518/2000], Step [8/8], train_loss: 284.4600\n",
            "Val Loss:  445.70043595631915\n",
            "------------------------------EPOCH: 1519 ------------------------\n",
            "Epoch [1519/2000], Step [2/8], train_loss: 361.6499\n",
            "Epoch [1519/2000], Step [4/8], train_loss: 650.9062\n",
            "Epoch [1519/2000], Step [6/8], train_loss: 179.6865\n",
            "Epoch [1519/2000], Step [8/8], train_loss: 284.8423\n",
            "Val Loss:  258.05225404103595\n",
            "------------------------------EPOCH: 1520 ------------------------\n",
            "Epoch [1520/2000], Step [2/8], train_loss: 368.5978\n",
            "Epoch [1520/2000], Step [4/8], train_loss: 626.8766\n",
            "Epoch [1520/2000], Step [6/8], train_loss: 180.0076\n",
            "Epoch [1520/2000], Step [8/8], train_loss: 284.4707\n",
            "Val Loss:  439.2518521944682\n",
            "------------------------------EPOCH: 1521 ------------------------\n",
            "Epoch [1521/2000], Step [2/8], train_loss: 362.4153\n",
            "Epoch [1521/2000], Step [4/8], train_loss: 646.9355\n",
            "Epoch [1521/2000], Step [6/8], train_loss: 179.3543\n",
            "Epoch [1521/2000], Step [8/8], train_loss: 284.7733\n",
            "Val Loss:  286.72516139348346\n",
            "------------------------------EPOCH: 1522 ------------------------\n",
            "Epoch [1522/2000], Step [2/8], train_loss: 367.5972\n",
            "Epoch [1522/2000], Step [4/8], train_loss: 637.5925\n",
            "Epoch [1522/2000], Step [6/8], train_loss: 180.0253\n",
            "Epoch [1522/2000], Step [8/8], train_loss: 284.4688\n",
            "Val Loss:  378.3961599667867\n",
            "------------------------------EPOCH: 1523 ------------------------\n",
            "Epoch [1523/2000], Step [2/8], train_loss: 362.8213\n",
            "Epoch [1523/2000], Step [4/8], train_loss: 652.1414\n",
            "Epoch [1523/2000], Step [6/8], train_loss: 178.7613\n",
            "Epoch [1523/2000], Step [8/8], train_loss: 284.8223\n",
            "Val Loss:  211.98365211486816\n",
            "------------------------------EPOCH: 1524 ------------------------\n",
            "Epoch [1524/2000], Step [2/8], train_loss: 368.5380\n",
            "Epoch [1524/2000], Step [4/8], train_loss: 625.4360\n",
            "Epoch [1524/2000], Step [6/8], train_loss: 178.9529\n",
            "Epoch [1524/2000], Step [8/8], train_loss: 284.5030\n",
            "Val Loss:  345.1207076708476\n",
            "------------------------------EPOCH: 1525 ------------------------\n",
            "Epoch [1525/2000], Step [2/8], train_loss: 362.5051\n",
            "Epoch [1525/2000], Step [4/8], train_loss: 645.1069\n",
            "Epoch [1525/2000], Step [6/8], train_loss: 178.8558\n",
            "Epoch [1525/2000], Step [8/8], train_loss: 284.7902\n",
            "Val Loss:  255.98371823628744\n",
            "------------------------------EPOCH: 1526 ------------------------\n",
            "Epoch [1526/2000], Step [2/8], train_loss: 367.2388\n",
            "Epoch [1526/2000], Step [4/8], train_loss: 635.9474\n",
            "Epoch [1526/2000], Step [6/8], train_loss: 179.8449\n",
            "Epoch [1526/2000], Step [8/8], train_loss: 284.5020\n",
            "Val Loss:  384.67674740155536\n",
            "------------------------------EPOCH: 1527 ------------------------\n",
            "Epoch [1527/2000], Step [2/8], train_loss: 362.4461\n",
            "Epoch [1527/2000], Step [4/8], train_loss: 650.6102\n",
            "Epoch [1527/2000], Step [6/8], train_loss: 178.3944\n",
            "Epoch [1527/2000], Step [8/8], train_loss: 284.8333\n",
            "Val Loss:  214.63429307937622\n",
            "------------------------------EPOCH: 1528 ------------------------\n",
            "Epoch [1528/2000], Step [2/8], train_loss: 367.6866\n",
            "Epoch [1528/2000], Step [4/8], train_loss: 628.0374\n",
            "Epoch [1528/2000], Step [6/8], train_loss: 178.5483\n",
            "Epoch [1528/2000], Step [8/8], train_loss: 284.4620\n",
            "Val Loss:  364.22848590215045\n",
            "------------------------------EPOCH: 1529 ------------------------\n",
            "Epoch [1529/2000], Step [2/8], train_loss: 361.5082\n",
            "Epoch [1529/2000], Step [4/8], train_loss: 646.7585\n",
            "Epoch [1529/2000], Step [6/8], train_loss: 179.2779\n",
            "Epoch [1529/2000], Step [8/8], train_loss: 284.6839\n",
            "Val Loss:  187.79824352264404\n",
            "------------------------------EPOCH: 1530 ------------------------\n",
            "Epoch [1530/2000], Step [2/8], train_loss: 365.5660\n",
            "Epoch [1530/2000], Step [4/8], train_loss: 634.6036\n",
            "Epoch [1530/2000], Step [6/8], train_loss: 181.0886\n",
            "Epoch [1530/2000], Step [8/8], train_loss: 284.3351\n",
            "Val Loss:  283.6372287273407\n",
            "------------------------------EPOCH: 1531 ------------------------\n",
            "Epoch [1531/2000], Step [2/8], train_loss: 360.6154\n",
            "Epoch [1531/2000], Step [4/8], train_loss: 650.7859\n",
            "Epoch [1531/2000], Step [6/8], train_loss: 179.5666\n",
            "Epoch [1531/2000], Step [8/8], train_loss: 284.6676\n",
            "Val Loss:  191.32473039627075\n",
            "------------------------------EPOCH: 1532 ------------------------\n",
            "Epoch [1532/2000], Step [2/8], train_loss: 366.2167\n",
            "Epoch [1532/2000], Step [4/8], train_loss: 627.2851\n",
            "Epoch [1532/2000], Step [6/8], train_loss: 179.2269\n",
            "Epoch [1532/2000], Step [8/8], train_loss: 284.3876\n",
            "Val Loss:  372.4384684562683\n",
            "------------------------------EPOCH: 1533 ------------------------\n",
            "Epoch [1533/2000], Step [2/8], train_loss: 361.1981\n",
            "Epoch [1533/2000], Step [4/8], train_loss: 644.1501\n",
            "Epoch [1533/2000], Step [6/8], train_loss: 178.7772\n",
            "Epoch [1533/2000], Step [8/8], train_loss: 284.6761\n",
            "Val Loss:  269.3522170384725\n",
            "------------------------------EPOCH: 1534 ------------------------\n",
            "Epoch [1534/2000], Step [2/8], train_loss: 366.1581\n",
            "Epoch [1534/2000], Step [4/8], train_loss: 627.0790\n",
            "Epoch [1534/2000], Step [6/8], train_loss: 178.9078\n",
            "Epoch [1534/2000], Step [8/8], train_loss: 284.5755\n",
            "Val Loss:  363.60480149586994\n",
            "------------------------------EPOCH: 1535 ------------------------\n",
            "Epoch [1535/2000], Step [2/8], train_loss: 363.0806\n",
            "Epoch [1535/2000], Step [4/8], train_loss: 640.5387\n",
            "Epoch [1535/2000], Step [6/8], train_loss: 178.6655\n",
            "Epoch [1535/2000], Step [8/8], train_loss: 284.7520\n",
            "Val Loss:  305.8940240542094\n",
            "------------------------------EPOCH: 1536 ------------------------\n",
            "Epoch [1536/2000], Step [2/8], train_loss: 365.1090\n",
            "Epoch [1536/2000], Step [4/8], train_loss: 635.6053\n",
            "Epoch [1536/2000], Step [6/8], train_loss: 179.3819\n",
            "Epoch [1536/2000], Step [8/8], train_loss: 284.5426\n",
            "Val Loss:  392.0968345006307\n",
            "------------------------------EPOCH: 1537 ------------------------\n",
            "Epoch [1537/2000], Step [2/8], train_loss: 361.9026\n",
            "Epoch [1537/2000], Step [4/8], train_loss: 642.8586\n",
            "Epoch [1537/2000], Step [6/8], train_loss: 178.2024\n",
            "Epoch [1537/2000], Step [8/8], train_loss: 284.8385\n",
            "Val Loss:  248.86535819371542\n",
            "------------------------------EPOCH: 1538 ------------------------\n",
            "Epoch [1538/2000], Step [2/8], train_loss: 366.0508\n",
            "Epoch [1538/2000], Step [4/8], train_loss: 627.9376\n",
            "Epoch [1538/2000], Step [6/8], train_loss: 178.6391\n",
            "Epoch [1538/2000], Step [8/8], train_loss: 284.5404\n",
            "Val Loss:  393.10538387298584\n",
            "------------------------------EPOCH: 1539 ------------------------\n",
            "Epoch [1539/2000], Step [2/8], train_loss: 361.1735\n",
            "Epoch [1539/2000], Step [4/8], train_loss: 643.2290\n",
            "Epoch [1539/2000], Step [6/8], train_loss: 177.9839\n",
            "Epoch [1539/2000], Step [8/8], train_loss: 284.8120\n",
            "Val Loss:  241.27352333068848\n",
            "------------------------------EPOCH: 1540 ------------------------\n",
            "Epoch [1540/2000], Step [2/8], train_loss: 365.6533\n",
            "Epoch [1540/2000], Step [4/8], train_loss: 628.3242\n",
            "Epoch [1540/2000], Step [6/8], train_loss: 178.1976\n",
            "Epoch [1540/2000], Step [8/8], train_loss: 284.6048\n",
            "Val Loss:  357.44644355773926\n",
            "------------------------------EPOCH: 1541 ------------------------\n",
            "Epoch [1541/2000], Step [2/8], train_loss: 361.3385\n",
            "Epoch [1541/2000], Step [4/8], train_loss: 641.4192\n",
            "Epoch [1541/2000], Step [6/8], train_loss: 178.8387\n",
            "Epoch [1541/2000], Step [8/8], train_loss: 284.7803\n",
            "Val Loss:  251.20374830563864\n",
            "------------------------------EPOCH: 1542 ------------------------\n",
            "Epoch [1542/2000], Step [2/8], train_loss: 363.2272\n",
            "Epoch [1542/2000], Step [4/8], train_loss: 635.6371\n",
            "Epoch [1542/2000], Step [6/8], train_loss: 180.3187\n",
            "Epoch [1542/2000], Step [8/8], train_loss: 284.5093\n",
            "Val Loss:  375.9658284187317\n",
            "------------------------------EPOCH: 1543 ------------------------\n",
            "Epoch [1543/2000], Step [2/8], train_loss: 360.0601\n",
            "Epoch [1543/2000], Step [4/8], train_loss: 645.9449\n",
            "Epoch [1543/2000], Step [6/8], train_loss: 178.4503\n",
            "Epoch [1543/2000], Step [8/8], train_loss: 284.8735\n",
            "Val Loss:  196.16681241989136\n",
            "------------------------------EPOCH: 1544 ------------------------\n",
            "Epoch [1544/2000], Step [2/8], train_loss: 365.7948\n",
            "Epoch [1544/2000], Step [4/8], train_loss: 626.6185\n",
            "Epoch [1544/2000], Step [6/8], train_loss: 178.3996\n",
            "Epoch [1544/2000], Step [8/8], train_loss: 284.5101\n",
            "Val Loss:  321.05402517318726\n",
            "------------------------------EPOCH: 1545 ------------------------\n",
            "Epoch [1545/2000], Step [2/8], train_loss: 359.6052\n",
            "Epoch [1545/2000], Step [4/8], train_loss: 645.7396\n",
            "Epoch [1545/2000], Step [6/8], train_loss: 178.6896\n",
            "Epoch [1545/2000], Step [8/8], train_loss: 284.7855\n",
            "Val Loss:  219.50469621022543\n",
            "------------------------------EPOCH: 1546 ------------------------\n",
            "Epoch [1546/2000], Step [2/8], train_loss: 364.2251\n",
            "Epoch [1546/2000], Step [4/8], train_loss: 627.5374\n",
            "Epoch [1546/2000], Step [6/8], train_loss: 179.6334\n",
            "Epoch [1546/2000], Step [8/8], train_loss: 284.6058\n",
            "Val Loss:  377.56594403584796\n",
            "------------------------------EPOCH: 1547 ------------------------\n",
            "Epoch [1547/2000], Step [2/8], train_loss: 360.3762\n",
            "Epoch [1547/2000], Step [4/8], train_loss: 642.2186\n",
            "Epoch [1547/2000], Step [6/8], train_loss: 178.9756\n",
            "Epoch [1547/2000], Step [8/8], train_loss: 284.8439\n",
            "Val Loss:  300.01165739695233\n",
            "------------------------------EPOCH: 1548 ------------------------\n",
            "Epoch [1548/2000], Step [2/8], train_loss: 363.2931\n",
            "Epoch [1548/2000], Step [4/8], train_loss: 636.2198\n",
            "Epoch [1548/2000], Step [6/8], train_loss: 179.2500\n",
            "Epoch [1548/2000], Step [8/8], train_loss: 284.6086\n",
            "Val Loss:  443.1241463025411\n",
            "------------------------------EPOCH: 1549 ------------------------\n",
            "Epoch [1549/2000], Step [2/8], train_loss: 359.8282\n",
            "Epoch [1549/2000], Step [4/8], train_loss: 644.9349\n",
            "Epoch [1549/2000], Step [6/8], train_loss: 178.0583\n",
            "Epoch [1549/2000], Step [8/8], train_loss: 284.9380\n",
            "Val Loss:  301.7649703025818\n",
            "------------------------------EPOCH: 1550 ------------------------\n",
            "Epoch [1550/2000], Step [2/8], train_loss: 364.8575\n",
            "Epoch [1550/2000], Step [4/8], train_loss: 626.9993\n",
            "Epoch [1550/2000], Step [6/8], train_loss: 178.6393\n",
            "Epoch [1550/2000], Step [8/8], train_loss: 284.6423\n",
            "Val Loss:  356.53383294741315\n",
            "------------------------------EPOCH: 1551 ------------------------\n",
            "Epoch [1551/2000], Step [2/8], train_loss: 360.2073\n",
            "Epoch [1551/2000], Step [4/8], train_loss: 643.4326\n",
            "Epoch [1551/2000], Step [6/8], train_loss: 177.7133\n",
            "Epoch [1551/2000], Step [8/8], train_loss: 284.9150\n",
            "Val Loss:  201.7315093676249\n",
            "------------------------------EPOCH: 1552 ------------------------\n",
            "Epoch [1552/2000], Step [2/8], train_loss: 364.4821\n",
            "Epoch [1552/2000], Step [4/8], train_loss: 627.6567\n",
            "Epoch [1552/2000], Step [6/8], train_loss: 177.7542\n",
            "Epoch [1552/2000], Step [8/8], train_loss: 284.7348\n",
            "Val Loss:  266.12647612889606\n",
            "------------------------------EPOCH: 1553 ------------------------\n",
            "Epoch [1553/2000], Step [2/8], train_loss: 360.0313\n",
            "Epoch [1553/2000], Step [4/8], train_loss: 640.9864\n",
            "Epoch [1553/2000], Step [6/8], train_loss: 178.5963\n",
            "Epoch [1553/2000], Step [8/8], train_loss: 284.9044\n",
            "Val Loss:  193.10025715827942\n",
            "------------------------------EPOCH: 1554 ------------------------\n",
            "Epoch [1554/2000], Step [2/8], train_loss: 361.6132\n",
            "Epoch [1554/2000], Step [4/8], train_loss: 634.1333\n",
            "Epoch [1554/2000], Step [6/8], train_loss: 180.1580\n",
            "Epoch [1554/2000], Step [8/8], train_loss: 284.6536\n",
            "Val Loss:  323.32222922643024\n",
            "------------------------------EPOCH: 1555 ------------------------\n",
            "Epoch [1555/2000], Step [2/8], train_loss: 358.8060\n",
            "Epoch [1555/2000], Step [4/8], train_loss: 645.5606\n",
            "Epoch [1555/2000], Step [6/8], train_loss: 178.3632\n",
            "Epoch [1555/2000], Step [8/8], train_loss: 285.0261\n",
            "Val Loss:  216.3528520266215\n",
            "------------------------------EPOCH: 1556 ------------------------\n",
            "Epoch [1556/2000], Step [2/8], train_loss: 364.5533\n",
            "Epoch [1556/2000], Step [4/8], train_loss: 626.2568\n",
            "Epoch [1556/2000], Step [6/8], train_loss: 178.3720\n",
            "Epoch [1556/2000], Step [8/8], train_loss: 284.7127\n",
            "Val Loss:  322.0925367673238\n",
            "------------------------------EPOCH: 1557 ------------------------\n",
            "Epoch [1557/2000], Step [2/8], train_loss: 359.3727\n",
            "Epoch [1557/2000], Step [4/8], train_loss: 644.0244\n",
            "Epoch [1557/2000], Step [6/8], train_loss: 177.5894\n",
            "Epoch [1557/2000], Step [8/8], train_loss: 285.0161\n",
            "Val Loss:  211.96512031555176\n",
            "------------------------------EPOCH: 1558 ------------------------\n",
            "Epoch [1558/2000], Step [2/8], train_loss: 364.1221\n",
            "Epoch [1558/2000], Step [4/8], train_loss: 628.1732\n",
            "Epoch [1558/2000], Step [6/8], train_loss: 177.9397\n",
            "Epoch [1558/2000], Step [8/8], train_loss: 284.8470\n",
            "Val Loss:  365.85753472646076\n",
            "------------------------------EPOCH: 1559 ------------------------\n",
            "Epoch [1559/2000], Step [2/8], train_loss: 360.0735\n",
            "Epoch [1559/2000], Step [4/8], train_loss: 641.5092\n",
            "Epoch [1559/2000], Step [6/8], train_loss: 177.8037\n",
            "Epoch [1559/2000], Step [8/8], train_loss: 285.0650\n",
            "Val Loss:  257.58433111508685\n",
            "------------------------------EPOCH: 1560 ------------------------\n",
            "Epoch [1560/2000], Step [2/8], train_loss: 362.5770\n",
            "Epoch [1560/2000], Step [4/8], train_loss: 635.8245\n",
            "Epoch [1560/2000], Step [6/8], train_loss: 178.9595\n",
            "Epoch [1560/2000], Step [8/8], train_loss: 284.8013\n",
            "Val Loss:  366.2027870814006\n",
            "------------------------------EPOCH: 1561 ------------------------\n",
            "Epoch [1561/2000], Step [2/8], train_loss: 359.0292\n",
            "Epoch [1561/2000], Step [4/8], train_loss: 645.6010\n",
            "Epoch [1561/2000], Step [6/8], train_loss: 177.3248\n",
            "Epoch [1561/2000], Step [8/8], train_loss: 285.1174\n",
            "Val Loss:  213.7773601214091\n",
            "------------------------------EPOCH: 1562 ------------------------\n",
            "Epoch [1562/2000], Step [2/8], train_loss: 363.6737\n",
            "Epoch [1562/2000], Step [4/8], train_loss: 627.8710\n",
            "Epoch [1562/2000], Step [6/8], train_loss: 177.5905\n",
            "Epoch [1562/2000], Step [8/8], train_loss: 284.7196\n",
            "Val Loss:  360.3949851989746\n",
            "------------------------------EPOCH: 1563 ------------------------\n",
            "Epoch [1563/2000], Step [2/8], train_loss: 357.5892\n",
            "Epoch [1563/2000], Step [4/8], train_loss: 644.5974\n",
            "Epoch [1563/2000], Step [6/8], train_loss: 178.1108\n",
            "Epoch [1563/2000], Step [8/8], train_loss: 284.9888\n",
            "Val Loss:  233.61048261324564\n",
            "------------------------------EPOCH: 1564 ------------------------\n",
            "Epoch [1564/2000], Step [2/8], train_loss: 362.2700\n",
            "Epoch [1564/2000], Step [4/8], train_loss: 627.0057\n",
            "Epoch [1564/2000], Step [6/8], train_loss: 179.2281\n",
            "Epoch [1564/2000], Step [8/8], train_loss: 284.7941\n",
            "Val Loss:  298.6673113505046\n",
            "------------------------------EPOCH: 1565 ------------------------\n",
            "Epoch [1565/2000], Step [2/8], train_loss: 358.5675\n",
            "Epoch [1565/2000], Step [4/8], train_loss: 642.1548\n",
            "Epoch [1565/2000], Step [6/8], train_loss: 178.5302\n",
            "Epoch [1565/2000], Step [8/8], train_loss: 285.0151\n",
            "Val Loss:  218.86374759674072\n",
            "------------------------------EPOCH: 1566 ------------------------\n",
            "Epoch [1566/2000], Step [2/8], train_loss: 361.1624\n",
            "Epoch [1566/2000], Step [4/8], train_loss: 636.1899\n",
            "Epoch [1566/2000], Step [6/8], train_loss: 178.7585\n",
            "Epoch [1566/2000], Step [8/8], train_loss: 284.7882\n",
            "Val Loss:  358.1173226038615\n",
            "------------------------------EPOCH: 1567 ------------------------\n",
            "Epoch [1567/2000], Step [2/8], train_loss: 358.1556\n",
            "Epoch [1567/2000], Step [4/8], train_loss: 645.0601\n",
            "Epoch [1567/2000], Step [6/8], train_loss: 177.2320\n",
            "Epoch [1567/2000], Step [8/8], train_loss: 285.1982\n",
            "Val Loss:  229.88710816701254\n",
            "------------------------------EPOCH: 1568 ------------------------\n",
            "Epoch [1568/2000], Step [2/8], train_loss: 364.5275\n",
            "Epoch [1568/2000], Step [4/8], train_loss: 626.4957\n",
            "Epoch [1568/2000], Step [6/8], train_loss: 177.9694\n",
            "Epoch [1568/2000], Step [8/8], train_loss: 284.8953\n",
            "Val Loss:  432.72013632456463\n",
            "------------------------------EPOCH: 1569 ------------------------\n",
            "Epoch [1569/2000], Step [2/8], train_loss: 358.7759\n",
            "Epoch [1569/2000], Step [4/8], train_loss: 644.4529\n",
            "Epoch [1569/2000], Step [6/8], train_loss: 177.1707\n",
            "Epoch [1569/2000], Step [8/8], train_loss: 285.1259\n",
            "Val Loss:  291.9715592066447\n",
            "------------------------------EPOCH: 1570 ------------------------\n",
            "Epoch [1570/2000], Step [2/8], train_loss: 361.5556\n",
            "Epoch [1570/2000], Step [4/8], train_loss: 630.4600\n",
            "Epoch [1570/2000], Step [6/8], train_loss: 177.2995\n",
            "Epoch [1570/2000], Step [8/8], train_loss: 284.8907\n",
            "Val Loss:  414.4120349884033\n",
            "------------------------------EPOCH: 1571 ------------------------\n",
            "Epoch [1571/2000], Step [2/8], train_loss: 357.3181\n",
            "Epoch [1571/2000], Step [4/8], train_loss: 642.6334\n",
            "Epoch [1571/2000], Step [6/8], train_loss: 178.0734\n",
            "Epoch [1571/2000], Step [8/8], train_loss: 285.1116\n",
            "Val Loss:  285.5182194709778\n",
            "------------------------------EPOCH: 1572 ------------------------\n",
            "Epoch [1572/2000], Step [2/8], train_loss: 360.8628\n",
            "Epoch [1572/2000], Step [4/8], train_loss: 631.5797\n",
            "Epoch [1572/2000], Step [6/8], train_loss: 179.2493\n",
            "Epoch [1572/2000], Step [8/8], train_loss: 284.9056\n",
            "Val Loss:  456.10895411173504\n",
            "------------------------------EPOCH: 1573 ------------------------\n",
            "Epoch [1573/2000], Step [2/8], train_loss: 357.2529\n",
            "Epoch [1573/2000], Step [4/8], train_loss: 641.6429\n",
            "Epoch [1573/2000], Step [6/8], train_loss: 178.4298\n",
            "Epoch [1573/2000], Step [8/8], train_loss: 285.0846\n",
            "Val Loss:  327.32785447438556\n",
            "------------------------------EPOCH: 1574 ------------------------\n",
            "Epoch [1574/2000], Step [2/8], train_loss: 360.0138\n",
            "Epoch [1574/2000], Step [4/8], train_loss: 632.6760\n",
            "Epoch [1574/2000], Step [6/8], train_loss: 178.0864\n",
            "Epoch [1574/2000], Step [8/8], train_loss: 285.0225\n",
            "Val Loss:  353.8221443494161\n",
            "------------------------------EPOCH: 1575 ------------------------\n",
            "Epoch [1575/2000], Step [2/8], train_loss: 358.8596\n",
            "Epoch [1575/2000], Step [4/8], train_loss: 638.4886\n",
            "Epoch [1575/2000], Step [6/8], train_loss: 177.6217\n",
            "Epoch [1575/2000], Step [8/8], train_loss: 285.2005\n",
            "Val Loss:  277.96752230326337\n",
            "------------------------------EPOCH: 1576 ------------------------\n",
            "Epoch [1576/2000], Step [2/8], train_loss: 360.7721\n",
            "Epoch [1576/2000], Step [4/8], train_loss: 630.1063\n",
            "Epoch [1576/2000], Step [6/8], train_loss: 177.7918\n",
            "Epoch [1576/2000], Step [8/8], train_loss: 285.1621\n",
            "Val Loss:  344.79609870910645\n",
            "------------------------------EPOCH: 1577 ------------------------\n",
            "Epoch [1577/2000], Step [2/8], train_loss: 358.7003\n",
            "Epoch [1577/2000], Step [4/8], train_loss: 636.5935\n",
            "Epoch [1577/2000], Step [6/8], train_loss: 177.5802\n",
            "Epoch [1577/2000], Step [8/8], train_loss: 285.3482\n",
            "Val Loss:  263.19739564259845\n",
            "------------------------------EPOCH: 1578 ------------------------\n",
            "Epoch [1578/2000], Step [2/8], train_loss: 360.3039\n",
            "Epoch [1578/2000], Step [4/8], train_loss: 636.0575\n",
            "Epoch [1578/2000], Step [6/8], train_loss: 178.3676\n",
            "Epoch [1578/2000], Step [8/8], train_loss: 285.2094\n",
            "Val Loss:  364.87472041447955\n",
            "------------------------------EPOCH: 1579 ------------------------\n",
            "Epoch [1579/2000], Step [2/8], train_loss: 358.4842\n",
            "Epoch [1579/2000], Step [4/8], train_loss: 639.6677\n",
            "Epoch [1579/2000], Step [6/8], train_loss: 176.5942\n",
            "Epoch [1579/2000], Step [8/8], train_loss: 285.5683\n",
            "Val Loss:  248.86198663711548\n",
            "------------------------------EPOCH: 1580 ------------------------\n",
            "Epoch [1580/2000], Step [2/8], train_loss: 362.7892\n",
            "Epoch [1580/2000], Step [4/8], train_loss: 629.5065\n",
            "Epoch [1580/2000], Step [6/8], train_loss: 177.7641\n",
            "Epoch [1580/2000], Step [8/8], train_loss: 285.1686\n",
            "Val Loss:  392.61511182785034\n",
            "------------------------------EPOCH: 1581 ------------------------\n",
            "Epoch [1581/2000], Step [2/8], train_loss: 356.8378\n",
            "Epoch [1581/2000], Step [4/8], train_loss: 644.1502\n",
            "Epoch [1581/2000], Step [6/8], train_loss: 176.8479\n",
            "Epoch [1581/2000], Step [8/8], train_loss: 285.4595\n",
            "Val Loss:  208.63176695505777\n",
            "------------------------------EPOCH: 1582 ------------------------\n",
            "Epoch [1582/2000], Step [2/8], train_loss: 361.3462\n",
            "Epoch [1582/2000], Step [4/8], train_loss: 629.6423\n",
            "Epoch [1582/2000], Step [6/8], train_loss: 177.5259\n",
            "Epoch [1582/2000], Step [8/8], train_loss: 285.1237\n",
            "Val Loss:  348.39928674697876\n",
            "------------------------------EPOCH: 1583 ------------------------\n",
            "Epoch [1583/2000], Step [2/8], train_loss: 356.5952\n",
            "Epoch [1583/2000], Step [4/8], train_loss: 645.5828\n",
            "Epoch [1583/2000], Step [6/8], train_loss: 177.7873\n",
            "Epoch [1583/2000], Step [8/8], train_loss: 285.3608\n",
            "Val Loss:  226.1328207651774\n",
            "------------------------------EPOCH: 1584 ------------------------\n",
            "Epoch [1584/2000], Step [2/8], train_loss: 359.7165\n",
            "Epoch [1584/2000], Step [4/8], train_loss: 633.0159\n",
            "Epoch [1584/2000], Step [6/8], train_loss: 179.5800\n",
            "Epoch [1584/2000], Step [8/8], train_loss: 285.0000\n",
            "Val Loss:  364.1251986026764\n",
            "------------------------------EPOCH: 1585 ------------------------\n",
            "Epoch [1585/2000], Step [2/8], train_loss: 355.2972\n",
            "Epoch [1585/2000], Step [4/8], train_loss: 650.0037\n",
            "Epoch [1585/2000], Step [6/8], train_loss: 177.4909\n",
            "Epoch [1585/2000], Step [8/8], train_loss: 285.4577\n",
            "Val Loss:  181.42737491925558\n",
            "------------------------------EPOCH: 1586 ------------------------\n",
            "Epoch [1586/2000], Step [2/8], train_loss: 362.5841\n",
            "Epoch [1586/2000], Step [4/8], train_loss: 623.3293\n",
            "Epoch [1586/2000], Step [6/8], train_loss: 177.5195\n",
            "Epoch [1586/2000], Step [8/8], train_loss: 285.1238\n",
            "Val Loss:  401.2818703651428\n",
            "------------------------------EPOCH: 1587 ------------------------\n",
            "Epoch [1587/2000], Step [2/8], train_loss: 356.1032\n",
            "Epoch [1587/2000], Step [4/8], train_loss: 647.1902\n",
            "Epoch [1587/2000], Step [6/8], train_loss: 177.1345\n",
            "Epoch [1587/2000], Step [8/8], train_loss: 285.4270\n",
            "Val Loss:  283.9261306126912\n",
            "------------------------------EPOCH: 1588 ------------------------\n",
            "Epoch [1588/2000], Step [2/8], train_loss: 360.5885\n",
            "Epoch [1588/2000], Step [4/8], train_loss: 633.8926\n",
            "Epoch [1588/2000], Step [6/8], train_loss: 178.2298\n",
            "Epoch [1588/2000], Step [8/8], train_loss: 285.1207\n",
            "Val Loss:  464.678466796875\n",
            "------------------------------EPOCH: 1589 ------------------------\n",
            "Epoch [1589/2000], Step [2/8], train_loss: 355.6402\n",
            "Epoch [1589/2000], Step [4/8], train_loss: 649.6621\n",
            "Epoch [1589/2000], Step [6/8], train_loss: 177.0035\n",
            "Epoch [1589/2000], Step [8/8], train_loss: 285.4626\n",
            "Val Loss:  267.07100343704224\n",
            "------------------------------EPOCH: 1590 ------------------------\n",
            "Epoch [1590/2000], Step [2/8], train_loss: 360.5677\n",
            "Epoch [1590/2000], Step [4/8], train_loss: 626.9169\n",
            "Epoch [1590/2000], Step [6/8], train_loss: 177.5253\n",
            "Epoch [1590/2000], Step [8/8], train_loss: 285.1270\n",
            "Val Loss:  465.46787039438885\n",
            "------------------------------EPOCH: 1591 ------------------------\n",
            "Epoch [1591/2000], Step [2/8], train_loss: 355.1900\n",
            "Epoch [1591/2000], Step [4/8], train_loss: 643.5754\n",
            "Epoch [1591/2000], Step [6/8], train_loss: 177.1272\n",
            "Epoch [1591/2000], Step [8/8], train_loss: 285.4378\n",
            "Val Loss:  284.99643580118817\n",
            "------------------------------EPOCH: 1592 ------------------------\n",
            "Epoch [1592/2000], Step [2/8], train_loss: 360.2207\n",
            "Epoch [1592/2000], Step [4/8], train_loss: 638.5314\n",
            "Epoch [1592/2000], Step [6/8], train_loss: 178.1398\n",
            "Epoch [1592/2000], Step [8/8], train_loss: 285.1116\n",
            "Val Loss:  491.34826548894245\n",
            "------------------------------EPOCH: 1593 ------------------------\n",
            "Epoch [1593/2000], Step [2/8], train_loss: 355.8914\n",
            "Epoch [1593/2000], Step [4/8], train_loss: 652.4142\n",
            "Epoch [1593/2000], Step [6/8], train_loss: 176.2916\n",
            "Epoch [1593/2000], Step [8/8], train_loss: 285.4617\n",
            "Val Loss:  301.8928979237874\n",
            "------------------------------EPOCH: 1594 ------------------------\n",
            "Epoch [1594/2000], Step [2/8], train_loss: 361.3380\n",
            "Epoch [1594/2000], Step [4/8], train_loss: 626.4006\n",
            "Epoch [1594/2000], Step [6/8], train_loss: 176.4590\n",
            "Epoch [1594/2000], Step [8/8], train_loss: 285.0458\n",
            "Val Loss:  483.61724694569904\n",
            "------------------------------EPOCH: 1595 ------------------------\n",
            "Epoch [1595/2000], Step [2/8], train_loss: 354.1803\n",
            "Epoch [1595/2000], Step [4/8], train_loss: 647.3170\n",
            "Epoch [1595/2000], Step [6/8], train_loss: 177.4385\n",
            "Epoch [1595/2000], Step [8/8], train_loss: 285.2993\n",
            "Val Loss:  291.8472137451172\n",
            "------------------------------EPOCH: 1596 ------------------------\n",
            "Epoch [1596/2000], Step [2/8], train_loss: 358.3908\n",
            "Epoch [1596/2000], Step [4/8], train_loss: 634.4355\n",
            "Epoch [1596/2000], Step [6/8], train_loss: 179.3729\n",
            "Epoch [1596/2000], Step [8/8], train_loss: 284.9786\n",
            "Val Loss:  477.8758249282837\n",
            "------------------------------EPOCH: 1597 ------------------------\n",
            "Epoch [1597/2000], Step [2/8], train_loss: 354.0119\n",
            "Epoch [1597/2000], Step [4/8], train_loss: 651.1211\n",
            "Epoch [1597/2000], Step [6/8], train_loss: 177.5525\n",
            "Epoch [1597/2000], Step [8/8], train_loss: 285.3351\n",
            "Val Loss:  289.54029687245685\n",
            "------------------------------EPOCH: 1598 ------------------------\n",
            "Epoch [1598/2000], Step [2/8], train_loss: 359.6559\n",
            "Epoch [1598/2000], Step [4/8], train_loss: 625.4452\n",
            "Epoch [1598/2000], Step [6/8], train_loss: 177.2240\n",
            "Epoch [1598/2000], Step [8/8], train_loss: 285.0524\n",
            "Val Loss:  419.1947145462036\n",
            "------------------------------EPOCH: 1599 ------------------------\n",
            "Epoch [1599/2000], Step [2/8], train_loss: 354.5855\n",
            "Epoch [1599/2000], Step [4/8], train_loss: 642.8218\n",
            "Epoch [1599/2000], Step [6/8], train_loss: 176.9063\n",
            "Epoch [1599/2000], Step [8/8], train_loss: 285.3428\n",
            "Val Loss:  280.78369251887005\n",
            "------------------------------EPOCH: 1600 ------------------------\n",
            "Epoch [1600/2000], Step [2/8], train_loss: 359.0315\n",
            "Epoch [1600/2000], Step [4/8], train_loss: 637.4832\n",
            "Epoch [1600/2000], Step [6/8], train_loss: 177.9519\n",
            "Epoch [1600/2000], Step [8/8], train_loss: 285.0632\n",
            "Val Loss:  501.0673252741496\n",
            "------------------------------EPOCH: 1601 ------------------------\n",
            "Epoch [1601/2000], Step [2/8], train_loss: 355.0573\n",
            "Epoch [1601/2000], Step [4/8], train_loss: 650.4923\n",
            "Epoch [1601/2000], Step [6/8], train_loss: 176.2237\n",
            "Epoch [1601/2000], Step [8/8], train_loss: 285.4016\n",
            "Val Loss:  281.9551436106364\n",
            "------------------------------EPOCH: 1602 ------------------------\n",
            "Epoch [1602/2000], Step [2/8], train_loss: 359.9253\n",
            "Epoch [1602/2000], Step [4/8], train_loss: 626.4617\n",
            "Epoch [1602/2000], Step [6/8], train_loss: 176.3024\n",
            "Epoch [1602/2000], Step [8/8], train_loss: 285.0306\n",
            "Val Loss:  439.29758675893146\n",
            "------------------------------EPOCH: 1603 ------------------------\n",
            "Epoch [1603/2000], Step [2/8], train_loss: 353.4019\n",
            "Epoch [1603/2000], Step [4/8], train_loss: 645.1814\n",
            "Epoch [1603/2000], Step [6/8], train_loss: 177.1560\n",
            "Epoch [1603/2000], Step [8/8], train_loss: 285.2847\n",
            "Val Loss:  331.0382620493571\n",
            "------------------------------EPOCH: 1604 ------------------------\n",
            "Epoch [1604/2000], Step [2/8], train_loss: 357.3889\n",
            "Epoch [1604/2000], Step [4/8], train_loss: 634.3752\n",
            "Epoch [1604/2000], Step [6/8], train_loss: 179.0404\n",
            "Epoch [1604/2000], Step [8/8], train_loss: 284.9799\n",
            "Val Loss:  503.66540002822876\n",
            "------------------------------EPOCH: 1605 ------------------------\n",
            "Epoch [1605/2000], Step [2/8], train_loss: 353.1772\n",
            "Epoch [1605/2000], Step [4/8], train_loss: 649.4871\n",
            "Epoch [1605/2000], Step [6/8], train_loss: 177.1415\n",
            "Epoch [1605/2000], Step [8/8], train_loss: 285.3277\n",
            "Val Loss:  280.0974175135295\n",
            "------------------------------EPOCH: 1606 ------------------------\n",
            "Epoch [1606/2000], Step [2/8], train_loss: 358.3405\n",
            "Epoch [1606/2000], Step [4/8], train_loss: 627.4462\n",
            "Epoch [1606/2000], Step [6/8], train_loss: 176.8134\n",
            "Epoch [1606/2000], Step [8/8], train_loss: 284.9911\n",
            "Val Loss:  437.25273005167645\n",
            "------------------------------EPOCH: 1607 ------------------------\n",
            "Epoch [1607/2000], Step [2/8], train_loss: 352.8171\n",
            "Epoch [1607/2000], Step [4/8], train_loss: 645.0380\n",
            "Epoch [1607/2000], Step [6/8], train_loss: 177.2211\n",
            "Epoch [1607/2000], Step [8/8], train_loss: 285.2707\n",
            "Val Loss:  227.21827014287314\n",
            "------------------------------EPOCH: 1608 ------------------------\n",
            "Epoch [1608/2000], Step [2/8], train_loss: 357.1881\n",
            "Epoch [1608/2000], Step [4/8], train_loss: 633.4355\n",
            "Epoch [1608/2000], Step [6/8], train_loss: 178.8074\n",
            "Epoch [1608/2000], Step [8/8], train_loss: 284.9942\n",
            "Val Loss:  422.539523601532\n",
            "------------------------------EPOCH: 1609 ------------------------\n",
            "Epoch [1609/2000], Step [2/8], train_loss: 353.0222\n",
            "Epoch [1609/2000], Step [4/8], train_loss: 649.5272\n",
            "Epoch [1609/2000], Step [6/8], train_loss: 177.0775\n",
            "Epoch [1609/2000], Step [8/8], train_loss: 285.3495\n",
            "Val Loss:  256.1132532755534\n",
            "------------------------------EPOCH: 1610 ------------------------\n",
            "Epoch [1610/2000], Step [2/8], train_loss: 358.1577\n",
            "Epoch [1610/2000], Step [4/8], train_loss: 625.3591\n",
            "Epoch [1610/2000], Step [6/8], train_loss: 176.9750\n",
            "Epoch [1610/2000], Step [8/8], train_loss: 285.0812\n",
            "Val Loss:  339.1807125409444\n",
            "------------------------------EPOCH: 1611 ------------------------\n",
            "Epoch [1611/2000], Step [2/8], train_loss: 353.0969\n",
            "Epoch [1611/2000], Step [4/8], train_loss: 642.2631\n",
            "Epoch [1611/2000], Step [6/8], train_loss: 176.6310\n",
            "Epoch [1611/2000], Step [8/8], train_loss: 285.3779\n",
            "Val Loss:  224.55750958124796\n",
            "------------------------------EPOCH: 1612 ------------------------\n",
            "Epoch [1612/2000], Step [2/8], train_loss: 357.5397\n",
            "Epoch [1612/2000], Step [4/8], train_loss: 625.7605\n",
            "Epoch [1612/2000], Step [6/8], train_loss: 176.7695\n",
            "Epoch [1612/2000], Step [8/8], train_loss: 285.2900\n",
            "Val Loss:  270.9985203742981\n",
            "------------------------------EPOCH: 1613 ------------------------\n",
            "Epoch [1613/2000], Step [2/8], train_loss: 354.6372\n",
            "Epoch [1613/2000], Step [4/8], train_loss: 638.6184\n",
            "Epoch [1613/2000], Step [6/8], train_loss: 176.6609\n",
            "Epoch [1613/2000], Step [8/8], train_loss: 285.4644\n",
            "Val Loss:  228.06973854700723\n",
            "------------------------------EPOCH: 1614 ------------------------\n",
            "Epoch [1614/2000], Step [2/8], train_loss: 356.2214\n",
            "Epoch [1614/2000], Step [4/8], train_loss: 634.1005\n",
            "Epoch [1614/2000], Step [6/8], train_loss: 177.3050\n",
            "Epoch [1614/2000], Step [8/8], train_loss: 285.2985\n",
            "Val Loss:  407.2386426925659\n",
            "------------------------------EPOCH: 1615 ------------------------\n",
            "Epoch [1615/2000], Step [2/8], train_loss: 354.1373\n",
            "Epoch [1615/2000], Step [4/8], train_loss: 641.9069\n",
            "Epoch [1615/2000], Step [6/8], train_loss: 175.5374\n",
            "Epoch [1615/2000], Step [8/8], train_loss: 285.6215\n",
            "Val Loss:  251.08772563934326\n",
            "------------------------------EPOCH: 1616 ------------------------\n",
            "Epoch [1616/2000], Step [2/8], train_loss: 358.0106\n",
            "Epoch [1616/2000], Step [4/8], train_loss: 627.3077\n",
            "Epoch [1616/2000], Step [6/8], train_loss: 176.0123\n",
            "Epoch [1616/2000], Step [8/8], train_loss: 285.2849\n",
            "Val Loss:  349.9269684155782\n",
            "------------------------------EPOCH: 1617 ------------------------\n",
            "Epoch [1617/2000], Step [2/8], train_loss: 352.4829\n",
            "Epoch [1617/2000], Step [4/8], train_loss: 641.4374\n",
            "Epoch [1617/2000], Step [6/8], train_loss: 176.6234\n",
            "Epoch [1617/2000], Step [8/8], train_loss: 285.5402\n",
            "Val Loss:  200.62583462397257\n",
            "------------------------------EPOCH: 1618 ------------------------\n",
            "Epoch [1618/2000], Step [2/8], train_loss: 356.3401\n",
            "Epoch [1618/2000], Step [4/8], train_loss: 626.1793\n",
            "Epoch [1618/2000], Step [6/8], train_loss: 177.3922\n",
            "Epoch [1618/2000], Step [8/8], train_loss: 285.4073\n",
            "Val Loss:  358.5032130877177\n",
            "------------------------------EPOCH: 1619 ------------------------\n",
            "Epoch [1619/2000], Step [2/8], train_loss: 353.2315\n",
            "Epoch [1619/2000], Step [4/8], train_loss: 638.0827\n",
            "Epoch [1619/2000], Step [6/8], train_loss: 176.4745\n",
            "Epoch [1619/2000], Step [8/8], train_loss: 285.6538\n",
            "Val Loss:  295.0680847167969\n",
            "------------------------------EPOCH: 1620 ------------------------\n",
            "Epoch [1620/2000], Step [2/8], train_loss: 356.0360\n",
            "Epoch [1620/2000], Step [4/8], train_loss: 632.6731\n",
            "Epoch [1620/2000], Step [6/8], train_loss: 177.1632\n",
            "Epoch [1620/2000], Step [8/8], train_loss: 285.4974\n",
            "Val Loss:  397.00642999013263\n",
            "------------------------------EPOCH: 1621 ------------------------\n",
            "Epoch [1621/2000], Step [2/8], train_loss: 353.2939\n",
            "Epoch [1621/2000], Step [4/8], train_loss: 643.9442\n",
            "Epoch [1621/2000], Step [6/8], train_loss: 175.5577\n",
            "Epoch [1621/2000], Step [8/8], train_loss: 285.8377\n",
            "Val Loss:  239.746467034022\n",
            "------------------------------EPOCH: 1622 ------------------------\n",
            "Epoch [1622/2000], Step [2/8], train_loss: 357.0331\n",
            "Epoch [1622/2000], Step [4/8], train_loss: 627.2240\n",
            "Epoch [1622/2000], Step [6/8], train_loss: 176.1142\n",
            "Epoch [1622/2000], Step [8/8], train_loss: 285.4776\n",
            "Val Loss:  395.71780665715534\n",
            "------------------------------EPOCH: 1623 ------------------------\n",
            "Epoch [1623/2000], Step [2/8], train_loss: 351.1240\n",
            "Epoch [1623/2000], Step [4/8], train_loss: 643.0458\n",
            "Epoch [1623/2000], Step [6/8], train_loss: 176.6301\n",
            "Epoch [1623/2000], Step [8/8], train_loss: 285.7487\n",
            "Val Loss:  252.4406189918518\n",
            "------------------------------EPOCH: 1624 ------------------------\n",
            "Epoch [1624/2000], Step [2/8], train_loss: 355.2320\n",
            "Epoch [1624/2000], Step [4/8], train_loss: 626.4180\n",
            "Epoch [1624/2000], Step [6/8], train_loss: 177.6741\n",
            "Epoch [1624/2000], Step [8/8], train_loss: 285.5960\n",
            "Val Loss:  396.4476367632548\n",
            "------------------------------EPOCH: 1625 ------------------------\n",
            "Epoch [1625/2000], Step [2/8], train_loss: 352.4529\n",
            "Epoch [1625/2000], Step [4/8], train_loss: 640.7351\n",
            "Epoch [1625/2000], Step [6/8], train_loss: 176.7602\n",
            "Epoch [1625/2000], Step [8/8], train_loss: 285.8534\n",
            "Val Loss:  314.22849464416504\n",
            "------------------------------EPOCH: 1626 ------------------------\n",
            "Epoch [1626/2000], Step [2/8], train_loss: 355.1110\n",
            "Epoch [1626/2000], Step [4/8], train_loss: 634.6368\n",
            "Epoch [1626/2000], Step [6/8], train_loss: 177.1422\n",
            "Epoch [1626/2000], Step [8/8], train_loss: 285.6369\n",
            "Val Loss:  460.630033493042\n",
            "------------------------------EPOCH: 1627 ------------------------\n",
            "Epoch [1627/2000], Step [2/8], train_loss: 352.0962\n",
            "Epoch [1627/2000], Step [4/8], train_loss: 643.3979\n",
            "Epoch [1627/2000], Step [6/8], train_loss: 175.4986\n",
            "Epoch [1627/2000], Step [8/8], train_loss: 286.0199\n",
            "Val Loss:  244.14596446355185\n",
            "------------------------------EPOCH: 1628 ------------------------\n",
            "Epoch [1628/2000], Step [2/8], train_loss: 357.2058\n",
            "Epoch [1628/2000], Step [4/8], train_loss: 625.1993\n",
            "Epoch [1628/2000], Step [6/8], train_loss: 176.4749\n",
            "Epoch [1628/2000], Step [8/8], train_loss: 285.7020\n",
            "Val Loss:  420.8182412783305\n",
            "------------------------------EPOCH: 1629 ------------------------\n",
            "Epoch [1629/2000], Step [2/8], train_loss: 351.9816\n",
            "Epoch [1629/2000], Step [4/8], train_loss: 642.3611\n",
            "Epoch [1629/2000], Step [6/8], train_loss: 175.4939\n",
            "Epoch [1629/2000], Step [8/8], train_loss: 286.0198\n",
            "Val Loss:  261.0980221430461\n",
            "------------------------------EPOCH: 1630 ------------------------\n",
            "Epoch [1630/2000], Step [2/8], train_loss: 355.8430\n",
            "Epoch [1630/2000], Step [4/8], train_loss: 637.9789\n",
            "Epoch [1630/2000], Step [6/8], train_loss: 176.2899\n",
            "Epoch [1630/2000], Step [8/8], train_loss: 285.6359\n",
            "Val Loss:  498.53095118204754\n",
            "------------------------------EPOCH: 1631 ------------------------\n",
            "Epoch [1631/2000], Step [2/8], train_loss: 350.5514\n",
            "Epoch [1631/2000], Step [4/8], train_loss: 650.4948\n",
            "Epoch [1631/2000], Step [6/8], train_loss: 175.8483\n",
            "Epoch [1631/2000], Step [8/8], train_loss: 286.0132\n",
            "Val Loss:  321.6497173309326\n",
            "------------------------------EPOCH: 1632 ------------------------\n",
            "Epoch [1632/2000], Step [2/8], train_loss: 356.1678\n",
            "Epoch [1632/2000], Step [4/8], train_loss: 622.3246\n",
            "Epoch [1632/2000], Step [6/8], train_loss: 177.3207\n",
            "Epoch [1632/2000], Step [8/8], train_loss: 285.6268\n",
            "Val Loss:  489.6363350550334\n",
            "------------------------------EPOCH: 1633 ------------------------\n",
            "Epoch [1633/2000], Step [2/8], train_loss: 349.9971\n",
            "Epoch [1633/2000], Step [4/8], train_loss: 646.2666\n",
            "Epoch [1633/2000], Step [6/8], train_loss: 176.8514\n",
            "Epoch [1633/2000], Step [8/8], train_loss: 285.9376\n",
            "Val Loss:  317.44065777460736\n",
            "------------------------------EPOCH: 1634 ------------------------\n",
            "Epoch [1634/2000], Step [2/8], train_loss: 354.8249\n",
            "Epoch [1634/2000], Step [4/8], train_loss: 631.1558\n",
            "Epoch [1634/2000], Step [6/8], train_loss: 177.1341\n",
            "Epoch [1634/2000], Step [8/8], train_loss: 285.6815\n",
            "Val Loss:  517.0741802851359\n",
            "------------------------------EPOCH: 1635 ------------------------\n",
            "Epoch [1635/2000], Step [2/8], train_loss: 350.5872\n",
            "Epoch [1635/2000], Step [4/8], train_loss: 647.7077\n",
            "Epoch [1635/2000], Step [6/8], train_loss: 175.6784\n",
            "Epoch [1635/2000], Step [8/8], train_loss: 286.0477\n",
            "Val Loss:  328.31368255615234\n",
            "------------------------------EPOCH: 1636 ------------------------\n",
            "Epoch [1636/2000], Step [2/8], train_loss: 355.3205\n",
            "Epoch [1636/2000], Step [4/8], train_loss: 623.5430\n",
            "Epoch [1636/2000], Step [6/8], train_loss: 176.1059\n",
            "Epoch [1636/2000], Step [8/8], train_loss: 285.7606\n",
            "Val Loss:  423.0592730840047\n",
            "------------------------------EPOCH: 1637 ------------------------\n",
            "Epoch [1637/2000], Step [2/8], train_loss: 350.3436\n",
            "Epoch [1637/2000], Step [4/8], train_loss: 642.6306\n",
            "Epoch [1637/2000], Step [6/8], train_loss: 175.9265\n",
            "Epoch [1637/2000], Step [8/8], train_loss: 285.9998\n",
            "Val Loss:  292.87705691655475\n",
            "------------------------------EPOCH: 1638 ------------------------\n",
            "Epoch [1638/2000], Step [2/8], train_loss: 353.9917\n",
            "Epoch [1638/2000], Step [4/8], train_loss: 636.8337\n",
            "Epoch [1638/2000], Step [6/8], train_loss: 176.8476\n",
            "Epoch [1638/2000], Step [8/8], train_loss: 285.7322\n",
            "Val Loss:  397.5451930363973\n",
            "------------------------------EPOCH: 1639 ------------------------\n",
            "Epoch [1639/2000], Step [2/8], train_loss: 350.3554\n",
            "Epoch [1639/2000], Step [4/8], train_loss: 647.2267\n",
            "Epoch [1639/2000], Step [6/8], train_loss: 175.1687\n",
            "Epoch [1639/2000], Step [8/8], train_loss: 286.0598\n",
            "Val Loss:  272.8324303627014\n",
            "------------------------------EPOCH: 1640 ------------------------\n",
            "Epoch [1640/2000], Step [2/8], train_loss: 354.6410\n",
            "Epoch [1640/2000], Step [4/8], train_loss: 626.7275\n",
            "Epoch [1640/2000], Step [6/8], train_loss: 175.5172\n",
            "Epoch [1640/2000], Step [8/8], train_loss: 285.6949\n",
            "Val Loss:  488.8509775797526\n",
            "------------------------------EPOCH: 1641 ------------------------\n",
            "Epoch [1641/2000], Step [2/8], train_loss: 348.5530\n",
            "Epoch [1641/2000], Step [4/8], train_loss: 643.5898\n",
            "Epoch [1641/2000], Step [6/8], train_loss: 176.3709\n",
            "Epoch [1641/2000], Step [8/8], train_loss: 285.9484\n",
            "Val Loss:  336.74522972106934\n",
            "------------------------------EPOCH: 1642 ------------------------\n",
            "Epoch [1642/2000], Step [2/8], train_loss: 352.4908\n",
            "Epoch [1642/2000], Step [4/8], train_loss: 634.6915\n",
            "Epoch [1642/2000], Step [6/8], train_loss: 178.2655\n",
            "Epoch [1642/2000], Step [8/8], train_loss: 285.6321\n",
            "Val Loss:  466.3167498906453\n",
            "------------------------------EPOCH: 1643 ------------------------\n",
            "Epoch [1643/2000], Step [2/8], train_loss: 348.6660\n",
            "Epoch [1643/2000], Step [4/8], train_loss: 648.2177\n",
            "Epoch [1643/2000], Step [6/8], train_loss: 176.0644\n",
            "Epoch [1643/2000], Step [8/8], train_loss: 285.9883\n",
            "Val Loss:  259.73916657765704\n",
            "------------------------------EPOCH: 1644 ------------------------\n",
            "Epoch [1644/2000], Step [2/8], train_loss: 353.8865\n",
            "Epoch [1644/2000], Step [4/8], train_loss: 626.2301\n",
            "Epoch [1644/2000], Step [6/8], train_loss: 175.7957\n",
            "Epoch [1644/2000], Step [8/8], train_loss: 285.5850\n",
            "Val Loss:  452.6172046661377\n",
            "------------------------------EPOCH: 1645 ------------------------\n",
            "Epoch [1645/2000], Step [2/8], train_loss: 347.9904\n",
            "Epoch [1645/2000], Step [4/8], train_loss: 645.0679\n",
            "Epoch [1645/2000], Step [6/8], train_loss: 176.2629\n",
            "Epoch [1645/2000], Step [8/8], train_loss: 285.8179\n",
            "Val Loss:  279.96813503901166\n",
            "------------------------------EPOCH: 1646 ------------------------\n",
            "Epoch [1646/2000], Step [2/8], train_loss: 351.9742\n",
            "Epoch [1646/2000], Step [4/8], train_loss: 633.7222\n",
            "Epoch [1646/2000], Step [6/8], train_loss: 177.9389\n",
            "Epoch [1646/2000], Step [8/8], train_loss: 285.5392\n",
            "Val Loss:  415.1001551946004\n",
            "------------------------------EPOCH: 1647 ------------------------\n",
            "Epoch [1647/2000], Step [2/8], train_loss: 348.2260\n",
            "Epoch [1647/2000], Step [4/8], train_loss: 649.4417\n",
            "Epoch [1647/2000], Step [6/8], train_loss: 176.0669\n",
            "Epoch [1647/2000], Step [8/8], train_loss: 285.9249\n",
            "Val Loss:  272.08705202738446\n",
            "------------------------------EPOCH: 1648 ------------------------\n",
            "Epoch [1648/2000], Step [2/8], train_loss: 353.3956\n",
            "Epoch [1648/2000], Step [4/8], train_loss: 624.3531\n",
            "Epoch [1648/2000], Step [6/8], train_loss: 176.0023\n",
            "Epoch [1648/2000], Step [8/8], train_loss: 285.6607\n",
            "Val Loss:  485.48737049102783\n",
            "------------------------------EPOCH: 1649 ------------------------\n",
            "Epoch [1649/2000], Step [2/8], train_loss: 348.4496\n",
            "Epoch [1649/2000], Step [4/8], train_loss: 641.3462\n",
            "Epoch [1649/2000], Step [6/8], train_loss: 175.6666\n",
            "Epoch [1649/2000], Step [8/8], train_loss: 285.9251\n",
            "Val Loss:  329.30710005760193\n",
            "------------------------------EPOCH: 1650 ------------------------\n",
            "Epoch [1650/2000], Step [2/8], train_loss: 352.1300\n",
            "Epoch [1650/2000], Step [4/8], train_loss: 635.0515\n",
            "Epoch [1650/2000], Step [6/8], train_loss: 176.8307\n",
            "Epoch [1650/2000], Step [8/8], train_loss: 285.6242\n",
            "Val Loss:  548.2981255849203\n",
            "------------------------------EPOCH: 1651 ------------------------\n",
            "Epoch [1651/2000], Step [2/8], train_loss: 348.6492\n",
            "Epoch [1651/2000], Step [4/8], train_loss: 648.1447\n",
            "Epoch [1651/2000], Step [6/8], train_loss: 174.8748\n",
            "Epoch [1651/2000], Step [8/8], train_loss: 285.9310\n",
            "Val Loss:  295.1440312067668\n",
            "------------------------------EPOCH: 1652 ------------------------\n",
            "Epoch [1652/2000], Step [2/8], train_loss: 353.4734\n",
            "Epoch [1652/2000], Step [4/8], train_loss: 626.1156\n",
            "Epoch [1652/2000], Step [6/8], train_loss: 174.7994\n",
            "Epoch [1652/2000], Step [8/8], train_loss: 285.5457\n",
            "Val Loss:  525.1897125244141\n",
            "------------------------------EPOCH: 1653 ------------------------\n",
            "Epoch [1653/2000], Step [2/8], train_loss: 347.4564\n",
            "Epoch [1653/2000], Step [4/8], train_loss: 644.4479\n",
            "Epoch [1653/2000], Step [6/8], train_loss: 175.6453\n",
            "Epoch [1653/2000], Step [8/8], train_loss: 285.7827\n",
            "Val Loss:  264.62644322713214\n",
            "------------------------------EPOCH: 1654 ------------------------\n",
            "Epoch [1654/2000], Step [2/8], train_loss: 350.9465\n",
            "Epoch [1654/2000], Step [4/8], train_loss: 634.2703\n",
            "Epoch [1654/2000], Step [6/8], train_loss: 177.7503\n",
            "Epoch [1654/2000], Step [8/8], train_loss: 285.4746\n",
            "Val Loss:  429.9643562634786\n",
            "------------------------------EPOCH: 1655 ------------------------\n",
            "Epoch [1655/2000], Step [2/8], train_loss: 346.8632\n",
            "Epoch [1655/2000], Step [4/8], train_loss: 647.3318\n",
            "Epoch [1655/2000], Step [6/8], train_loss: 175.9706\n",
            "Epoch [1655/2000], Step [8/8], train_loss: 285.8342\n",
            "Val Loss:  252.29638195037842\n",
            "------------------------------EPOCH: 1656 ------------------------\n",
            "Epoch [1656/2000], Step [2/8], train_loss: 351.9697\n",
            "Epoch [1656/2000], Step [4/8], train_loss: 624.8632\n",
            "Epoch [1656/2000], Step [6/8], train_loss: 175.8233\n",
            "Epoch [1656/2000], Step [8/8], train_loss: 285.5299\n",
            "Val Loss:  395.20256209373474\n",
            "------------------------------EPOCH: 1657 ------------------------\n",
            "Epoch [1657/2000], Step [2/8], train_loss: 347.6245\n",
            "Epoch [1657/2000], Step [4/8], train_loss: 641.3602\n",
            "Epoch [1657/2000], Step [6/8], train_loss: 175.1160\n",
            "Epoch [1657/2000], Step [8/8], train_loss: 285.8253\n",
            "Val Loss:  271.4772602717082\n",
            "------------------------------EPOCH: 1658 ------------------------\n",
            "Epoch [1658/2000], Step [2/8], train_loss: 352.2657\n",
            "Epoch [1658/2000], Step [4/8], train_loss: 632.5281\n",
            "Epoch [1658/2000], Step [6/8], train_loss: 176.0527\n",
            "Epoch [1658/2000], Step [8/8], train_loss: 285.4854\n",
            "Val Loss:  448.39568249384564\n",
            "------------------------------EPOCH: 1659 ------------------------\n",
            "Epoch [1659/2000], Step [2/8], train_loss: 347.6499\n",
            "Epoch [1659/2000], Step [4/8], train_loss: 647.8995\n",
            "Epoch [1659/2000], Step [6/8], train_loss: 174.3761\n",
            "Epoch [1659/2000], Step [8/8], train_loss: 285.7988\n",
            "Val Loss:  277.6019423007965\n",
            "------------------------------EPOCH: 1660 ------------------------\n",
            "Epoch [1660/2000], Step [2/8], train_loss: 352.2596\n",
            "Epoch [1660/2000], Step [4/8], train_loss: 627.4702\n",
            "Epoch [1660/2000], Step [6/8], train_loss: 174.6894\n",
            "Epoch [1660/2000], Step [8/8], train_loss: 285.4220\n",
            "Val Loss:  439.293564637502\n",
            "------------------------------EPOCH: 1661 ------------------------\n",
            "Epoch [1661/2000], Step [2/8], train_loss: 346.3230\n",
            "Epoch [1661/2000], Step [4/8], train_loss: 644.0582\n",
            "Epoch [1661/2000], Step [6/8], train_loss: 175.5095\n",
            "Epoch [1661/2000], Step [8/8], train_loss: 285.6761\n",
            "Val Loss:  282.5339856147766\n",
            "------------------------------EPOCH: 1662 ------------------------\n",
            "Epoch [1662/2000], Step [2/8], train_loss: 350.3461\n",
            "Epoch [1662/2000], Step [4/8], train_loss: 631.5493\n",
            "Epoch [1662/2000], Step [6/8], train_loss: 177.3887\n",
            "Epoch [1662/2000], Step [8/8], train_loss: 285.3641\n",
            "Val Loss:  424.2259078025818\n",
            "------------------------------EPOCH: 1663 ------------------------\n",
            "Epoch [1663/2000], Step [2/8], train_loss: 346.2863\n",
            "Epoch [1663/2000], Step [4/8], train_loss: 648.2205\n",
            "Epoch [1663/2000], Step [6/8], train_loss: 175.4967\n",
            "Epoch [1663/2000], Step [8/8], train_loss: 285.7104\n",
            "Val Loss:  234.92294851938883\n",
            "------------------------------EPOCH: 1664 ------------------------\n",
            "Epoch [1664/2000], Step [2/8], train_loss: 351.4006\n",
            "Epoch [1664/2000], Step [4/8], train_loss: 624.2732\n",
            "Epoch [1664/2000], Step [6/8], train_loss: 175.3708\n",
            "Epoch [1664/2000], Step [8/8], train_loss: 285.3852\n",
            "Val Loss:  223.45939481258392\n",
            "------------------------------EPOCH: 1665 ------------------------\n",
            "Epoch [1665/2000], Step [2/8], train_loss: 346.3723\n",
            "Epoch [1665/2000], Step [4/8], train_loss: 641.8802\n",
            "Epoch [1665/2000], Step [6/8], train_loss: 175.0557\n",
            "Epoch [1665/2000], Step [8/8], train_loss: 285.6263\n",
            "Val Loss:  134.97149946292242\n",
            "------------------------------EPOCH: 1666 ------------------------\n",
            "Epoch [1666/2000], Step [2/8], train_loss: 350.2247\n",
            "Epoch [1666/2000], Step [4/8], train_loss: 636.2394\n",
            "Epoch [1666/2000], Step [6/8], train_loss: 176.0730\n",
            "Epoch [1666/2000], Step [8/8], train_loss: 285.3436\n",
            "Val Loss:  249.48817773660025\n",
            "------------------------------EPOCH: 1667 ------------------------\n",
            "Epoch [1667/2000], Step [2/8], train_loss: 346.8019\n",
            "Epoch [1667/2000], Step [4/8], train_loss: 647.3580\n",
            "Epoch [1667/2000], Step [6/8], train_loss: 174.2248\n",
            "Epoch [1667/2000], Step [8/8], train_loss: 285.6669\n",
            "Val Loss:  161.68357074260712\n",
            "------------------------------EPOCH: 1668 ------------------------\n",
            "Epoch [1668/2000], Step [2/8], train_loss: 351.1966\n",
            "Epoch [1668/2000], Step [4/8], train_loss: 625.4816\n",
            "Epoch [1668/2000], Step [6/8], train_loss: 174.4706\n",
            "Epoch [1668/2000], Step [8/8], train_loss: 285.2831\n",
            "Val Loss:  263.77687827746075\n",
            "------------------------------EPOCH: 1669 ------------------------\n",
            "Epoch [1669/2000], Step [2/8], train_loss: 345.2830\n",
            "Epoch [1669/2000], Step [4/8], train_loss: 642.7396\n",
            "Epoch [1669/2000], Step [6/8], train_loss: 175.2659\n",
            "Epoch [1669/2000], Step [8/8], train_loss: 285.5213\n",
            "Val Loss:  169.5508991877238\n",
            "------------------------------EPOCH: 1670 ------------------------\n",
            "Epoch [1670/2000], Step [2/8], train_loss: 349.1429\n",
            "Epoch [1670/2000], Step [4/8], train_loss: 632.0437\n",
            "Epoch [1670/2000], Step [6/8], train_loss: 177.0672\n",
            "Epoch [1670/2000], Step [8/8], train_loss: 285.1957\n",
            "Val Loss:  378.45437479019165\n",
            "------------------------------EPOCH: 1671 ------------------------\n",
            "Epoch [1671/2000], Step [2/8], train_loss: 345.5827\n",
            "Epoch [1671/2000], Step [4/8], train_loss: 648.9294\n",
            "Epoch [1671/2000], Step [6/8], train_loss: 174.8876\n",
            "Epoch [1671/2000], Step [8/8], train_loss: 285.5246\n",
            "Val Loss:  219.56017462412515\n",
            "------------------------------EPOCH: 1672 ------------------------\n",
            "Epoch [1672/2000], Step [2/8], train_loss: 350.4076\n",
            "Epoch [1672/2000], Step [4/8], train_loss: 626.3658\n",
            "Epoch [1672/2000], Step [6/8], train_loss: 174.7311\n",
            "Epoch [1672/2000], Step [8/8], train_loss: 285.1762\n",
            "Val Loss:  348.448229153951\n",
            "------------------------------EPOCH: 1673 ------------------------\n",
            "Epoch [1673/2000], Step [2/8], train_loss: 344.8477\n",
            "Epoch [1673/2000], Step [4/8], train_loss: 643.0552\n",
            "Epoch [1673/2000], Step [6/8], train_loss: 175.3093\n",
            "Epoch [1673/2000], Step [8/8], train_loss: 285.4251\n",
            "Val Loss:  200.36495653788248\n",
            "------------------------------EPOCH: 1674 ------------------------\n",
            "Epoch [1674/2000], Step [2/8], train_loss: 348.6939\n",
            "Epoch [1674/2000], Step [4/8], train_loss: 631.0887\n",
            "Epoch [1674/2000], Step [6/8], train_loss: 177.1016\n",
            "Epoch [1674/2000], Step [8/8], train_loss: 285.1381\n",
            "Val Loss:  416.7851074536641\n",
            "------------------------------EPOCH: 1675 ------------------------\n",
            "Epoch [1675/2000], Step [2/8], train_loss: 344.9014\n",
            "Epoch [1675/2000], Step [4/8], train_loss: 647.1315\n",
            "Epoch [1675/2000], Step [6/8], train_loss: 175.3603\n",
            "Epoch [1675/2000], Step [8/8], train_loss: 285.4421\n",
            "Val Loss:  279.9873987833659\n",
            "------------------------------EPOCH: 1676 ------------------------\n",
            "Epoch [1676/2000], Step [2/8], train_loss: 349.1808\n",
            "Epoch [1676/2000], Step [4/8], train_loss: 625.2097\n",
            "Epoch [1676/2000], Step [6/8], train_loss: 175.3083\n",
            "Epoch [1676/2000], Step [8/8], train_loss: 285.1336\n",
            "Val Loss:  511.8066102663676\n",
            "------------------------------EPOCH: 1677 ------------------------\n",
            "Epoch [1677/2000], Step [2/8], train_loss: 344.7393\n",
            "Epoch [1677/2000], Step [4/8], train_loss: 641.1754\n",
            "Epoch [1677/2000], Step [6/8], train_loss: 174.5830\n",
            "Epoch [1677/2000], Step [8/8], train_loss: 285.4178\n",
            "Val Loss:  251.30123074849448\n",
            "------------------------------EPOCH: 1678 ------------------------\n",
            "Epoch [1678/2000], Step [2/8], train_loss: 348.9881\n",
            "Epoch [1678/2000], Step [4/8], train_loss: 635.1771\n",
            "Epoch [1678/2000], Step [6/8], train_loss: 175.5166\n",
            "Epoch [1678/2000], Step [8/8], train_loss: 285.1475\n",
            "Val Loss:  328.307719707489\n",
            "------------------------------EPOCH: 1679 ------------------------\n",
            "Epoch [1679/2000], Step [2/8], train_loss: 345.7035\n",
            "Epoch [1679/2000], Step [4/8], train_loss: 646.7506\n",
            "Epoch [1679/2000], Step [6/8], train_loss: 173.7096\n",
            "Epoch [1679/2000], Step [8/8], train_loss: 285.4803\n",
            "Val Loss:  202.37157893180847\n",
            "------------------------------EPOCH: 1680 ------------------------\n",
            "Epoch [1680/2000], Step [2/8], train_loss: 350.1174\n",
            "Epoch [1680/2000], Step [4/8], train_loss: 625.1753\n",
            "Epoch [1680/2000], Step [6/8], train_loss: 174.0664\n",
            "Epoch [1680/2000], Step [8/8], train_loss: 285.0536\n",
            "Val Loss:  442.0355378786723\n",
            "------------------------------EPOCH: 1681 ------------------------\n",
            "Epoch [1681/2000], Step [2/8], train_loss: 343.8673\n",
            "Epoch [1681/2000], Step [4/8], train_loss: 643.5745\n",
            "Epoch [1681/2000], Step [6/8], train_loss: 174.9267\n",
            "Epoch [1681/2000], Step [8/8], train_loss: 285.2476\n",
            "Val Loss:  315.7830282847087\n",
            "------------------------------EPOCH: 1682 ------------------------\n",
            "Epoch [1682/2000], Step [2/8], train_loss: 347.3073\n",
            "Epoch [1682/2000], Step [4/8], train_loss: 633.6181\n",
            "Epoch [1682/2000], Step [6/8], train_loss: 176.7878\n",
            "Epoch [1682/2000], Step [8/8], train_loss: 284.9455\n",
            "Val Loss:  541.2679986953735\n",
            "------------------------------EPOCH: 1683 ------------------------\n",
            "Epoch [1683/2000], Step [2/8], train_loss: 344.0861\n",
            "Epoch [1683/2000], Step [4/8], train_loss: 647.2560\n",
            "Epoch [1683/2000], Step [6/8], train_loss: 174.6505\n",
            "Epoch [1683/2000], Step [8/8], train_loss: 285.3326\n",
            "Val Loss:  342.38592036565143\n",
            "------------------------------EPOCH: 1684 ------------------------\n",
            "Epoch [1684/2000], Step [2/8], train_loss: 349.4562\n",
            "Epoch [1684/2000], Step [4/8], train_loss: 623.6270\n",
            "Epoch [1684/2000], Step [6/8], train_loss: 174.6936\n",
            "Epoch [1684/2000], Step [8/8], train_loss: 285.0113\n",
            "Val Loss:  381.1844235261281\n",
            "------------------------------EPOCH: 1685 ------------------------\n",
            "Epoch [1685/2000], Step [2/8], train_loss: 344.3614\n",
            "Epoch [1685/2000], Step [4/8], train_loss: 641.0811\n",
            "Epoch [1685/2000], Step [6/8], train_loss: 174.3931\n",
            "Epoch [1685/2000], Step [8/8], train_loss: 285.2306\n",
            "Val Loss:  172.87940176328024\n",
            "------------------------------EPOCH: 1686 ------------------------\n",
            "Epoch [1686/2000], Step [2/8], train_loss: 347.9788\n",
            "Epoch [1686/2000], Step [4/8], train_loss: 632.8512\n",
            "Epoch [1686/2000], Step [6/8], train_loss: 175.3116\n",
            "Epoch [1686/2000], Step [8/8], train_loss: 284.9214\n",
            "Val Loss:  280.37002936999005\n",
            "------------------------------EPOCH: 1687 ------------------------\n",
            "Epoch [1687/2000], Step [2/8], train_loss: 344.5263\n",
            "Epoch [1687/2000], Step [4/8], train_loss: 646.2389\n",
            "Epoch [1687/2000], Step [6/8], train_loss: 173.3725\n",
            "Epoch [1687/2000], Step [8/8], train_loss: 285.2557\n",
            "Val Loss:  152.498877286911\n",
            "------------------------------EPOCH: 1688 ------------------------\n",
            "Epoch [1688/2000], Step [2/8], train_loss: 348.8086\n",
            "Epoch [1688/2000], Step [4/8], train_loss: 626.5438\n",
            "Epoch [1688/2000], Step [6/8], train_loss: 173.8045\n",
            "Epoch [1688/2000], Step [8/8], train_loss: 284.8851\n",
            "Val Loss:  328.0926590760549\n",
            "------------------------------EPOCH: 1689 ------------------------\n",
            "Epoch [1689/2000], Step [2/8], train_loss: 343.2473\n",
            "Epoch [1689/2000], Step [4/8], train_loss: 642.0081\n",
            "Epoch [1689/2000], Step [6/8], train_loss: 174.6087\n",
            "Epoch [1689/2000], Step [8/8], train_loss: 285.1248\n",
            "Val Loss:  229.5553552309672\n",
            "------------------------------EPOCH: 1690 ------------------------\n",
            "Epoch [1690/2000], Step [2/8], train_loss: 347.1451\n",
            "Epoch [1690/2000], Step [4/8], train_loss: 630.4586\n",
            "Epoch [1690/2000], Step [6/8], train_loss: 176.4807\n",
            "Epoch [1690/2000], Step [8/8], train_loss: 284.7816\n",
            "Val Loss:  392.6556770801544\n",
            "------------------------------EPOCH: 1691 ------------------------\n",
            "Epoch [1691/2000], Step [2/8], train_loss: 343.2074\n",
            "Epoch [1691/2000], Step [4/8], train_loss: 648.5756\n",
            "Epoch [1691/2000], Step [6/8], train_loss: 174.4486\n",
            "Epoch [1691/2000], Step [8/8], train_loss: 285.0932\n",
            "Val Loss:  254.709867477417\n",
            "------------------------------EPOCH: 1692 ------------------------\n",
            "Epoch [1692/2000], Step [2/8], train_loss: 347.8979\n",
            "Epoch [1692/2000], Step [4/8], train_loss: 623.4504\n",
            "Epoch [1692/2000], Step [6/8], train_loss: 174.1878\n",
            "Epoch [1692/2000], Step [8/8], train_loss: 284.8289\n",
            "Val Loss:  328.31519651412964\n",
            "------------------------------EPOCH: 1693 ------------------------\n",
            "Epoch [1693/2000], Step [2/8], train_loss: 343.4977\n",
            "Epoch [1693/2000], Step [4/8], train_loss: 639.9114\n",
            "Epoch [1693/2000], Step [6/8], train_loss: 173.9776\n",
            "Epoch [1693/2000], Step [8/8], train_loss: 285.0847\n",
            "Val Loss:  224.3639849026998\n",
            "------------------------------EPOCH: 1694 ------------------------\n",
            "Epoch [1694/2000], Step [2/8], train_loss: 347.0971\n",
            "Epoch [1694/2000], Step [4/8], train_loss: 629.5649\n",
            "Epoch [1694/2000], Step [6/8], train_loss: 175.0864\n",
            "Epoch [1694/2000], Step [8/8], train_loss: 284.7972\n",
            "Val Loss:  421.7202804883321\n",
            "------------------------------EPOCH: 1695 ------------------------\n",
            "Epoch [1695/2000], Step [2/8], train_loss: 343.9005\n",
            "Epoch [1695/2000], Step [4/8], train_loss: 647.4190\n",
            "Epoch [1695/2000], Step [6/8], train_loss: 173.0715\n",
            "Epoch [1695/2000], Step [8/8], train_loss: 285.1065\n",
            "Val Loss:  209.75011189778647\n",
            "------------------------------EPOCH: 1696 ------------------------\n",
            "Epoch [1696/2000], Step [2/8], train_loss: 348.3871\n",
            "Epoch [1696/2000], Step [4/8], train_loss: 623.8524\n",
            "Epoch [1696/2000], Step [6/8], train_loss: 173.0003\n",
            "Epoch [1696/2000], Step [8/8], train_loss: 284.7845\n",
            "Val Loss:  257.9412947495778\n",
            "------------------------------EPOCH: 1697 ------------------------\n",
            "Epoch [1697/2000], Step [2/8], train_loss: 342.9337\n",
            "Epoch [1697/2000], Step [4/8], train_loss: 641.8073\n",
            "Epoch [1697/2000], Step [6/8], train_loss: 173.9743\n",
            "Epoch [1697/2000], Step [8/8], train_loss: 284.9924\n",
            "Val Loss:  190.07691550254822\n",
            "------------------------------EPOCH: 1698 ------------------------\n",
            "Epoch [1698/2000], Step [2/8], train_loss: 345.9833\n",
            "Epoch [1698/2000], Step [4/8], train_loss: 627.1350\n",
            "Epoch [1698/2000], Step [6/8], train_loss: 175.8405\n",
            "Epoch [1698/2000], Step [8/8], train_loss: 284.7362\n",
            "Val Loss:  300.2275643348694\n",
            "------------------------------EPOCH: 1699 ------------------------\n",
            "Epoch [1699/2000], Step [2/8], train_loss: 342.3503\n",
            "Epoch [1699/2000], Step [4/8], train_loss: 640.5785\n",
            "Epoch [1699/2000], Step [6/8], train_loss: 175.1462\n",
            "Epoch [1699/2000], Step [8/8], train_loss: 284.8982\n",
            "Val Loss:  237.03576211134592\n",
            "------------------------------EPOCH: 1700 ------------------------\n",
            "Epoch [1700/2000], Step [2/8], train_loss: 344.8535\n",
            "Epoch [1700/2000], Step [4/8], train_loss: 629.2917\n",
            "Epoch [1700/2000], Step [6/8], train_loss: 174.3619\n",
            "Epoch [1700/2000], Step [8/8], train_loss: 284.7676\n",
            "Val Loss:  385.0427843729655\n",
            "------------------------------EPOCH: 1701 ------------------------\n",
            "Epoch [1701/2000], Step [2/8], train_loss: 343.5339\n",
            "Epoch [1701/2000], Step [4/8], train_loss: 635.5603\n",
            "Epoch [1701/2000], Step [6/8], train_loss: 173.4392\n",
            "Epoch [1701/2000], Step [8/8], train_loss: 284.9844\n",
            "Val Loss:  302.89894278844196\n",
            "------------------------------EPOCH: 1702 ------------------------\n",
            "Epoch [1702/2000], Step [2/8], train_loss: 346.5673\n",
            "Epoch [1702/2000], Step [4/8], train_loss: 625.7808\n",
            "Epoch [1702/2000], Step [6/8], train_loss: 173.5188\n",
            "Epoch [1702/2000], Step [8/8], train_loss: 284.9701\n",
            "Val Loss:  380.96206188201904\n",
            "------------------------------EPOCH: 1703 ------------------------\n",
            "Epoch [1703/2000], Step [2/8], train_loss: 344.9305\n",
            "Epoch [1703/2000], Step [4/8], train_loss: 635.1832\n",
            "Epoch [1703/2000], Step [6/8], train_loss: 173.5410\n",
            "Epoch [1703/2000], Step [8/8], train_loss: 285.0829\n",
            "Val Loss:  303.91816838582355\n",
            "------------------------------EPOCH: 1704 ------------------------\n",
            "Epoch [1704/2000], Step [2/8], train_loss: 345.6170\n",
            "Epoch [1704/2000], Step [4/8], train_loss: 630.3348\n",
            "Epoch [1704/2000], Step [6/8], train_loss: 174.2622\n",
            "Epoch [1704/2000], Step [8/8], train_loss: 284.9327\n",
            "Val Loss:  405.25715525945026\n",
            "------------------------------EPOCH: 1705 ------------------------\n",
            "Epoch [1705/2000], Step [2/8], train_loss: 343.2823\n",
            "Epoch [1705/2000], Step [4/8], train_loss: 635.8258\n",
            "Epoch [1705/2000], Step [6/8], train_loss: 173.9417\n",
            "Epoch [1705/2000], Step [8/8], train_loss: 285.0651\n",
            "Val Loss:  304.90849272410077\n",
            "------------------------------EPOCH: 1706 ------------------------\n",
            "Epoch [1706/2000], Step [2/8], train_loss: 345.1456\n",
            "Epoch [1706/2000], Step [4/8], train_loss: 630.7781\n",
            "Epoch [1706/2000], Step [6/8], train_loss: 174.0160\n",
            "Epoch [1706/2000], Step [8/8], train_loss: 285.0045\n",
            "Val Loss:  334.4696687062581\n",
            "------------------------------EPOCH: 1707 ------------------------\n",
            "Epoch [1707/2000], Step [2/8], train_loss: 344.0409\n",
            "Epoch [1707/2000], Step [4/8], train_loss: 632.4155\n",
            "Epoch [1707/2000], Step [6/8], train_loss: 173.4355\n",
            "Epoch [1707/2000], Step [8/8], train_loss: 285.1735\n",
            "Val Loss:  282.88018767038983\n",
            "------------------------------EPOCH: 1708 ------------------------\n",
            "Epoch [1708/2000], Step [2/8], train_loss: 345.7405\n",
            "Epoch [1708/2000], Step [4/8], train_loss: 628.0996\n",
            "Epoch [1708/2000], Step [6/8], train_loss: 173.3056\n",
            "Epoch [1708/2000], Step [8/8], train_loss: 285.1196\n",
            "Val Loss:  374.77698802948\n",
            "------------------------------EPOCH: 1709 ------------------------\n",
            "Epoch [1709/2000], Step [2/8], train_loss: 344.9774\n",
            "Epoch [1709/2000], Step [4/8], train_loss: 633.7587\n",
            "Epoch [1709/2000], Step [6/8], train_loss: 173.0058\n",
            "Epoch [1709/2000], Step [8/8], train_loss: 285.2351\n",
            "Val Loss:  324.69973882039386\n",
            "------------------------------EPOCH: 1710 ------------------------\n",
            "Epoch [1710/2000], Step [2/8], train_loss: 345.6008\n",
            "Epoch [1710/2000], Step [4/8], train_loss: 631.3993\n",
            "Epoch [1710/2000], Step [6/8], train_loss: 174.1664\n",
            "Epoch [1710/2000], Step [8/8], train_loss: 285.0513\n",
            "Val Loss:  480.98236242930096\n",
            "------------------------------EPOCH: 1711 ------------------------\n",
            "Epoch [1711/2000], Step [2/8], train_loss: 343.1809\n",
            "Epoch [1711/2000], Step [4/8], train_loss: 635.9943\n",
            "Epoch [1711/2000], Step [6/8], train_loss: 173.3173\n",
            "Epoch [1711/2000], Step [8/8], train_loss: 285.2570\n",
            "Val Loss:  333.24643484751385\n",
            "------------------------------EPOCH: 1712 ------------------------\n",
            "Epoch [1712/2000], Step [2/8], train_loss: 345.6965\n",
            "Epoch [1712/2000], Step [4/8], train_loss: 630.9933\n",
            "Epoch [1712/2000], Step [6/8], train_loss: 173.7592\n",
            "Epoch [1712/2000], Step [8/8], train_loss: 285.0801\n",
            "Val Loss:  309.6284957726796\n",
            "------------------------------EPOCH: 1713 ------------------------\n",
            "Epoch [1713/2000], Step [2/8], train_loss: 343.5732\n",
            "Epoch [1713/2000], Step [4/8], train_loss: 635.6254\n",
            "Epoch [1713/2000], Step [6/8], train_loss: 172.9508\n",
            "Epoch [1713/2000], Step [8/8], train_loss: 285.2710\n",
            "Val Loss:  239.60570812225342\n",
            "------------------------------EPOCH: 1714 ------------------------\n",
            "Epoch [1714/2000], Step [2/8], train_loss: 345.5606\n",
            "Epoch [1714/2000], Step [4/8], train_loss: 627.3083\n",
            "Epoch [1714/2000], Step [6/8], train_loss: 173.2858\n",
            "Epoch [1714/2000], Step [8/8], train_loss: 285.1722\n",
            "Val Loss:  351.28472900390625\n",
            "------------------------------EPOCH: 1715 ------------------------\n",
            "Epoch [1715/2000], Step [2/8], train_loss: 343.5907\n",
            "Epoch [1715/2000], Step [4/8], train_loss: 634.7147\n",
            "Epoch [1715/2000], Step [6/8], train_loss: 172.8594\n",
            "Epoch [1715/2000], Step [8/8], train_loss: 285.4212\n",
            "Val Loss:  254.591148853302\n",
            "------------------------------EPOCH: 1716 ------------------------\n",
            "Epoch [1716/2000], Step [2/8], train_loss: 345.9137\n",
            "Epoch [1716/2000], Step [4/8], train_loss: 630.0214\n",
            "Epoch [1716/2000], Step [6/8], train_loss: 174.3109\n",
            "Epoch [1716/2000], Step [8/8], train_loss: 285.1842\n",
            "Val Loss:  527.6339928309122\n",
            "------------------------------EPOCH: 1717 ------------------------\n",
            "Epoch [1717/2000], Step [2/8], train_loss: 342.7975\n",
            "Epoch [1717/2000], Step [4/8], train_loss: 637.1282\n",
            "Epoch [1717/2000], Step [6/8], train_loss: 173.3522\n",
            "Epoch [1717/2000], Step [8/8], train_loss: 285.3455\n",
            "Val Loss:  322.7863934834798\n",
            "------------------------------EPOCH: 1718 ------------------------\n",
            "Epoch [1718/2000], Step [2/8], train_loss: 344.5856\n",
            "Epoch [1718/2000], Step [4/8], train_loss: 632.4274\n",
            "Epoch [1718/2000], Step [6/8], train_loss: 173.9030\n",
            "Epoch [1718/2000], Step [8/8], train_loss: 285.0772\n",
            "Val Loss:  398.9362891515096\n",
            "------------------------------EPOCH: 1719 ------------------------\n",
            "Epoch [1719/2000], Step [2/8], train_loss: 342.2390\n",
            "Epoch [1719/2000], Step [4/8], train_loss: 635.1888\n",
            "Epoch [1719/2000], Step [6/8], train_loss: 172.8985\n",
            "Epoch [1719/2000], Step [8/8], train_loss: 285.3487\n",
            "Val Loss:  284.6649395624797\n",
            "------------------------------EPOCH: 1720 ------------------------\n",
            "Epoch [1720/2000], Step [2/8], train_loss: 345.8541\n",
            "Epoch [1720/2000], Step [4/8], train_loss: 626.0555\n",
            "Epoch [1720/2000], Step [6/8], train_loss: 173.5078\n",
            "Epoch [1720/2000], Step [8/8], train_loss: 285.2212\n",
            "Val Loss:  260.224093914032\n",
            "------------------------------EPOCH: 1721 ------------------------\n",
            "Epoch [1721/2000], Step [2/8], train_loss: 342.8181\n",
            "Epoch [1721/2000], Step [4/8], train_loss: 636.6168\n",
            "Epoch [1721/2000], Step [6/8], train_loss: 174.0596\n",
            "Epoch [1721/2000], Step [8/8], train_loss: 285.2244\n",
            "Val Loss:  297.99337498346966\n",
            "------------------------------EPOCH: 1722 ------------------------\n",
            "Epoch [1722/2000], Step [2/8], train_loss: 343.3487\n",
            "Epoch [1722/2000], Step [4/8], train_loss: 629.7088\n",
            "Epoch [1722/2000], Step [6/8], train_loss: 172.7307\n",
            "Epoch [1722/2000], Step [8/8], train_loss: 285.3887\n",
            "Val Loss:  266.31284777323407\n",
            "------------------------------EPOCH: 1723 ------------------------\n",
            "Epoch [1723/2000], Step [2/8], train_loss: 344.7057\n",
            "Epoch [1723/2000], Step [4/8], train_loss: 634.1561\n",
            "Epoch [1723/2000], Step [6/8], train_loss: 172.7709\n",
            "Epoch [1723/2000], Step [8/8], train_loss: 285.3693\n",
            "Val Loss:  249.15621527036032\n",
            "------------------------------EPOCH: 1724 ------------------------\n",
            "Epoch [1724/2000], Step [2/8], train_loss: 343.5443\n",
            "Epoch [1724/2000], Step [4/8], train_loss: 630.4341\n",
            "Epoch [1724/2000], Step [6/8], train_loss: 173.2561\n",
            "Epoch [1724/2000], Step [8/8], train_loss: 285.4263\n",
            "Val Loss:  235.62506262461343\n",
            "------------------------------EPOCH: 1725 ------------------------\n",
            "Epoch [1725/2000], Step [2/8], train_loss: 343.4735\n",
            "Epoch [1725/2000], Step [4/8], train_loss: 631.2368\n",
            "Epoch [1725/2000], Step [6/8], train_loss: 173.6582\n",
            "Epoch [1725/2000], Step [8/8], train_loss: 285.3678\n",
            "Val Loss:  235.72032471497855\n",
            "------------------------------EPOCH: 1726 ------------------------\n",
            "Epoch [1726/2000], Step [2/8], train_loss: 342.6809\n",
            "Epoch [1726/2000], Step [4/8], train_loss: 633.1267\n",
            "Epoch [1726/2000], Step [6/8], train_loss: 173.2495\n",
            "Epoch [1726/2000], Step [8/8], train_loss: 285.4174\n",
            "Val Loss:  218.4857398668925\n",
            "------------------------------EPOCH: 1727 ------------------------\n",
            "Epoch [1727/2000], Step [2/8], train_loss: 343.8001\n",
            "Epoch [1727/2000], Step [4/8], train_loss: 631.3964\n",
            "Epoch [1727/2000], Step [6/8], train_loss: 172.8258\n",
            "Epoch [1727/2000], Step [8/8], train_loss: 285.4543\n",
            "Val Loss:  244.25940338770548\n",
            "------------------------------EPOCH: 1728 ------------------------\n",
            "Epoch [1728/2000], Step [2/8], train_loss: 343.4749\n",
            "Epoch [1728/2000], Step [4/8], train_loss: 631.3599\n",
            "Epoch [1728/2000], Step [6/8], train_loss: 172.5965\n",
            "Epoch [1728/2000], Step [8/8], train_loss: 285.5427\n",
            "Val Loss:  224.72475814819336\n",
            "------------------------------EPOCH: 1729 ------------------------\n",
            "Epoch [1729/2000], Step [2/8], train_loss: 343.3036\n",
            "Epoch [1729/2000], Step [4/8], train_loss: 632.8491\n",
            "Epoch [1729/2000], Step [6/8], train_loss: 174.0341\n",
            "Epoch [1729/2000], Step [8/8], train_loss: 285.4389\n",
            "Val Loss:  361.23667589823407\n",
            "------------------------------EPOCH: 1730 ------------------------\n",
            "Epoch [1730/2000], Step [2/8], train_loss: 342.5687\n",
            "Epoch [1730/2000], Step [4/8], train_loss: 635.1989\n",
            "Epoch [1730/2000], Step [6/8], train_loss: 172.4459\n",
            "Epoch [1730/2000], Step [8/8], train_loss: 285.6942\n",
            "Val Loss:  267.91628789901733\n",
            "------------------------------EPOCH: 1731 ------------------------\n",
            "Epoch [1731/2000], Step [2/8], train_loss: 344.7054\n",
            "Epoch [1731/2000], Step [4/8], train_loss: 628.9237\n",
            "Epoch [1731/2000], Step [6/8], train_loss: 174.1877\n",
            "Epoch [1731/2000], Step [8/8], train_loss: 285.2842\n",
            "Val Loss:  489.0850890477498\n",
            "------------------------------EPOCH: 1732 ------------------------\n",
            "Epoch [1732/2000], Step [2/8], train_loss: 340.4896\n",
            "Epoch [1732/2000], Step [4/8], train_loss: 634.8724\n",
            "Epoch [1732/2000], Step [6/8], train_loss: 172.6585\n",
            "Epoch [1732/2000], Step [8/8], train_loss: 285.6184\n",
            "Val Loss:  231.2782440185547\n",
            "------------------------------EPOCH: 1733 ------------------------\n",
            "Epoch [1733/2000], Step [2/8], train_loss: 344.7930\n",
            "Epoch [1733/2000], Step [4/8], train_loss: 626.6031\n",
            "Epoch [1733/2000], Step [6/8], train_loss: 173.9775\n",
            "Epoch [1733/2000], Step [8/8], train_loss: 285.3046\n",
            "Val Loss:  308.2811468442281\n",
            "------------------------------EPOCH: 1734 ------------------------\n",
            "Epoch [1734/2000], Step [2/8], train_loss: 340.7941\n",
            "Epoch [1734/2000], Step [4/8], train_loss: 636.6942\n",
            "Epoch [1734/2000], Step [6/8], train_loss: 172.9410\n",
            "Epoch [1734/2000], Step [8/8], train_loss: 285.5781\n",
            "Val Loss:  203.3490695953369\n",
            "------------------------------EPOCH: 1735 ------------------------\n",
            "Epoch [1735/2000], Step [2/8], train_loss: 343.9115\n",
            "Epoch [1735/2000], Step [4/8], train_loss: 629.2107\n",
            "Epoch [1735/2000], Step [6/8], train_loss: 174.7403\n",
            "Epoch [1735/2000], Step [8/8], train_loss: 285.1840\n",
            "Val Loss:  258.19434213638306\n",
            "------------------------------EPOCH: 1736 ------------------------\n",
            "Epoch [1736/2000], Step [2/8], train_loss: 340.0679\n",
            "Epoch [1736/2000], Step [4/8], train_loss: 639.0979\n",
            "Epoch [1736/2000], Step [6/8], train_loss: 172.9774\n",
            "Epoch [1736/2000], Step [8/8], train_loss: 285.5222\n",
            "Val Loss:  153.71593709786734\n",
            "------------------------------EPOCH: 1737 ------------------------\n",
            "Epoch [1737/2000], Step [2/8], train_loss: 344.0540\n",
            "Epoch [1737/2000], Step [4/8], train_loss: 625.9946\n",
            "Epoch [1737/2000], Step [6/8], train_loss: 173.7359\n",
            "Epoch [1737/2000], Step [8/8], train_loss: 285.1735\n",
            "Val Loss:  436.2345740000407\n",
            "------------------------------EPOCH: 1738 ------------------------\n",
            "Epoch [1738/2000], Step [2/8], train_loss: 339.5569\n",
            "Epoch [1738/2000], Step [4/8], train_loss: 637.4587\n",
            "Epoch [1738/2000], Step [6/8], train_loss: 173.1363\n",
            "Epoch [1738/2000], Step [8/8], train_loss: 285.4651\n",
            "Val Loss:  297.8265078862508\n",
            "------------------------------EPOCH: 1739 ------------------------\n",
            "Epoch [1739/2000], Step [2/8], train_loss: 343.2656\n",
            "Epoch [1739/2000], Step [4/8], train_loss: 632.9136\n",
            "Epoch [1739/2000], Step [6/8], train_loss: 175.2957\n",
            "Epoch [1739/2000], Step [8/8], train_loss: 285.0587\n",
            "Val Loss:  574.5136426289877\n",
            "------------------------------EPOCH: 1740 ------------------------\n",
            "Epoch [1740/2000], Step [2/8], train_loss: 339.4868\n",
            "Epoch [1740/2000], Step [4/8], train_loss: 646.5238\n",
            "Epoch [1740/2000], Step [6/8], train_loss: 172.3021\n",
            "Epoch [1740/2000], Step [8/8], train_loss: 285.4854\n",
            "Val Loss:  309.5314989089966\n",
            "------------------------------EPOCH: 1741 ------------------------\n",
            "Epoch [1741/2000], Step [2/8], train_loss: 344.9404\n",
            "Epoch [1741/2000], Step [4/8], train_loss: 622.5264\n",
            "Epoch [1741/2000], Step [6/8], train_loss: 173.0871\n",
            "Epoch [1741/2000], Step [8/8], train_loss: 284.9215\n",
            "Val Loss:  792.7374159495035\n",
            "------------------------------EPOCH: 1742 ------------------------\n",
            "Epoch [1742/2000], Step [2/8], train_loss: 337.8102\n",
            "Epoch [1742/2000], Step [4/8], train_loss: 645.8528\n",
            "Epoch [1742/2000], Step [6/8], train_loss: 173.6998\n",
            "Epoch [1742/2000], Step [8/8], train_loss: 285.1581\n",
            "Val Loss:  443.8187430699666\n",
            "------------------------------EPOCH: 1743 ------------------------\n",
            "Epoch [1743/2000], Step [2/8], train_loss: 341.8881\n",
            "Epoch [1743/2000], Step [4/8], train_loss: 625.6199\n",
            "Epoch [1743/2000], Step [6/8], train_loss: 175.6969\n",
            "Epoch [1743/2000], Step [8/8], train_loss: 284.8133\n",
            "Val Loss:  719.8651707967123\n",
            "------------------------------EPOCH: 1744 ------------------------\n",
            "Epoch [1744/2000], Step [2/8], train_loss: 337.8123\n",
            "Epoch [1744/2000], Step [4/8], train_loss: 640.3265\n",
            "Epoch [1744/2000], Step [6/8], train_loss: 174.5003\n",
            "Epoch [1744/2000], Step [8/8], train_loss: 285.0814\n",
            "Val Loss:  381.16116730372113\n",
            "------------------------------EPOCH: 1745 ------------------------\n",
            "Epoch [1745/2000], Step [2/8], train_loss: 341.2457\n",
            "Epoch [1745/2000], Step [4/8], train_loss: 626.7110\n",
            "Epoch [1745/2000], Step [6/8], train_loss: 174.1530\n",
            "Epoch [1745/2000], Step [8/8], train_loss: 284.8889\n",
            "Val Loss:  486.43964227040607\n",
            "------------------------------EPOCH: 1746 ------------------------\n",
            "Epoch [1746/2000], Step [2/8], train_loss: 338.6704\n",
            "Epoch [1746/2000], Step [4/8], train_loss: 638.4937\n",
            "Epoch [1746/2000], Step [6/8], train_loss: 173.1864\n",
            "Epoch [1746/2000], Step [8/8], train_loss: 285.1795\n",
            "Val Loss:  279.49222230911255\n",
            "------------------------------EPOCH: 1747 ------------------------\n",
            "Epoch [1747/2000], Step [2/8], train_loss: 342.2927\n",
            "Epoch [1747/2000], Step [4/8], train_loss: 627.7354\n",
            "Epoch [1747/2000], Step [6/8], train_loss: 174.1756\n",
            "Epoch [1747/2000], Step [8/8], train_loss: 284.9355\n",
            "Val Loss:  313.4540824890137\n",
            "------------------------------EPOCH: 1748 ------------------------\n",
            "Epoch [1748/2000], Step [2/8], train_loss: 338.8145\n",
            "Epoch [1748/2000], Step [4/8], train_loss: 638.5299\n",
            "Epoch [1748/2000], Step [6/8], train_loss: 173.1667\n",
            "Epoch [1748/2000], Step [8/8], train_loss: 285.1577\n",
            "Val Loss:  193.37099679311117\n",
            "------------------------------EPOCH: 1749 ------------------------\n",
            "Epoch [1749/2000], Step [2/8], train_loss: 341.9060\n",
            "Epoch [1749/2000], Step [4/8], train_loss: 628.3329\n",
            "Epoch [1749/2000], Step [6/8], train_loss: 173.3498\n",
            "Epoch [1749/2000], Step [8/8], train_loss: 285.0210\n",
            "Val Loss:  201.9279047648112\n",
            "------------------------------EPOCH: 1750 ------------------------\n",
            "Epoch [1750/2000], Step [2/8], train_loss: 339.4090\n",
            "Epoch [1750/2000], Step [4/8], train_loss: 634.8529\n",
            "Epoch [1750/2000], Step [6/8], train_loss: 173.0743\n",
            "Epoch [1750/2000], Step [8/8], train_loss: 285.1938\n",
            "Val Loss:  148.5374323527018\n",
            "------------------------------EPOCH: 1751 ------------------------\n",
            "Epoch [1751/2000], Step [2/8], train_loss: 341.3068\n",
            "Epoch [1751/2000], Step [4/8], train_loss: 632.0707\n",
            "Epoch [1751/2000], Step [6/8], train_loss: 174.6533\n",
            "Epoch [1751/2000], Step [8/8], train_loss: 284.8999\n",
            "Val Loss:  281.2100729942322\n",
            "------------------------------EPOCH: 1752 ------------------------\n",
            "Epoch [1752/2000], Step [2/8], train_loss: 338.9673\n",
            "Epoch [1752/2000], Step [4/8], train_loss: 642.1732\n",
            "Epoch [1752/2000], Step [6/8], train_loss: 171.9413\n",
            "Epoch [1752/2000], Step [8/8], train_loss: 285.2855\n",
            "Val Loss:  183.43012269337973\n",
            "------------------------------EPOCH: 1753 ------------------------\n",
            "Epoch [1753/2000], Step [2/8], train_loss: 343.7112\n",
            "Epoch [1753/2000], Step [4/8], train_loss: 622.0670\n",
            "Epoch [1753/2000], Step [6/8], train_loss: 172.9823\n",
            "Epoch [1753/2000], Step [8/8], train_loss: 284.9089\n",
            "Val Loss:  328.8589551448822\n",
            "------------------------------EPOCH: 1754 ------------------------\n",
            "Epoch [1754/2000], Step [2/8], train_loss: 338.7103\n",
            "Epoch [1754/2000], Step [4/8], train_loss: 639.0544\n",
            "Epoch [1754/2000], Step [6/8], train_loss: 172.3058\n",
            "Epoch [1754/2000], Step [8/8], train_loss: 285.1808\n",
            "Val Loss:  185.86742623647055\n",
            "------------------------------EPOCH: 1755 ------------------------\n",
            "Epoch [1755/2000], Step [2/8], train_loss: 342.3405\n",
            "Epoch [1755/2000], Step [4/8], train_loss: 629.0694\n",
            "Epoch [1755/2000], Step [6/8], train_loss: 174.3801\n",
            "Epoch [1755/2000], Step [8/8], train_loss: 284.7440\n",
            "Val Loss:  587.151959101359\n",
            "------------------------------EPOCH: 1756 ------------------------\n",
            "Epoch [1756/2000], Step [2/8], train_loss: 338.0933\n",
            "Epoch [1756/2000], Step [4/8], train_loss: 646.8052\n",
            "Epoch [1756/2000], Step [6/8], train_loss: 171.7772\n",
            "Epoch [1756/2000], Step [8/8], train_loss: 285.1694\n",
            "Val Loss:  365.8106269836426\n",
            "------------------------------EPOCH: 1757 ------------------------\n",
            "Epoch [1757/2000], Step [2/8], train_loss: 343.3561\n",
            "Epoch [1757/2000], Step [4/8], train_loss: 621.0143\n",
            "Epoch [1757/2000], Step [6/8], train_loss: 173.0152\n",
            "Epoch [1757/2000], Step [8/8], train_loss: 284.6923\n",
            "Val Loss:  587.5316886901855\n",
            "------------------------------EPOCH: 1758 ------------------------\n",
            "Epoch [1758/2000], Step [2/8], train_loss: 337.2037\n",
            "Epoch [1758/2000], Step [4/8], train_loss: 643.5045\n",
            "Epoch [1758/2000], Step [6/8], train_loss: 172.6515\n",
            "Epoch [1758/2000], Step [8/8], train_loss: 284.9595\n",
            "Val Loss:  295.3844579060872\n",
            "------------------------------EPOCH: 1759 ------------------------\n",
            "Epoch [1759/2000], Step [2/8], train_loss: 341.4542\n",
            "Epoch [1759/2000], Step [4/8], train_loss: 626.3561\n",
            "Epoch [1759/2000], Step [6/8], train_loss: 173.9572\n",
            "Epoch [1759/2000], Step [8/8], train_loss: 284.6170\n",
            "Val Loss:  546.3987967173258\n",
            "------------------------------EPOCH: 1760 ------------------------\n",
            "Epoch [1760/2000], Step [2/8], train_loss: 337.0957\n",
            "Epoch [1760/2000], Step [4/8], train_loss: 638.8483\n",
            "Epoch [1760/2000], Step [6/8], train_loss: 173.0957\n",
            "Epoch [1760/2000], Step [8/8], train_loss: 284.8901\n",
            "Val Loss:  389.11030944188434\n",
            "------------------------------EPOCH: 1761 ------------------------\n",
            "Epoch [1761/2000], Step [2/8], train_loss: 341.2638\n",
            "Epoch [1761/2000], Step [4/8], train_loss: 625.6592\n",
            "Epoch [1761/2000], Step [6/8], train_loss: 173.0200\n",
            "Epoch [1761/2000], Step [8/8], train_loss: 284.6880\n",
            "Val Loss:  418.1338068644206\n",
            "------------------------------EPOCH: 1762 ------------------------\n",
            "Epoch [1762/2000], Step [2/8], train_loss: 338.4669\n",
            "Epoch [1762/2000], Step [4/8], train_loss: 638.2383\n",
            "Epoch [1762/2000], Step [6/8], train_loss: 172.4437\n",
            "Epoch [1762/2000], Step [8/8], train_loss: 284.8751\n",
            "Val Loss:  258.52770646413165\n",
            "------------------------------EPOCH: 1763 ------------------------\n",
            "Epoch [1763/2000], Step [2/8], train_loss: 340.4818\n",
            "Epoch [1763/2000], Step [4/8], train_loss: 626.6709\n",
            "Epoch [1763/2000], Step [6/8], train_loss: 173.3554\n",
            "Epoch [1763/2000], Step [8/8], train_loss: 284.6439\n",
            "Val Loss:  482.2686716715495\n",
            "------------------------------EPOCH: 1764 ------------------------\n",
            "Epoch [1764/2000], Step [2/8], train_loss: 337.2486\n",
            "Epoch [1764/2000], Step [4/8], train_loss: 636.3310\n",
            "Epoch [1764/2000], Step [6/8], train_loss: 172.7403\n",
            "Epoch [1764/2000], Step [8/8], train_loss: 284.8607\n",
            "Val Loss:  254.36184422175089\n",
            "------------------------------EPOCH: 1765 ------------------------\n",
            "Epoch [1765/2000], Step [2/8], train_loss: 340.2998\n",
            "Epoch [1765/2000], Step [4/8], train_loss: 627.6135\n",
            "Epoch [1765/2000], Step [6/8], train_loss: 172.9140\n",
            "Epoch [1765/2000], Step [8/8], train_loss: 284.7648\n",
            "Val Loss:  382.9581227302551\n",
            "------------------------------EPOCH: 1766 ------------------------\n",
            "Epoch [1766/2000], Step [2/8], train_loss: 338.4396\n",
            "Epoch [1766/2000], Step [4/8], train_loss: 636.1059\n",
            "Epoch [1766/2000], Step [6/8], train_loss: 172.3005\n",
            "Epoch [1766/2000], Step [8/8], train_loss: 284.9706\n",
            "Val Loss:  407.54113324483234\n",
            "------------------------------EPOCH: 1767 ------------------------\n",
            "Epoch [1767/2000], Step [2/8], train_loss: 340.4093\n",
            "Epoch [1767/2000], Step [4/8], train_loss: 628.1417\n",
            "Epoch [1767/2000], Step [6/8], train_loss: 173.4022\n",
            "Epoch [1767/2000], Step [8/8], train_loss: 284.7247\n",
            "Val Loss:  574.1425011952718\n",
            "------------------------------EPOCH: 1768 ------------------------\n",
            "Epoch [1768/2000], Step [2/8], train_loss: 337.6284\n",
            "Epoch [1768/2000], Step [4/8], train_loss: 635.3384\n",
            "Epoch [1768/2000], Step [6/8], train_loss: 172.3075\n",
            "Epoch [1768/2000], Step [8/8], train_loss: 284.9151\n",
            "Val Loss:  246.8093388080597\n",
            "------------------------------EPOCH: 1769 ------------------------\n",
            "Epoch [1769/2000], Step [2/8], train_loss: 339.9743\n",
            "Epoch [1769/2000], Step [4/8], train_loss: 626.7656\n",
            "Epoch [1769/2000], Step [6/8], train_loss: 172.3999\n",
            "Epoch [1769/2000], Step [8/8], train_loss: 284.7833\n",
            "Val Loss:  428.25169944763184\n",
            "------------------------------EPOCH: 1770 ------------------------\n",
            "Epoch [1770/2000], Step [2/8], train_loss: 338.0740\n",
            "Epoch [1770/2000], Step [4/8], train_loss: 635.2241\n",
            "Epoch [1770/2000], Step [6/8], train_loss: 171.7115\n",
            "Epoch [1770/2000], Step [8/8], train_loss: 285.0373\n",
            "Val Loss:  262.7961564064026\n",
            "------------------------------EPOCH: 1771 ------------------------\n",
            "Epoch [1771/2000], Step [2/8], train_loss: 340.6031\n",
            "Epoch [1771/2000], Step [4/8], train_loss: 628.5634\n",
            "Epoch [1771/2000], Step [6/8], train_loss: 173.2408\n",
            "Epoch [1771/2000], Step [8/8], train_loss: 284.7740\n",
            "Val Loss:  780.3331921895345\n",
            "------------------------------EPOCH: 1772 ------------------------\n",
            "Epoch [1772/2000], Step [2/8], train_loss: 337.6570\n",
            "Epoch [1772/2000], Step [4/8], train_loss: 635.3367\n",
            "Epoch [1772/2000], Step [6/8], train_loss: 171.7307\n",
            "Epoch [1772/2000], Step [8/8], train_loss: 285.0406\n",
            "Val Loss:  390.9936059315999\n",
            "------------------------------EPOCH: 1773 ------------------------\n",
            "Epoch [1773/2000], Step [2/8], train_loss: 340.5616\n",
            "Epoch [1773/2000], Step [4/8], train_loss: 625.8137\n",
            "Epoch [1773/2000], Step [6/8], train_loss: 172.0939\n",
            "Epoch [1773/2000], Step [8/8], train_loss: 284.8596\n",
            "Val Loss:  777.1325937906901\n",
            "------------------------------EPOCH: 1774 ------------------------\n",
            "Epoch [1774/2000], Step [2/8], train_loss: 338.1724\n",
            "Epoch [1774/2000], Step [4/8], train_loss: 636.4056\n",
            "Epoch [1774/2000], Step [6/8], train_loss: 171.3608\n",
            "Epoch [1774/2000], Step [8/8], train_loss: 285.0835\n",
            "Val Loss:  317.8670123418172\n",
            "------------------------------EPOCH: 1775 ------------------------\n",
            "Epoch [1775/2000], Step [2/8], train_loss: 340.6228\n",
            "Epoch [1775/2000], Step [4/8], train_loss: 627.8890\n",
            "Epoch [1775/2000], Step [6/8], train_loss: 173.1765\n",
            "Epoch [1775/2000], Step [8/8], train_loss: 284.7583\n",
            "Val Loss:  444.4110180536906\n",
            "------------------------------EPOCH: 1776 ------------------------\n",
            "Epoch [1776/2000], Step [2/8], train_loss: 336.4515\n",
            "Epoch [1776/2000], Step [4/8], train_loss: 634.1642\n",
            "Epoch [1776/2000], Step [6/8], train_loss: 172.1290\n",
            "Epoch [1776/2000], Step [8/8], train_loss: 285.0262\n",
            "Val Loss:  120.92227204640706\n",
            "------------------------------EPOCH: 1777 ------------------------\n",
            "Epoch [1777/2000], Step [2/8], train_loss: 339.4919\n",
            "Epoch [1777/2000], Step [4/8], train_loss: 625.7783\n",
            "Epoch [1777/2000], Step [6/8], train_loss: 172.6331\n",
            "Epoch [1777/2000], Step [8/8], train_loss: 284.8907\n",
            "Val Loss:  183.2190844217936\n",
            "------------------------------EPOCH: 1778 ------------------------\n",
            "Epoch [1778/2000], Step [2/8], train_loss: 337.0256\n",
            "Epoch [1778/2000], Step [4/8], train_loss: 634.2401\n",
            "Epoch [1778/2000], Step [6/8], train_loss: 172.8329\n",
            "Epoch [1778/2000], Step [8/8], train_loss: 284.9605\n",
            "Val Loss:  171.4931723276774\n",
            "------------------------------EPOCH: 1779 ------------------------\n",
            "Epoch [1779/2000], Step [2/8], train_loss: 337.8198\n",
            "Epoch [1779/2000], Step [4/8], train_loss: 631.7794\n",
            "Epoch [1779/2000], Step [6/8], train_loss: 172.5856\n",
            "Epoch [1779/2000], Step [8/8], train_loss: 284.9069\n",
            "Val Loss:  214.38801908493042\n",
            "------------------------------EPOCH: 1780 ------------------------\n",
            "Epoch [1780/2000], Step [2/8], train_loss: 337.6739\n",
            "Epoch [1780/2000], Step [4/8], train_loss: 631.8122\n",
            "Epoch [1780/2000], Step [6/8], train_loss: 171.9342\n",
            "Epoch [1780/2000], Step [8/8], train_loss: 285.0424\n",
            "Val Loss:  221.4413162867228\n",
            "------------------------------EPOCH: 1781 ------------------------\n",
            "Epoch [1781/2000], Step [2/8], train_loss: 338.6511\n",
            "Epoch [1781/2000], Step [4/8], train_loss: 631.9187\n",
            "Epoch [1781/2000], Step [6/8], train_loss: 172.3157\n",
            "Epoch [1781/2000], Step [8/8], train_loss: 285.0290\n",
            "Val Loss:  354.1501447359721\n",
            "------------------------------EPOCH: 1782 ------------------------\n",
            "Epoch [1782/2000], Step [2/8], train_loss: 337.3933\n",
            "Epoch [1782/2000], Step [4/8], train_loss: 632.8646\n",
            "Epoch [1782/2000], Step [6/8], train_loss: 172.6659\n",
            "Epoch [1782/2000], Step [8/8], train_loss: 285.0293\n",
            "Val Loss:  297.22841755549115\n",
            "------------------------------EPOCH: 1783 ------------------------\n",
            "Epoch [1783/2000], Step [2/8], train_loss: 337.2315\n",
            "Epoch [1783/2000], Step [4/8], train_loss: 632.0803\n",
            "Epoch [1783/2000], Step [6/8], train_loss: 172.1903\n",
            "Epoch [1783/2000], Step [8/8], train_loss: 285.0994\n",
            "Val Loss:  250.12305680910745\n",
            "------------------------------EPOCH: 1784 ------------------------\n",
            "Epoch [1784/2000], Step [2/8], train_loss: 337.6080\n",
            "Epoch [1784/2000], Step [4/8], train_loss: 629.1823\n",
            "Epoch [1784/2000], Step [6/8], train_loss: 171.8439\n",
            "Epoch [1784/2000], Step [8/8], train_loss: 285.1585\n",
            "Val Loss:  292.51134459177655\n",
            "------------------------------EPOCH: 1785 ------------------------\n",
            "Epoch [1785/2000], Step [2/8], train_loss: 337.8927\n",
            "Epoch [1785/2000], Step [4/8], train_loss: 633.0186\n",
            "Epoch [1785/2000], Step [6/8], train_loss: 171.7558\n",
            "Epoch [1785/2000], Step [8/8], train_loss: 285.2277\n",
            "Val Loss:  209.9236863454183\n",
            "------------------------------EPOCH: 1786 ------------------------\n",
            "Epoch [1786/2000], Step [2/8], train_loss: 337.8362\n",
            "Epoch [1786/2000], Step [4/8], train_loss: 631.8712\n",
            "Epoch [1786/2000], Step [6/8], train_loss: 172.9186\n",
            "Epoch [1786/2000], Step [8/8], train_loss: 285.0883\n",
            "Val Loss:  264.23643859227496\n",
            "------------------------------EPOCH: 1787 ------------------------\n",
            "Epoch [1787/2000], Step [2/8], train_loss: 336.1237\n",
            "Epoch [1787/2000], Step [4/8], train_loss: 632.3557\n",
            "Epoch [1787/2000], Step [6/8], train_loss: 172.1571\n",
            "Epoch [1787/2000], Step [8/8], train_loss: 285.2572\n",
            "Val Loss:  192.9313099384308\n",
            "------------------------------EPOCH: 1788 ------------------------\n",
            "Epoch [1788/2000], Step [2/8], train_loss: 337.8593\n",
            "Epoch [1788/2000], Step [4/8], train_loss: 629.0579\n",
            "Epoch [1788/2000], Step [6/8], train_loss: 172.2173\n",
            "Epoch [1788/2000], Step [8/8], train_loss: 285.2480\n",
            "Val Loss:  171.54876136779785\n",
            "------------------------------EPOCH: 1789 ------------------------\n",
            "Epoch [1789/2000], Step [2/8], train_loss: 336.6146\n",
            "Epoch [1789/2000], Step [4/8], train_loss: 632.4721\n",
            "Epoch [1789/2000], Step [6/8], train_loss: 172.7386\n",
            "Epoch [1789/2000], Step [8/8], train_loss: 285.2202\n",
            "Val Loss:  219.83391682306925\n",
            "------------------------------EPOCH: 1790 ------------------------\n",
            "Epoch [1790/2000], Step [2/8], train_loss: 336.6806\n",
            "Epoch [1790/2000], Step [4/8], train_loss: 633.2128\n",
            "Epoch [1790/2000], Step [6/8], train_loss: 172.1649\n",
            "Epoch [1790/2000], Step [8/8], train_loss: 285.2617\n",
            "Val Loss:  239.70671331882477\n",
            "------------------------------EPOCH: 1791 ------------------------\n",
            "Epoch [1791/2000], Step [2/8], train_loss: 337.1148\n",
            "Epoch [1791/2000], Step [4/8], train_loss: 630.5529\n",
            "Epoch [1791/2000], Step [6/8], train_loss: 171.8903\n",
            "Epoch [1791/2000], Step [8/8], train_loss: 285.2961\n",
            "Val Loss:  230.0060010353724\n",
            "------------------------------EPOCH: 1792 ------------------------\n",
            "Epoch [1792/2000], Step [2/8], train_loss: 336.9078\n",
            "Epoch [1792/2000], Step [4/8], train_loss: 634.0828\n",
            "Epoch [1792/2000], Step [6/8], train_loss: 171.6238\n",
            "Epoch [1792/2000], Step [8/8], train_loss: 285.4181\n",
            "Val Loss:  219.3294942776362\n",
            "------------------------------EPOCH: 1793 ------------------------\n",
            "Epoch [1793/2000], Step [2/8], train_loss: 337.0455\n",
            "Epoch [1793/2000], Step [4/8], train_loss: 631.4219\n",
            "Epoch [1793/2000], Step [6/8], train_loss: 172.8029\n",
            "Epoch [1793/2000], Step [8/8], train_loss: 285.3203\n",
            "Val Loss:  284.4717940489451\n",
            "------------------------------EPOCH: 1794 ------------------------\n",
            "Epoch [1794/2000], Step [2/8], train_loss: 335.5013\n",
            "Epoch [1794/2000], Step [4/8], train_loss: 630.5969\n",
            "Epoch [1794/2000], Step [6/8], train_loss: 172.0714\n",
            "Epoch [1794/2000], Step [8/8], train_loss: 285.4560\n",
            "Val Loss:  227.77169036865234\n",
            "------------------------------EPOCH: 1795 ------------------------\n",
            "Epoch [1795/2000], Step [2/8], train_loss: 336.9732\n",
            "Epoch [1795/2000], Step [4/8], train_loss: 630.3284\n",
            "Epoch [1795/2000], Step [6/8], train_loss: 172.2799\n",
            "Epoch [1795/2000], Step [8/8], train_loss: 285.4363\n",
            "Val Loss:  272.62345965703327\n",
            "------------------------------EPOCH: 1796 ------------------------\n",
            "Epoch [1796/2000], Step [2/8], train_loss: 336.2031\n",
            "Epoch [1796/2000], Step [4/8], train_loss: 632.4995\n",
            "Epoch [1796/2000], Step [6/8], train_loss: 172.4863\n",
            "Epoch [1796/2000], Step [8/8], train_loss: 285.4511\n",
            "Val Loss:  294.799765030543\n",
            "------------------------------EPOCH: 1797 ------------------------\n",
            "Epoch [1797/2000], Step [2/8], train_loss: 336.3012\n",
            "Epoch [1797/2000], Step [4/8], train_loss: 632.9255\n",
            "Epoch [1797/2000], Step [6/8], train_loss: 172.1702\n",
            "Epoch [1797/2000], Step [8/8], train_loss: 285.5021\n",
            "Val Loss:  330.5062017440796\n",
            "------------------------------EPOCH: 1798 ------------------------\n",
            "Epoch [1798/2000], Step [2/8], train_loss: 336.0129\n",
            "Epoch [1798/2000], Step [4/8], train_loss: 630.5453\n",
            "Epoch [1798/2000], Step [6/8], train_loss: 172.1147\n",
            "Epoch [1798/2000], Step [8/8], train_loss: 285.5023\n",
            "Val Loss:  455.528860727946\n",
            "------------------------------EPOCH: 1799 ------------------------\n",
            "Epoch [1799/2000], Step [2/8], train_loss: 336.2438\n",
            "Epoch [1799/2000], Step [4/8], train_loss: 634.9163\n",
            "Epoch [1799/2000], Step [6/8], train_loss: 171.6006\n",
            "Epoch [1799/2000], Step [8/8], train_loss: 285.6381\n",
            "Val Loss:  411.187645594279\n",
            "------------------------------EPOCH: 1800 ------------------------\n",
            "Epoch [1800/2000], Step [2/8], train_loss: 336.7413\n",
            "Epoch [1800/2000], Step [4/8], train_loss: 630.3771\n",
            "Epoch [1800/2000], Step [6/8], train_loss: 172.6427\n",
            "Epoch [1800/2000], Step [8/8], train_loss: 285.5003\n",
            "Val Loss:  481.4031276702881\n",
            "------------------------------EPOCH: 1801 ------------------------\n",
            "Epoch [1801/2000], Step [2/8], train_loss: 334.9358\n",
            "Epoch [1801/2000], Step [4/8], train_loss: 630.9829\n",
            "Epoch [1801/2000], Step [6/8], train_loss: 171.8416\n",
            "Epoch [1801/2000], Step [8/8], train_loss: 285.6653\n",
            "Val Loss:  376.7622489929199\n",
            "------------------------------EPOCH: 1802 ------------------------\n",
            "Epoch [1802/2000], Step [2/8], train_loss: 336.4953\n",
            "Epoch [1802/2000], Step [4/8], train_loss: 632.7212\n",
            "Epoch [1802/2000], Step [6/8], train_loss: 172.3219\n",
            "Epoch [1802/2000], Step [8/8], train_loss: 285.5919\n",
            "Val Loss:  463.33876673380536\n",
            "------------------------------EPOCH: 1803 ------------------------\n",
            "Epoch [1803/2000], Step [2/8], train_loss: 334.8329\n",
            "Epoch [1803/2000], Step [4/8], train_loss: 631.7481\n",
            "Epoch [1803/2000], Step [6/8], train_loss: 172.6486\n",
            "Epoch [1803/2000], Step [8/8], train_loss: 285.6512\n",
            "Val Loss:  425.81958198547363\n",
            "------------------------------EPOCH: 1804 ------------------------\n",
            "Epoch [1804/2000], Step [2/8], train_loss: 335.2287\n",
            "Epoch [1804/2000], Step [4/8], train_loss: 632.2820\n",
            "Epoch [1804/2000], Step [6/8], train_loss: 172.7547\n",
            "Epoch [1804/2000], Step [8/8], train_loss: 285.6563\n",
            "Val Loss:  321.77507495880127\n",
            "------------------------------EPOCH: 1805 ------------------------\n",
            "Epoch [1805/2000], Step [2/8], train_loss: 335.5377\n",
            "Epoch [1805/2000], Step [4/8], train_loss: 632.2588\n",
            "Epoch [1805/2000], Step [6/8], train_loss: 172.2163\n",
            "Epoch [1805/2000], Step [8/8], train_loss: 285.7130\n",
            "Val Loss:  232.52253778775534\n",
            "------------------------------EPOCH: 1806 ------------------------\n",
            "Epoch [1806/2000], Step [2/8], train_loss: 334.9002\n",
            "Epoch [1806/2000], Step [4/8], train_loss: 635.2715\n",
            "Epoch [1806/2000], Step [6/8], train_loss: 173.6165\n",
            "Epoch [1806/2000], Step [8/8], train_loss: 285.5272\n",
            "Val Loss:  388.96869802474976\n",
            "------------------------------EPOCH: 1807 ------------------------\n",
            "Epoch [1807/2000], Step [2/8], train_loss: 333.6339\n",
            "Epoch [1807/2000], Step [4/8], train_loss: 632.0787\n",
            "Epoch [1807/2000], Step [6/8], train_loss: 171.8970\n",
            "Epoch [1807/2000], Step [8/8], train_loss: 285.7628\n",
            "Val Loss:  241.36659065882364\n",
            "------------------------------EPOCH: 1808 ------------------------\n",
            "Epoch [1808/2000], Step [2/8], train_loss: 336.1642\n",
            "Epoch [1808/2000], Step [4/8], train_loss: 632.9078\n",
            "Epoch [1808/2000], Step [6/8], train_loss: 172.6614\n",
            "Epoch [1808/2000], Step [8/8], train_loss: 285.6025\n",
            "Val Loss:  370.25802755355835\n",
            "------------------------------EPOCH: 1809 ------------------------\n",
            "Epoch [1809/2000], Step [2/8], train_loss: 333.4691\n",
            "Epoch [1809/2000], Step [4/8], train_loss: 633.1319\n",
            "Epoch [1809/2000], Step [6/8], train_loss: 172.5431\n",
            "Epoch [1809/2000], Step [8/8], train_loss: 285.7469\n",
            "Val Loss:  325.0130790869395\n",
            "------------------------------EPOCH: 1810 ------------------------\n",
            "Epoch [1810/2000], Step [2/8], train_loss: 334.4150\n",
            "Epoch [1810/2000], Step [4/8], train_loss: 631.9993\n",
            "Epoch [1810/2000], Step [6/8], train_loss: 172.8865\n",
            "Epoch [1810/2000], Step [8/8], train_loss: 285.7502\n",
            "Val Loss:  397.18137804667157\n",
            "------------------------------EPOCH: 1811 ------------------------\n",
            "Epoch [1811/2000], Step [2/8], train_loss: 334.6929\n",
            "Epoch [1811/2000], Step [4/8], train_loss: 632.1554\n",
            "Epoch [1811/2000], Step [6/8], train_loss: 171.9800\n",
            "Epoch [1811/2000], Step [8/8], train_loss: 285.8841\n",
            "Val Loss:  285.7988748550415\n",
            "------------------------------EPOCH: 1812 ------------------------\n",
            "Epoch [1812/2000], Step [2/8], train_loss: 335.6698\n",
            "Epoch [1812/2000], Step [4/8], train_loss: 635.0710\n",
            "Epoch [1812/2000], Step [6/8], train_loss: 173.6165\n",
            "Epoch [1812/2000], Step [8/8], train_loss: 285.5990\n",
            "Val Loss:  299.8334943453471\n",
            "------------------------------EPOCH: 1813 ------------------------\n",
            "Epoch [1813/2000], Step [2/8], train_loss: 332.6823\n",
            "Epoch [1813/2000], Step [4/8], train_loss: 632.7315\n",
            "Epoch [1813/2000], Step [6/8], train_loss: 172.0491\n",
            "Epoch [1813/2000], Step [8/8], train_loss: 285.8347\n",
            "Val Loss:  185.03902411460876\n",
            "------------------------------EPOCH: 1814 ------------------------\n",
            "Epoch [1814/2000], Step [2/8], train_loss: 335.1831\n",
            "Epoch [1814/2000], Step [4/8], train_loss: 631.4377\n",
            "Epoch [1814/2000], Step [6/8], train_loss: 173.3448\n",
            "Epoch [1814/2000], Step [8/8], train_loss: 285.6151\n",
            "Val Loss:  360.3921751976013\n",
            "------------------------------EPOCH: 1815 ------------------------\n",
            "Epoch [1815/2000], Step [2/8], train_loss: 332.6137\n",
            "Epoch [1815/2000], Step [4/8], train_loss: 634.0977\n",
            "Epoch [1815/2000], Step [6/8], train_loss: 173.3047\n",
            "Epoch [1815/2000], Step [8/8], train_loss: 285.7343\n",
            "Val Loss:  332.7884840965271\n",
            "------------------------------EPOCH: 1816 ------------------------\n",
            "Epoch [1816/2000], Step [2/8], train_loss: 334.0096\n",
            "Epoch [1816/2000], Step [4/8], train_loss: 633.1848\n",
            "Epoch [1816/2000], Step [6/8], train_loss: 173.2523\n",
            "Epoch [1816/2000], Step [8/8], train_loss: 285.6892\n",
            "Val Loss:  429.6108055114746\n",
            "------------------------------EPOCH: 1817 ------------------------\n",
            "Epoch [1817/2000], Step [2/8], train_loss: 333.8942\n",
            "Epoch [1817/2000], Step [4/8], train_loss: 630.7797\n",
            "Epoch [1817/2000], Step [6/8], train_loss: 172.0210\n",
            "Epoch [1817/2000], Step [8/8], train_loss: 285.8670\n",
            "Val Loss:  232.1478819847107\n",
            "------------------------------EPOCH: 1818 ------------------------\n",
            "Epoch [1818/2000], Step [2/8], train_loss: 334.9983\n",
            "Epoch [1818/2000], Step [4/8], train_loss: 634.3328\n",
            "Epoch [1818/2000], Step [6/8], train_loss: 174.1905\n",
            "Epoch [1818/2000], Step [8/8], train_loss: 285.5409\n",
            "Val Loss:  415.25640392303467\n",
            "------------------------------EPOCH: 1819 ------------------------\n",
            "Epoch [1819/2000], Step [2/8], train_loss: 332.0060\n",
            "Epoch [1819/2000], Step [4/8], train_loss: 631.8502\n",
            "Epoch [1819/2000], Step [6/8], train_loss: 172.1696\n",
            "Epoch [1819/2000], Step [8/8], train_loss: 285.8368\n",
            "Val Loss:  255.28343868255615\n",
            "------------------------------EPOCH: 1820 ------------------------\n",
            "Epoch [1820/2000], Step [2/8], train_loss: 335.1250\n",
            "Epoch [1820/2000], Step [4/8], train_loss: 631.5142\n",
            "Epoch [1820/2000], Step [6/8], train_loss: 173.5696\n",
            "Epoch [1820/2000], Step [8/8], train_loss: 285.5559\n",
            "Val Loss:  392.875253200531\n",
            "------------------------------EPOCH: 1821 ------------------------\n",
            "Epoch [1821/2000], Step [2/8], train_loss: 331.6936\n",
            "Epoch [1821/2000], Step [4/8], train_loss: 633.7568\n",
            "Epoch [1821/2000], Step [6/8], train_loss: 173.2599\n",
            "Epoch [1821/2000], Step [8/8], train_loss: 285.7357\n",
            "Val Loss:  325.5996147791545\n",
            "------------------------------EPOCH: 1822 ------------------------\n",
            "Epoch [1822/2000], Step [2/8], train_loss: 333.3128\n",
            "Epoch [1822/2000], Step [4/8], train_loss: 632.5066\n",
            "Epoch [1822/2000], Step [6/8], train_loss: 173.8570\n",
            "Epoch [1822/2000], Step [8/8], train_loss: 285.6455\n",
            "Val Loss:  395.52848386764526\n",
            "------------------------------EPOCH: 1823 ------------------------\n",
            "Epoch [1823/2000], Step [2/8], train_loss: 332.4701\n",
            "Epoch [1823/2000], Step [4/8], train_loss: 631.8296\n",
            "Epoch [1823/2000], Step [6/8], train_loss: 172.4700\n",
            "Epoch [1823/2000], Step [8/8], train_loss: 285.8712\n",
            "Val Loss:  293.7142939567566\n",
            "------------------------------EPOCH: 1824 ------------------------\n",
            "Epoch [1824/2000], Step [2/8], train_loss: 334.2827\n",
            "Epoch [1824/2000], Step [4/8], train_loss: 634.3932\n",
            "Epoch [1824/2000], Step [6/8], train_loss: 175.0006\n",
            "Epoch [1824/2000], Step [8/8], train_loss: 285.5019\n",
            "Val Loss:  502.54518445332843\n",
            "------------------------------EPOCH: 1825 ------------------------\n",
            "Epoch [1825/2000], Step [2/8], train_loss: 330.8995\n",
            "Epoch [1825/2000], Step [4/8], train_loss: 632.6980\n",
            "Epoch [1825/2000], Step [6/8], train_loss: 172.4326\n",
            "Epoch [1825/2000], Step [8/8], train_loss: 285.8580\n",
            "Val Loss:  302.15160942077637\n",
            "------------------------------EPOCH: 1826 ------------------------\n",
            "Epoch [1826/2000], Step [2/8], train_loss: 335.1252\n",
            "Epoch [1826/2000], Step [4/8], train_loss: 629.1703\n",
            "Epoch [1826/2000], Step [6/8], train_loss: 173.9195\n",
            "Epoch [1826/2000], Step [8/8], train_loss: 285.5091\n",
            "Val Loss:  382.4798822402954\n",
            "------------------------------EPOCH: 1827 ------------------------\n",
            "Epoch [1827/2000], Step [2/8], train_loss: 331.2579\n",
            "Epoch [1827/2000], Step [4/8], train_loss: 634.8453\n",
            "Epoch [1827/2000], Step [6/8], train_loss: 173.3176\n",
            "Epoch [1827/2000], Step [8/8], train_loss: 285.6774\n",
            "Val Loss:  215.9584240913391\n",
            "------------------------------EPOCH: 1828 ------------------------\n",
            "Epoch [1828/2000], Step [2/8], train_loss: 333.7072\n",
            "Epoch [1828/2000], Step [4/8], train_loss: 632.0508\n",
            "Epoch [1828/2000], Step [6/8], train_loss: 173.5638\n",
            "Epoch [1828/2000], Step [8/8], train_loss: 285.5623\n",
            "Val Loss:  239.86904207865396\n",
            "------------------------------EPOCH: 1829 ------------------------\n",
            "Epoch [1829/2000], Step [2/8], train_loss: 331.8041\n",
            "Epoch [1829/2000], Step [4/8], train_loss: 632.6740\n",
            "Epoch [1829/2000], Step [6/8], train_loss: 172.4331\n",
            "Epoch [1829/2000], Step [8/8], train_loss: 285.8069\n",
            "Val Loss:  157.82635911305746\n",
            "------------------------------EPOCH: 1830 ------------------------\n",
            "Epoch [1830/2000], Step [2/8], train_loss: 333.0954\n",
            "Epoch [1830/2000], Step [4/8], train_loss: 633.1464\n",
            "Epoch [1830/2000], Step [6/8], train_loss: 175.1348\n",
            "Epoch [1830/2000], Step [8/8], train_loss: 285.4919\n",
            "Val Loss:  552.146681467692\n",
            "------------------------------EPOCH: 1831 ------------------------\n",
            "Epoch [1831/2000], Step [2/8], train_loss: 330.5185\n",
            "Epoch [1831/2000], Step [4/8], train_loss: 634.2806\n",
            "Epoch [1831/2000], Step [6/8], train_loss: 172.5704\n",
            "Epoch [1831/2000], Step [8/8], train_loss: 285.8610\n",
            "Val Loss:  366.3867139816284\n",
            "------------------------------EPOCH: 1832 ------------------------\n",
            "Epoch [1832/2000], Step [2/8], train_loss: 334.9874\n",
            "Epoch [1832/2000], Step [4/8], train_loss: 630.5046\n",
            "Epoch [1832/2000], Step [6/8], train_loss: 173.4925\n",
            "Epoch [1832/2000], Step [8/8], train_loss: 285.5045\n",
            "Val Loss:  365.9753930568695\n",
            "------------------------------EPOCH: 1833 ------------------------\n",
            "Epoch [1833/2000], Step [2/8], train_loss: 330.5824\n",
            "Epoch [1833/2000], Step [4/8], train_loss: 635.0466\n",
            "Epoch [1833/2000], Step [6/8], train_loss: 173.3818\n",
            "Epoch [1833/2000], Step [8/8], train_loss: 285.6220\n",
            "Val Loss:  227.56098047892252\n",
            "------------------------------EPOCH: 1834 ------------------------\n",
            "Epoch [1834/2000], Step [2/8], train_loss: 332.0972\n",
            "Epoch [1834/2000], Step [4/8], train_loss: 632.1983\n",
            "Epoch [1834/2000], Step [6/8], train_loss: 174.1651\n",
            "Epoch [1834/2000], Step [8/8], train_loss: 285.5154\n",
            "Val Loss:  353.6114062468211\n",
            "------------------------------EPOCH: 1835 ------------------------\n",
            "Epoch [1835/2000], Step [2/8], train_loss: 331.1749\n",
            "Epoch [1835/2000], Step [4/8], train_loss: 632.9214\n",
            "Epoch [1835/2000], Step [6/8], train_loss: 172.6784\n",
            "Epoch [1835/2000], Step [8/8], train_loss: 285.7617\n",
            "Val Loss:  241.5333152214686\n",
            "------------------------------EPOCH: 1836 ------------------------\n",
            "Epoch [1836/2000], Step [2/8], train_loss: 332.9728\n",
            "Epoch [1836/2000], Step [4/8], train_loss: 633.3188\n",
            "Epoch [1836/2000], Step [6/8], train_loss: 174.8457\n",
            "Epoch [1836/2000], Step [8/8], train_loss: 285.3858\n",
            "Val Loss:  387.8082248369853\n",
            "------------------------------EPOCH: 1837 ------------------------\n",
            "Epoch [1837/2000], Step [2/8], train_loss: 330.2839\n",
            "Epoch [1837/2000], Step [4/8], train_loss: 634.2933\n",
            "Epoch [1837/2000], Step [6/8], train_loss: 171.9585\n",
            "Epoch [1837/2000], Step [8/8], train_loss: 285.7737\n",
            "Val Loss:  240.5147486925125\n",
            "------------------------------EPOCH: 1838 ------------------------\n",
            "Epoch [1838/2000], Step [2/8], train_loss: 334.9927\n",
            "Epoch [1838/2000], Step [4/8], train_loss: 629.1316\n",
            "Epoch [1838/2000], Step [6/8], train_loss: 173.2763\n",
            "Epoch [1838/2000], Step [8/8], train_loss: 285.3946\n",
            "Val Loss:  487.6206707954407\n",
            "------------------------------EPOCH: 1839 ------------------------\n",
            "Epoch [1839/2000], Step [2/8], train_loss: 330.2126\n",
            "Epoch [1839/2000], Step [4/8], train_loss: 635.4323\n",
            "Epoch [1839/2000], Step [6/8], train_loss: 173.4229\n",
            "Epoch [1839/2000], Step [8/8], train_loss: 285.5809\n",
            "Val Loss:  341.552738348643\n",
            "------------------------------EPOCH: 1840 ------------------------\n",
            "Epoch [1840/2000], Step [2/8], train_loss: 332.1576\n",
            "Epoch [1840/2000], Step [4/8], train_loss: 631.8354\n",
            "Epoch [1840/2000], Step [6/8], train_loss: 174.3796\n",
            "Epoch [1840/2000], Step [8/8], train_loss: 285.3625\n",
            "Val Loss:  462.05047067006427\n",
            "------------------------------EPOCH: 1841 ------------------------\n",
            "Epoch [1841/2000], Step [2/8], train_loss: 330.1101\n",
            "Epoch [1841/2000], Step [4/8], train_loss: 634.0145\n",
            "Epoch [1841/2000], Step [6/8], train_loss: 172.9999\n",
            "Epoch [1841/2000], Step [8/8], train_loss: 285.6273\n",
            "Val Loss:  330.27803071339923\n",
            "------------------------------EPOCH: 1842 ------------------------\n",
            "Epoch [1842/2000], Step [2/8], train_loss: 332.5507\n",
            "Epoch [1842/2000], Step [4/8], train_loss: 632.8202\n",
            "Epoch [1842/2000], Step [6/8], train_loss: 175.6770\n",
            "Epoch [1842/2000], Step [8/8], train_loss: 285.2587\n",
            "Val Loss:  307.3084348837535\n",
            "------------------------------EPOCH: 1843 ------------------------\n",
            "Epoch [1843/2000], Step [2/8], train_loss: 328.8933\n",
            "Epoch [1843/2000], Step [4/8], train_loss: 635.0555\n",
            "Epoch [1843/2000], Step [6/8], train_loss: 173.0397\n",
            "Epoch [1843/2000], Step [8/8], train_loss: 285.5993\n",
            "Val Loss:  127.460862159729\n",
            "------------------------------EPOCH: 1844 ------------------------\n",
            "Epoch [1844/2000], Step [2/8], train_loss: 333.0260\n",
            "Epoch [1844/2000], Step [4/8], train_loss: 627.2931\n",
            "Epoch [1844/2000], Step [6/8], train_loss: 174.0687\n",
            "Epoch [1844/2000], Step [8/8], train_loss: 285.2393\n",
            "Val Loss:  171.31575767199197\n",
            "------------------------------EPOCH: 1845 ------------------------\n",
            "Epoch [1845/2000], Step [2/8], train_loss: 329.0525\n",
            "Epoch [1845/2000], Step [4/8], train_loss: 635.6305\n",
            "Epoch [1845/2000], Step [6/8], train_loss: 173.7840\n",
            "Epoch [1845/2000], Step [8/8], train_loss: 285.4785\n",
            "Val Loss:  128.05401722590128\n",
            "------------------------------EPOCH: 1846 ------------------------\n",
            "Epoch [1846/2000], Step [2/8], train_loss: 332.3072\n",
            "Epoch [1846/2000], Step [4/8], train_loss: 631.7998\n",
            "Epoch [1846/2000], Step [6/8], train_loss: 173.7917\n",
            "Epoch [1846/2000], Step [8/8], train_loss: 285.2877\n",
            "Val Loss:  220.93108781178793\n",
            "------------------------------EPOCH: 1847 ------------------------\n",
            "Epoch [1847/2000], Step [2/8], train_loss: 330.5335\n",
            "Epoch [1847/2000], Step [4/8], train_loss: 634.2775\n",
            "Epoch [1847/2000], Step [6/8], train_loss: 172.3458\n",
            "Epoch [1847/2000], Step [8/8], train_loss: 285.5253\n",
            "Val Loss:  182.53955181439719\n",
            "------------------------------EPOCH: 1848 ------------------------\n",
            "Epoch [1848/2000], Step [2/8], train_loss: 332.3118\n",
            "Epoch [1848/2000], Step [4/8], train_loss: 631.9721\n",
            "Epoch [1848/2000], Step [6/8], train_loss: 174.9151\n",
            "Epoch [1848/2000], Step [8/8], train_loss: 285.2412\n",
            "Val Loss:  360.03935623168945\n",
            "------------------------------EPOCH: 1849 ------------------------\n",
            "Epoch [1849/2000], Step [2/8], train_loss: 329.1141\n",
            "Epoch [1849/2000], Step [4/8], train_loss: 635.4827\n",
            "Epoch [1849/2000], Step [6/8], train_loss: 172.8319\n",
            "Epoch [1849/2000], Step [8/8], train_loss: 285.5698\n",
            "Val Loss:  223.4695989290873\n",
            "------------------------------EPOCH: 1850 ------------------------\n",
            "Epoch [1850/2000], Step [2/8], train_loss: 332.8112\n",
            "Epoch [1850/2000], Step [4/8], train_loss: 628.2910\n",
            "Epoch [1850/2000], Step [6/8], train_loss: 173.9830\n",
            "Epoch [1850/2000], Step [8/8], train_loss: 285.2417\n",
            "Val Loss:  498.07176144917804\n",
            "------------------------------EPOCH: 1851 ------------------------\n",
            "Epoch [1851/2000], Step [2/8], train_loss: 329.1067\n",
            "Epoch [1851/2000], Step [4/8], train_loss: 635.0184\n",
            "Epoch [1851/2000], Step [6/8], train_loss: 173.5507\n",
            "Epoch [1851/2000], Step [8/8], train_loss: 285.4384\n",
            "Val Loss:  323.01329787572223\n",
            "------------------------------EPOCH: 1852 ------------------------\n",
            "Epoch [1852/2000], Step [2/8], train_loss: 332.0699\n",
            "Epoch [1852/2000], Step [4/8], train_loss: 632.7328\n",
            "Epoch [1852/2000], Step [6/8], train_loss: 173.1903\n",
            "Epoch [1852/2000], Step [8/8], train_loss: 285.2525\n",
            "Val Loss:  313.88428513209027\n",
            "------------------------------EPOCH: 1853 ------------------------\n",
            "Epoch [1853/2000], Step [2/8], train_loss: 329.9978\n",
            "Epoch [1853/2000], Step [4/8], train_loss: 635.4202\n",
            "Epoch [1853/2000], Step [6/8], train_loss: 172.4619\n",
            "Epoch [1853/2000], Step [8/8], train_loss: 285.3915\n",
            "Val Loss:  192.54863786697388\n",
            "------------------------------EPOCH: 1854 ------------------------\n",
            "Epoch [1854/2000], Step [2/8], train_loss: 330.4982\n",
            "Epoch [1854/2000], Step [4/8], train_loss: 633.0132\n",
            "Epoch [1854/2000], Step [6/8], train_loss: 175.6088\n",
            "Epoch [1854/2000], Step [8/8], train_loss: 285.1543\n",
            "Val Loss:  338.99031551678974\n",
            "------------------------------EPOCH: 1855 ------------------------\n",
            "Epoch [1855/2000], Step [2/8], train_loss: 327.4148\n",
            "Epoch [1855/2000], Step [4/8], train_loss: 633.8782\n",
            "Epoch [1855/2000], Step [6/8], train_loss: 174.5645\n",
            "Epoch [1855/2000], Step [8/8], train_loss: 285.4198\n",
            "Val Loss:  248.0314598083496\n",
            "------------------------------EPOCH: 1856 ------------------------\n",
            "Epoch [1856/2000], Step [2/8], train_loss: 330.7226\n",
            "Epoch [1856/2000], Step [4/8], train_loss: 631.0410\n",
            "Epoch [1856/2000], Step [6/8], train_loss: 175.2394\n",
            "Epoch [1856/2000], Step [8/8], train_loss: 285.2069\n",
            "Val Loss:  283.7433271408081\n",
            "------------------------------EPOCH: 1857 ------------------------\n",
            "Epoch [1857/2000], Step [2/8], train_loss: 328.2476\n",
            "Epoch [1857/2000], Step [4/8], train_loss: 634.2294\n",
            "Epoch [1857/2000], Step [6/8], train_loss: 174.3357\n",
            "Epoch [1857/2000], Step [8/8], train_loss: 285.3668\n",
            "Val Loss:  269.2787826855977\n",
            "------------------------------EPOCH: 1858 ------------------------\n",
            "Epoch [1858/2000], Step [2/8], train_loss: 330.0201\n",
            "Epoch [1858/2000], Step [4/8], train_loss: 633.1983\n",
            "Epoch [1858/2000], Step [6/8], train_loss: 173.9456\n",
            "Epoch [1858/2000], Step [8/8], train_loss: 285.3217\n",
            "Val Loss:  316.1251788139343\n",
            "------------------------------EPOCH: 1859 ------------------------\n",
            "Epoch [1859/2000], Step [2/8], train_loss: 329.6019\n",
            "Epoch [1859/2000], Step [4/8], train_loss: 633.8333\n",
            "Epoch [1859/2000], Step [6/8], train_loss: 172.9663\n",
            "Epoch [1859/2000], Step [8/8], train_loss: 285.5466\n",
            "Val Loss:  225.52939224243164\n",
            "------------------------------EPOCH: 1860 ------------------------\n",
            "Epoch [1860/2000], Step [2/8], train_loss: 330.6897\n",
            "Epoch [1860/2000], Step [4/8], train_loss: 633.6674\n",
            "Epoch [1860/2000], Step [6/8], train_loss: 174.7288\n",
            "Epoch [1860/2000], Step [8/8], train_loss: 285.2757\n",
            "Val Loss:  273.20912504196167\n",
            "------------------------------EPOCH: 1861 ------------------------\n",
            "Epoch [1861/2000], Step [2/8], train_loss: 328.7072\n",
            "Epoch [1861/2000], Step [4/8], train_loss: 634.8070\n",
            "Epoch [1861/2000], Step [6/8], train_loss: 172.3659\n",
            "Epoch [1861/2000], Step [8/8], train_loss: 285.5783\n",
            "Val Loss:  172.1894407272339\n",
            "------------------------------EPOCH: 1862 ------------------------\n",
            "Epoch [1862/2000], Step [2/8], train_loss: 332.2725\n",
            "Epoch [1862/2000], Step [4/8], train_loss: 632.0037\n",
            "Epoch [1862/2000], Step [6/8], train_loss: 173.4272\n",
            "Epoch [1862/2000], Step [8/8], train_loss: 285.3361\n",
            "Val Loss:  278.7629539171855\n",
            "------------------------------EPOCH: 1863 ------------------------\n",
            "Epoch [1863/2000], Step [2/8], train_loss: 328.9604\n",
            "Epoch [1863/2000], Step [4/8], train_loss: 633.3439\n",
            "Epoch [1863/2000], Step [6/8], train_loss: 173.2892\n",
            "Epoch [1863/2000], Step [8/8], train_loss: 285.4864\n",
            "Val Loss:  264.07797225316364\n",
            "------------------------------EPOCH: 1864 ------------------------\n",
            "Epoch [1864/2000], Step [2/8], train_loss: 330.4153\n",
            "Epoch [1864/2000], Step [4/8], train_loss: 633.1765\n",
            "Epoch [1864/2000], Step [6/8], train_loss: 174.0034\n",
            "Epoch [1864/2000], Step [8/8], train_loss: 285.3389\n",
            "Val Loss:  354.2940745353699\n",
            "------------------------------EPOCH: 1865 ------------------------\n",
            "Epoch [1865/2000], Step [2/8], train_loss: 328.7648\n",
            "Epoch [1865/2000], Step [4/8], train_loss: 633.2938\n",
            "Epoch [1865/2000], Step [6/8], train_loss: 172.9380\n",
            "Epoch [1865/2000], Step [8/8], train_loss: 285.5869\n",
            "Val Loss:  220.56604401270548\n",
            "------------------------------EPOCH: 1866 ------------------------\n",
            "Epoch [1866/2000], Step [2/8], train_loss: 330.5105\n",
            "Epoch [1866/2000], Step [4/8], train_loss: 633.2323\n",
            "Epoch [1866/2000], Step [6/8], train_loss: 175.2898\n",
            "Epoch [1866/2000], Step [8/8], train_loss: 285.2569\n",
            "Val Loss:  311.1045564015706\n",
            "------------------------------EPOCH: 1867 ------------------------\n",
            "Epoch [1867/2000], Step [2/8], train_loss: 328.4745\n",
            "Epoch [1867/2000], Step [4/8], train_loss: 637.2693\n",
            "Epoch [1867/2000], Step [6/8], train_loss: 172.2407\n",
            "Epoch [1867/2000], Step [8/8], train_loss: 285.5589\n",
            "Val Loss:  222.37226104736328\n",
            "------------------------------EPOCH: 1868 ------------------------\n",
            "Epoch [1868/2000], Step [2/8], train_loss: 331.4235\n",
            "Epoch [1868/2000], Step [4/8], train_loss: 633.1387\n",
            "Epoch [1868/2000], Step [6/8], train_loss: 173.3167\n",
            "Epoch [1868/2000], Step [8/8], train_loss: 285.1731\n",
            "Val Loss:  416.4834861755371\n",
            "------------------------------EPOCH: 1869 ------------------------\n",
            "Epoch [1869/2000], Step [2/8], train_loss: 326.5764\n",
            "Epoch [1869/2000], Step [4/8], train_loss: 634.8626\n",
            "Epoch [1869/2000], Step [6/8], train_loss: 174.4267\n",
            "Epoch [1869/2000], Step [8/8], train_loss: 285.3548\n",
            "Val Loss:  274.6937699317932\n",
            "------------------------------EPOCH: 1870 ------------------------\n",
            "Epoch [1870/2000], Step [2/8], train_loss: 328.5806\n",
            "Epoch [1870/2000], Step [4/8], train_loss: 632.8491\n",
            "Epoch [1870/2000], Step [6/8], train_loss: 176.4471\n",
            "Epoch [1870/2000], Step [8/8], train_loss: 285.1353\n",
            "Val Loss:  356.28728946050006\n",
            "------------------------------EPOCH: 1871 ------------------------\n",
            "Epoch [1871/2000], Step [2/8], train_loss: 326.5635\n",
            "Epoch [1871/2000], Step [4/8], train_loss: 634.0726\n",
            "Epoch [1871/2000], Step [6/8], train_loss: 174.2174\n",
            "Epoch [1871/2000], Step [8/8], train_loss: 285.4669\n",
            "Val Loss:  186.93359009424844\n",
            "------------------------------EPOCH: 1872 ------------------------\n",
            "Epoch [1872/2000], Step [2/8], train_loss: 329.7076\n",
            "Epoch [1872/2000], Step [4/8], train_loss: 631.0467\n",
            "Epoch [1872/2000], Step [6/8], train_loss: 175.8722\n",
            "Epoch [1872/2000], Step [8/8], train_loss: 285.1404\n",
            "Val Loss:  394.00006739298504\n",
            "------------------------------EPOCH: 1873 ------------------------\n",
            "Epoch [1873/2000], Step [2/8], train_loss: 327.1246\n",
            "Epoch [1873/2000], Step [4/8], train_loss: 634.8157\n",
            "Epoch [1873/2000], Step [6/8], train_loss: 173.0068\n",
            "Epoch [1873/2000], Step [8/8], train_loss: 285.4933\n",
            "Val Loss:  246.11774698893228\n",
            "------------------------------EPOCH: 1874 ------------------------\n",
            "Epoch [1874/2000], Step [2/8], train_loss: 331.0508\n",
            "Epoch [1874/2000], Step [4/8], train_loss: 628.8909\n",
            "Epoch [1874/2000], Step [6/8], train_loss: 174.0961\n",
            "Epoch [1874/2000], Step [8/8], train_loss: 285.2021\n",
            "Val Loss:  372.0077870686849\n",
            "------------------------------EPOCH: 1875 ------------------------\n",
            "Epoch [1875/2000], Step [2/8], train_loss: 327.1727\n",
            "Epoch [1875/2000], Step [4/8], train_loss: 635.4921\n",
            "Epoch [1875/2000], Step [6/8], train_loss: 173.6651\n",
            "Epoch [1875/2000], Step [8/8], train_loss: 285.3791\n",
            "Val Loss:  280.16548426946\n",
            "------------------------------EPOCH: 1876 ------------------------\n",
            "Epoch [1876/2000], Step [2/8], train_loss: 329.3045\n",
            "Epoch [1876/2000], Step [4/8], train_loss: 629.7149\n",
            "Epoch [1876/2000], Step [6/8], train_loss: 173.9695\n",
            "Epoch [1876/2000], Step [8/8], train_loss: 285.2038\n",
            "Val Loss:  352.19879182179767\n",
            "------------------------------EPOCH: 1877 ------------------------\n",
            "Epoch [1877/2000], Step [2/8], train_loss: 327.7554\n",
            "Epoch [1877/2000], Step [4/8], train_loss: 633.2755\n",
            "Epoch [1877/2000], Step [6/8], train_loss: 172.7482\n",
            "Epoch [1877/2000], Step [8/8], train_loss: 285.4272\n",
            "Val Loss:  205.05714599291483\n",
            "------------------------------EPOCH: 1878 ------------------------\n",
            "Epoch [1878/2000], Step [2/8], train_loss: 329.5591\n",
            "Epoch [1878/2000], Step [4/8], train_loss: 632.7064\n",
            "Epoch [1878/2000], Step [6/8], train_loss: 175.0127\n",
            "Epoch [1878/2000], Step [8/8], train_loss: 285.0869\n",
            "Val Loss:  363.5843315124512\n",
            "------------------------------EPOCH: 1879 ------------------------\n",
            "Epoch [1879/2000], Step [2/8], train_loss: 326.4887\n",
            "Epoch [1879/2000], Step [4/8], train_loss: 634.0985\n",
            "Epoch [1879/2000], Step [6/8], train_loss: 172.7687\n",
            "Epoch [1879/2000], Step [8/8], train_loss: 285.4628\n",
            "Val Loss:  215.14562463760376\n",
            "------------------------------EPOCH: 1880 ------------------------\n",
            "Epoch [1880/2000], Step [2/8], train_loss: 330.4497\n",
            "Epoch [1880/2000], Step [4/8], train_loss: 628.7424\n",
            "Epoch [1880/2000], Step [6/8], train_loss: 173.9330\n",
            "Epoch [1880/2000], Step [8/8], train_loss: 285.1394\n",
            "Val Loss:  259.27652978897095\n",
            "------------------------------EPOCH: 1881 ------------------------\n",
            "Epoch [1881/2000], Step [2/8], train_loss: 326.6548\n",
            "Epoch [1881/2000], Step [4/8], train_loss: 634.9078\n",
            "Epoch [1881/2000], Step [6/8], train_loss: 173.6944\n",
            "Epoch [1881/2000], Step [8/8], train_loss: 285.3158\n",
            "Val Loss:  182.31259044011435\n",
            "------------------------------EPOCH: 1882 ------------------------\n",
            "Epoch [1882/2000], Step [2/8], train_loss: 328.5790\n",
            "Epoch [1882/2000], Step [4/8], train_loss: 630.7372\n",
            "Epoch [1882/2000], Step [6/8], train_loss: 173.9848\n",
            "Epoch [1882/2000], Step [8/8], train_loss: 285.1629\n",
            "Val Loss:  376.21803824106854\n",
            "------------------------------EPOCH: 1883 ------------------------\n",
            "Epoch [1883/2000], Step [2/8], train_loss: 327.6774\n",
            "Epoch [1883/2000], Step [4/8], train_loss: 632.1851\n",
            "Epoch [1883/2000], Step [6/8], train_loss: 172.4196\n",
            "Epoch [1883/2000], Step [8/8], train_loss: 285.4002\n",
            "Val Loss:  214.59305254618326\n",
            "------------------------------EPOCH: 1884 ------------------------\n",
            "Epoch [1884/2000], Step [2/8], train_loss: 329.8618\n",
            "Epoch [1884/2000], Step [4/8], train_loss: 631.6473\n",
            "Epoch [1884/2000], Step [6/8], train_loss: 174.6797\n",
            "Epoch [1884/2000], Step [8/8], train_loss: 284.9997\n",
            "Val Loss:  381.560063680013\n",
            "------------------------------EPOCH: 1885 ------------------------\n",
            "Epoch [1885/2000], Step [2/8], train_loss: 326.3569\n",
            "Epoch [1885/2000], Step [4/8], train_loss: 633.8389\n",
            "Epoch [1885/2000], Step [6/8], train_loss: 172.2549\n",
            "Epoch [1885/2000], Step [8/8], train_loss: 285.3901\n",
            "Val Loss:  253.56755860646567\n",
            "------------------------------EPOCH: 1886 ------------------------\n",
            "Epoch [1886/2000], Step [2/8], train_loss: 330.4826\n",
            "Epoch [1886/2000], Step [4/8], train_loss: 626.9451\n",
            "Epoch [1886/2000], Step [6/8], train_loss: 173.4225\n",
            "Epoch [1886/2000], Step [8/8], train_loss: 285.0103\n",
            "Val Loss:  513.1852903366089\n",
            "------------------------------EPOCH: 1887 ------------------------\n",
            "Epoch [1887/2000], Step [2/8], train_loss: 326.1283\n",
            "Epoch [1887/2000], Step [4/8], train_loss: 634.6469\n",
            "Epoch [1887/2000], Step [6/8], train_loss: 173.3143\n",
            "Epoch [1887/2000], Step [8/8], train_loss: 285.1589\n",
            "Val Loss:  322.2596960067749\n",
            "------------------------------EPOCH: 1888 ------------------------\n",
            "Epoch [1888/2000], Step [2/8], train_loss: 328.2250\n",
            "Epoch [1888/2000], Step [4/8], train_loss: 631.1427\n",
            "Epoch [1888/2000], Step [6/8], train_loss: 173.5893\n",
            "Epoch [1888/2000], Step [8/8], train_loss: 284.9229\n",
            "Val Loss:  482.6577212015788\n",
            "------------------------------EPOCH: 1889 ------------------------\n",
            "Epoch [1889/2000], Step [2/8], train_loss: 326.5342\n",
            "Epoch [1889/2000], Step [4/8], train_loss: 634.8994\n",
            "Epoch [1889/2000], Step [6/8], train_loss: 173.0642\n",
            "Epoch [1889/2000], Step [8/8], train_loss: 285.1362\n",
            "Val Loss:  289.9843160311381\n",
            "------------------------------EPOCH: 1890 ------------------------\n",
            "Epoch [1890/2000], Step [2/8], train_loss: 328.2109\n",
            "Epoch [1890/2000], Step [4/8], train_loss: 629.8829\n",
            "Epoch [1890/2000], Step [6/8], train_loss: 176.0396\n",
            "Epoch [1890/2000], Step [8/8], train_loss: 284.6953\n",
            "Val Loss:  468.9392228126526\n",
            "------------------------------EPOCH: 1891 ------------------------\n",
            "Epoch [1891/2000], Step [2/8], train_loss: 324.3607\n",
            "Epoch [1891/2000], Step [4/8], train_loss: 633.1972\n",
            "Epoch [1891/2000], Step [6/8], train_loss: 173.5729\n",
            "Epoch [1891/2000], Step [8/8], train_loss: 285.1141\n",
            "Val Loss:  219.66907246907553\n",
            "------------------------------EPOCH: 1892 ------------------------\n",
            "Epoch [1892/2000], Step [2/8], train_loss: 329.6125\n",
            "Epoch [1892/2000], Step [4/8], train_loss: 625.0081\n",
            "Epoch [1892/2000], Step [6/8], train_loss: 174.0912\n",
            "Epoch [1892/2000], Step [8/8], train_loss: 284.7701\n",
            "Val Loss:  370.2530501683553\n",
            "------------------------------EPOCH: 1893 ------------------------\n",
            "Epoch [1893/2000], Step [2/8], train_loss: 325.0758\n",
            "Epoch [1893/2000], Step [4/8], train_loss: 634.4152\n",
            "Epoch [1893/2000], Step [6/8], train_loss: 173.8491\n",
            "Epoch [1893/2000], Step [8/8], train_loss: 284.9016\n",
            "Val Loss:  253.26536750793457\n",
            "------------------------------EPOCH: 1894 ------------------------\n",
            "Epoch [1894/2000], Step [2/8], train_loss: 326.9232\n",
            "Epoch [1894/2000], Step [4/8], train_loss: 630.3097\n",
            "Epoch [1894/2000], Step [6/8], train_loss: 173.6945\n",
            "Epoch [1894/2000], Step [8/8], train_loss: 284.8091\n",
            "Val Loss:  344.3058675130208\n",
            "------------------------------EPOCH: 1895 ------------------------\n",
            "Epoch [1895/2000], Step [2/8], train_loss: 326.8582\n",
            "Epoch [1895/2000], Step [4/8], train_loss: 632.2307\n",
            "Epoch [1895/2000], Step [6/8], train_loss: 172.1040\n",
            "Epoch [1895/2000], Step [8/8], train_loss: 285.0330\n",
            "Val Loss:  231.01891152064005\n",
            "------------------------------EPOCH: 1896 ------------------------\n",
            "Epoch [1896/2000], Step [2/8], train_loss: 328.4260\n",
            "Epoch [1896/2000], Step [4/8], train_loss: 630.8298\n",
            "Epoch [1896/2000], Step [6/8], train_loss: 173.7402\n",
            "Epoch [1896/2000], Step [8/8], train_loss: 284.6956\n",
            "Val Loss:  533.6359624862671\n",
            "------------------------------EPOCH: 1897 ------------------------\n",
            "Epoch [1897/2000], Step [2/8], train_loss: 325.3526\n",
            "Epoch [1897/2000], Step [4/8], train_loss: 632.8586\n",
            "Epoch [1897/2000], Step [6/8], train_loss: 172.0005\n",
            "Epoch [1897/2000], Step [8/8], train_loss: 285.0484\n",
            "Val Loss:  383.94648838043213\n",
            "------------------------------EPOCH: 1898 ------------------------\n",
            "Epoch [1898/2000], Step [2/8], train_loss: 329.4895\n",
            "Epoch [1898/2000], Step [4/8], train_loss: 626.7567\n",
            "Epoch [1898/2000], Step [6/8], train_loss: 172.6657\n",
            "Epoch [1898/2000], Step [8/8], train_loss: 284.7326\n",
            "Val Loss:  497.9429346720378\n",
            "------------------------------EPOCH: 1899 ------------------------\n",
            "Epoch [1899/2000], Step [2/8], train_loss: 325.8025\n",
            "Epoch [1899/2000], Step [4/8], train_loss: 634.3644\n",
            "Epoch [1899/2000], Step [6/8], train_loss: 172.7408\n",
            "Epoch [1899/2000], Step [8/8], train_loss: 284.7902\n",
            "Val Loss:  343.18725872039795\n",
            "------------------------------EPOCH: 1900 ------------------------\n",
            "Epoch [1900/2000], Step [2/8], train_loss: 326.6627\n",
            "Epoch [1900/2000], Step [4/8], train_loss: 630.9409\n",
            "Epoch [1900/2000], Step [6/8], train_loss: 173.6971\n",
            "Epoch [1900/2000], Step [8/8], train_loss: 284.6920\n",
            "Val Loss:  431.0275993347168\n",
            "------------------------------EPOCH: 1901 ------------------------\n",
            "Epoch [1901/2000], Step [2/8], train_loss: 325.3668\n",
            "Epoch [1901/2000], Step [4/8], train_loss: 628.7551\n",
            "Epoch [1901/2000], Step [6/8], train_loss: 173.6414\n",
            "Epoch [1901/2000], Step [8/8], train_loss: 284.8185\n",
            "Val Loss:  255.12625312805176\n",
            "------------------------------EPOCH: 1902 ------------------------\n",
            "Epoch [1902/2000], Step [2/8], train_loss: 326.4017\n",
            "Epoch [1902/2000], Step [4/8], train_loss: 631.6944\n",
            "Epoch [1902/2000], Step [6/8], train_loss: 175.3694\n",
            "Epoch [1902/2000], Step [8/8], train_loss: 284.5508\n",
            "Val Loss:  374.4808406829834\n",
            "------------------------------EPOCH: 1903 ------------------------\n",
            "Epoch [1903/2000], Step [2/8], train_loss: 325.4041\n",
            "Epoch [1903/2000], Step [4/8], train_loss: 633.6993\n",
            "Epoch [1903/2000], Step [6/8], train_loss: 172.0748\n",
            "Epoch [1903/2000], Step [8/8], train_loss: 284.8953\n",
            "Val Loss:  216.69921143849692\n",
            "------------------------------EPOCH: 1904 ------------------------\n",
            "Epoch [1904/2000], Step [2/8], train_loss: 328.9830\n",
            "Epoch [1904/2000], Step [4/8], train_loss: 628.1389\n",
            "Epoch [1904/2000], Step [6/8], train_loss: 171.9027\n",
            "Epoch [1904/2000], Step [8/8], train_loss: 284.6232\n",
            "Val Loss:  441.08818372090656\n",
            "------------------------------EPOCH: 1905 ------------------------\n",
            "Epoch [1905/2000], Step [2/8], train_loss: 325.0565\n",
            "Epoch [1905/2000], Step [4/8], train_loss: 632.4401\n",
            "Epoch [1905/2000], Step [6/8], train_loss: 173.2954\n",
            "Epoch [1905/2000], Step [8/8], train_loss: 284.7151\n",
            "Val Loss:  279.5426360766093\n",
            "------------------------------EPOCH: 1906 ------------------------\n",
            "Epoch [1906/2000], Step [2/8], train_loss: 326.2650\n",
            "Epoch [1906/2000], Step [4/8], train_loss: 631.7672\n",
            "Epoch [1906/2000], Step [6/8], train_loss: 174.6163\n",
            "Epoch [1906/2000], Step [8/8], train_loss: 284.5468\n",
            "Val Loss:  391.44143708546954\n",
            "------------------------------EPOCH: 1907 ------------------------\n",
            "Epoch [1907/2000], Step [2/8], train_loss: 324.8385\n",
            "Epoch [1907/2000], Step [4/8], train_loss: 629.6619\n",
            "Epoch [1907/2000], Step [6/8], train_loss: 173.0472\n",
            "Epoch [1907/2000], Step [8/8], train_loss: 284.8138\n",
            "Val Loss:  209.8701134522756\n",
            "------------------------------EPOCH: 1908 ------------------------\n",
            "Epoch [1908/2000], Step [2/8], train_loss: 326.8535\n",
            "Epoch [1908/2000], Step [4/8], train_loss: 630.3924\n",
            "Epoch [1908/2000], Step [6/8], train_loss: 174.4537\n",
            "Epoch [1908/2000], Step [8/8], train_loss: 284.5104\n",
            "Val Loss:  475.1344596544902\n",
            "------------------------------EPOCH: 1909 ------------------------\n",
            "Epoch [1909/2000], Step [2/8], train_loss: 325.2479\n",
            "Epoch [1909/2000], Step [4/8], train_loss: 633.8479\n",
            "Epoch [1909/2000], Step [6/8], train_loss: 171.4376\n",
            "Epoch [1909/2000], Step [8/8], train_loss: 284.8555\n",
            "Val Loss:  309.4040543238322\n",
            "------------------------------EPOCH: 1910 ------------------------\n",
            "Epoch [1910/2000], Step [2/8], train_loss: 328.9259\n",
            "Epoch [1910/2000], Step [4/8], train_loss: 627.2881\n",
            "Epoch [1910/2000], Step [6/8], train_loss: 171.7607\n",
            "Epoch [1910/2000], Step [8/8], train_loss: 284.5030\n",
            "Val Loss:  437.15621821085614\n",
            "------------------------------EPOCH: 1911 ------------------------\n",
            "Epoch [1911/2000], Step [2/8], train_loss: 324.2586\n",
            "Epoch [1911/2000], Step [4/8], train_loss: 631.8179\n",
            "Epoch [1911/2000], Step [6/8], train_loss: 173.2346\n",
            "Epoch [1911/2000], Step [8/8], train_loss: 284.6707\n",
            "Val Loss:  223.06958826382956\n",
            "------------------------------EPOCH: 1912 ------------------------\n",
            "Epoch [1912/2000], Step [2/8], train_loss: 326.0755\n",
            "Epoch [1912/2000], Step [4/8], train_loss: 630.3210\n",
            "Epoch [1912/2000], Step [6/8], train_loss: 174.7141\n",
            "Epoch [1912/2000], Step [8/8], train_loss: 284.4487\n",
            "Val Loss:  383.3002179463704\n",
            "------------------------------EPOCH: 1913 ------------------------\n",
            "Epoch [1913/2000], Step [2/8], train_loss: 324.3060\n",
            "Epoch [1913/2000], Step [4/8], train_loss: 629.4615\n",
            "Epoch [1913/2000], Step [6/8], train_loss: 173.0444\n",
            "Epoch [1913/2000], Step [8/8], train_loss: 284.7113\n",
            "Val Loss:  274.38462050755817\n",
            "------------------------------EPOCH: 1914 ------------------------\n",
            "Epoch [1914/2000], Step [2/8], train_loss: 326.6642\n",
            "Epoch [1914/2000], Step [4/8], train_loss: 629.1523\n",
            "Epoch [1914/2000], Step [6/8], train_loss: 174.6484\n",
            "Epoch [1914/2000], Step [8/8], train_loss: 284.4032\n",
            "Val Loss:  453.2193233172099\n",
            "------------------------------EPOCH: 1915 ------------------------\n",
            "Epoch [1915/2000], Step [2/8], train_loss: 324.2484\n",
            "Epoch [1915/2000], Step [4/8], train_loss: 632.0713\n",
            "Epoch [1915/2000], Step [6/8], train_loss: 172.2676\n",
            "Epoch [1915/2000], Step [8/8], train_loss: 284.7299\n",
            "Val Loss:  250.14730183283487\n",
            "------------------------------EPOCH: 1916 ------------------------\n",
            "Epoch [1916/2000], Step [2/8], train_loss: 327.9959\n",
            "Epoch [1916/2000], Step [4/8], train_loss: 625.4503\n",
            "Epoch [1916/2000], Step [6/8], train_loss: 172.8284\n",
            "Epoch [1916/2000], Step [8/8], train_loss: 284.4086\n",
            "Val Loss:  350.4715765317281\n",
            "------------------------------EPOCH: 1917 ------------------------\n",
            "Epoch [1917/2000], Step [2/8], train_loss: 324.8802\n",
            "Epoch [1917/2000], Step [4/8], train_loss: 634.3127\n",
            "Epoch [1917/2000], Step [6/8], train_loss: 171.4881\n",
            "Epoch [1917/2000], Step [8/8], train_loss: 284.7295\n",
            "Val Loss:  201.13192462921143\n",
            "------------------------------EPOCH: 1918 ------------------------\n",
            "Epoch [1918/2000], Step [2/8], train_loss: 327.5170\n",
            "Epoch [1918/2000], Step [4/8], train_loss: 625.1362\n",
            "Epoch [1918/2000], Step [6/8], train_loss: 173.5797\n",
            "Epoch [1918/2000], Step [8/8], train_loss: 284.2886\n",
            "Val Loss:  492.43961747487384\n",
            "------------------------------EPOCH: 1919 ------------------------\n",
            "Epoch [1919/2000], Step [2/8], train_loss: 324.4714\n",
            "Epoch [1919/2000], Step [4/8], train_loss: 632.6431\n",
            "Epoch [1919/2000], Step [6/8], train_loss: 171.9544\n",
            "Epoch [1919/2000], Step [8/8], train_loss: 284.6302\n",
            "Val Loss:  281.2529579798381\n",
            "------------------------------EPOCH: 1920 ------------------------\n",
            "Epoch [1920/2000], Step [2/8], train_loss: 327.5520\n",
            "Epoch [1920/2000], Step [4/8], train_loss: 629.2876\n",
            "Epoch [1920/2000], Step [6/8], train_loss: 173.6913\n",
            "Epoch [1920/2000], Step [8/8], train_loss: 284.0002\n",
            "Val Loss:  578.6173922220866\n",
            "------------------------------EPOCH: 1921 ------------------------\n",
            "Epoch [1921/2000], Step [2/8], train_loss: 322.1757\n",
            "Epoch [1921/2000], Step [4/8], train_loss: 638.2603\n",
            "Epoch [1921/2000], Step [6/8], train_loss: 172.1764\n",
            "Epoch [1921/2000], Step [8/8], train_loss: 284.3909\n",
            "Val Loss:  311.92538619041443\n",
            "------------------------------EPOCH: 1922 ------------------------\n",
            "Epoch [1922/2000], Step [2/8], train_loss: 327.9686\n",
            "Epoch [1922/2000], Step [4/8], train_loss: 622.5370\n",
            "Epoch [1922/2000], Step [6/8], train_loss: 173.3558\n",
            "Epoch [1922/2000], Step [8/8], train_loss: 283.9203\n",
            "Val Loss:  655.3274618784586\n",
            "------------------------------EPOCH: 1923 ------------------------\n",
            "Epoch [1923/2000], Step [2/8], train_loss: 323.0630\n",
            "Epoch [1923/2000], Step [4/8], train_loss: 639.2052\n",
            "Epoch [1923/2000], Step [6/8], train_loss: 172.3025\n",
            "Epoch [1923/2000], Step [8/8], train_loss: 284.2627\n",
            "Val Loss:  435.64136060078937\n",
            "------------------------------EPOCH: 1924 ------------------------\n",
            "Epoch [1924/2000], Step [2/8], train_loss: 326.6708\n",
            "Epoch [1924/2000], Step [4/8], train_loss: 621.7858\n",
            "Epoch [1924/2000], Step [6/8], train_loss: 173.7216\n",
            "Epoch [1924/2000], Step [8/8], train_loss: 283.8568\n",
            "Val Loss:  535.0758085250854\n",
            "------------------------------EPOCH: 1925 ------------------------\n",
            "Epoch [1925/2000], Step [2/8], train_loss: 322.4603\n",
            "Epoch [1925/2000], Step [4/8], train_loss: 635.3160\n",
            "Epoch [1925/2000], Step [6/8], train_loss: 173.1013\n",
            "Epoch [1925/2000], Step [8/8], train_loss: 284.1167\n",
            "Val Loss:  234.58706649144492\n",
            "------------------------------EPOCH: 1926 ------------------------\n",
            "Epoch [1926/2000], Step [2/8], train_loss: 325.5017\n",
            "Epoch [1926/2000], Step [4/8], train_loss: 626.6411\n",
            "Epoch [1926/2000], Step [6/8], train_loss: 174.3411\n",
            "Epoch [1926/2000], Step [8/8], train_loss: 283.7740\n",
            "Val Loss:  247.987211227417\n",
            "------------------------------EPOCH: 1927 ------------------------\n",
            "Epoch [1927/2000], Step [2/8], train_loss: 322.5647\n",
            "Epoch [1927/2000], Step [4/8], train_loss: 635.2849\n",
            "Epoch [1927/2000], Step [6/8], train_loss: 172.1803\n",
            "Epoch [1927/2000], Step [8/8], train_loss: 284.1074\n",
            "Val Loss:  127.89510647455852\n",
            "------------------------------EPOCH: 1928 ------------------------\n",
            "Epoch [1928/2000], Step [2/8], train_loss: 326.7900\n",
            "Epoch [1928/2000], Step [4/8], train_loss: 622.4067\n",
            "Epoch [1928/2000], Step [6/8], train_loss: 172.2484\n",
            "Epoch [1928/2000], Step [8/8], train_loss: 283.8124\n",
            "Val Loss:  229.55668799082437\n",
            "------------------------------EPOCH: 1929 ------------------------\n",
            "Epoch [1929/2000], Step [2/8], train_loss: 323.6411\n",
            "Epoch [1929/2000], Step [4/8], train_loss: 635.9399\n",
            "Epoch [1929/2000], Step [6/8], train_loss: 171.5964\n",
            "Epoch [1929/2000], Step [8/8], train_loss: 284.0254\n",
            "Val Loss:  114.56572421391805\n",
            "------------------------------EPOCH: 1930 ------------------------\n",
            "Epoch [1930/2000], Step [2/8], train_loss: 326.0080\n",
            "Epoch [1930/2000], Step [4/8], train_loss: 625.4987\n",
            "Epoch [1930/2000], Step [6/8], train_loss: 173.0644\n",
            "Epoch [1930/2000], Step [8/8], train_loss: 283.6870\n",
            "Val Loss:  182.51036723454794\n",
            "------------------------------EPOCH: 1931 ------------------------\n",
            "Epoch [1931/2000], Step [2/8], train_loss: 323.2448\n",
            "Epoch [1931/2000], Step [4/8], train_loss: 631.8520\n",
            "Epoch [1931/2000], Step [6/8], train_loss: 171.8394\n",
            "Epoch [1931/2000], Step [8/8], train_loss: 283.9919\n",
            "Val Loss:  145.9134414990743\n",
            "------------------------------EPOCH: 1932 ------------------------\n",
            "Epoch [1932/2000], Step [2/8], train_loss: 326.0206\n",
            "Epoch [1932/2000], Step [4/8], train_loss: 628.9037\n",
            "Epoch [1932/2000], Step [6/8], train_loss: 173.0862\n",
            "Epoch [1932/2000], Step [8/8], train_loss: 283.5506\n",
            "Val Loss:  405.7030037244161\n",
            "------------------------------EPOCH: 1933 ------------------------\n",
            "Epoch [1933/2000], Step [2/8], train_loss: 322.4042\n",
            "Epoch [1933/2000], Step [4/8], train_loss: 637.5065\n",
            "Epoch [1933/2000], Step [6/8], train_loss: 171.8494\n",
            "Epoch [1933/2000], Step [8/8], train_loss: 283.8586\n",
            "Val Loss:  224.12666646639505\n",
            "------------------------------EPOCH: 1934 ------------------------\n",
            "Epoch [1934/2000], Step [2/8], train_loss: 326.5536\n",
            "Epoch [1934/2000], Step [4/8], train_loss: 621.3760\n",
            "Epoch [1934/2000], Step [6/8], train_loss: 173.1032\n",
            "Epoch [1934/2000], Step [8/8], train_loss: 283.4672\n",
            "Val Loss:  549.764066696167\n",
            "------------------------------EPOCH: 1935 ------------------------\n",
            "Epoch [1935/2000], Step [2/8], train_loss: 322.6763\n",
            "Epoch [1935/2000], Step [4/8], train_loss: 636.1742\n",
            "Epoch [1935/2000], Step [6/8], train_loss: 172.2174\n",
            "Epoch [1935/2000], Step [8/8], train_loss: 283.7834\n",
            "Val Loss:  324.1342309315999\n",
            "------------------------------EPOCH: 1936 ------------------------\n",
            "Epoch [1936/2000], Step [2/8], train_loss: 325.9392\n",
            "Epoch [1936/2000], Step [4/8], train_loss: 622.4777\n",
            "Epoch [1936/2000], Step [6/8], train_loss: 173.5652\n",
            "Epoch [1936/2000], Step [8/8], train_loss: 283.3892\n",
            "Val Loss:  657.8181546529134\n",
            "------------------------------EPOCH: 1937 ------------------------\n",
            "Epoch [1937/2000], Step [2/8], train_loss: 322.0990\n",
            "Epoch [1937/2000], Step [4/8], train_loss: 633.2009\n",
            "Epoch [1937/2000], Step [6/8], train_loss: 172.5080\n",
            "Epoch [1937/2000], Step [8/8], train_loss: 283.6734\n",
            "Val Loss:  366.7151343027751\n",
            "------------------------------EPOCH: 1938 ------------------------\n",
            "Epoch [1938/2000], Step [2/8], train_loss: 324.6852\n",
            "Epoch [1938/2000], Step [4/8], train_loss: 628.0089\n",
            "Epoch [1938/2000], Step [6/8], train_loss: 173.8241\n",
            "Epoch [1938/2000], Step [8/8], train_loss: 283.2744\n",
            "Val Loss:  575.325543721517\n",
            "------------------------------EPOCH: 1939 ------------------------\n",
            "Epoch [1939/2000], Step [2/8], train_loss: 321.6793\n",
            "Epoch [1939/2000], Step [4/8], train_loss: 635.3759\n",
            "Epoch [1939/2000], Step [6/8], train_loss: 171.4678\n",
            "Epoch [1939/2000], Step [8/8], train_loss: 283.6158\n",
            "Val Loss:  353.75924428304035\n",
            "------------------------------EPOCH: 1940 ------------------------\n",
            "Epoch [1940/2000], Step [2/8], train_loss: 326.5109\n",
            "Epoch [1940/2000], Step [4/8], train_loss: 621.8732\n",
            "Epoch [1940/2000], Step [6/8], train_loss: 171.2858\n",
            "Epoch [1940/2000], Step [8/8], train_loss: 283.2646\n",
            "Val Loss:  541.3563831647238\n",
            "------------------------------EPOCH: 1941 ------------------------\n",
            "Epoch [1941/2000], Step [2/8], train_loss: 322.8519\n",
            "Epoch [1941/2000], Step [4/8], train_loss: 639.1909\n",
            "Epoch [1941/2000], Step [6/8], train_loss: 171.3178\n",
            "Epoch [1941/2000], Step [8/8], train_loss: 283.5473\n",
            "Val Loss:  318.5273328622182\n",
            "------------------------------EPOCH: 1942 ------------------------\n",
            "Epoch [1942/2000], Step [2/8], train_loss: 324.8010\n",
            "Epoch [1942/2000], Step [4/8], train_loss: 622.5844\n",
            "Epoch [1942/2000], Step [6/8], train_loss: 173.4732\n",
            "Epoch [1942/2000], Step [8/8], train_loss: 283.1552\n",
            "Val Loss:  483.16561794281006\n",
            "------------------------------EPOCH: 1943 ------------------------\n",
            "Epoch [1943/2000], Step [2/8], train_loss: 320.9492\n",
            "Epoch [1943/2000], Step [4/8], train_loss: 632.2795\n",
            "Epoch [1943/2000], Step [6/8], train_loss: 173.1719\n",
            "Epoch [1943/2000], Step [8/8], train_loss: 283.4344\n",
            "Val Loss:  229.14539432525635\n",
            "------------------------------EPOCH: 1944 ------------------------\n",
            "Epoch [1944/2000], Step [2/8], train_loss: 324.2138\n",
            "Epoch [1944/2000], Step [4/8], train_loss: 627.3368\n",
            "Epoch [1944/2000], Step [6/8], train_loss: 174.7770\n",
            "Epoch [1944/2000], Step [8/8], train_loss: 283.0513\n",
            "Val Loss:  337.8273213704427\n",
            "------------------------------EPOCH: 1945 ------------------------\n",
            "Epoch [1945/2000], Step [2/8], train_loss: 320.9878\n",
            "Epoch [1945/2000], Step [4/8], train_loss: 635.4313\n",
            "Epoch [1945/2000], Step [6/8], train_loss: 171.8083\n",
            "Epoch [1945/2000], Step [8/8], train_loss: 283.4026\n",
            "Val Loss:  175.0577755769094\n",
            "------------------------------EPOCH: 1946 ------------------------\n",
            "Epoch [1946/2000], Step [2/8], train_loss: 325.2412\n",
            "Epoch [1946/2000], Step [4/8], train_loss: 624.3512\n",
            "Epoch [1946/2000], Step [6/8], train_loss: 170.9590\n",
            "Epoch [1946/2000], Step [8/8], train_loss: 283.0689\n",
            "Val Loss:  246.24313608805338\n",
            "------------------------------EPOCH: 1947 ------------------------\n",
            "Epoch [1947/2000], Step [2/8], train_loss: 322.2642\n",
            "Epoch [1947/2000], Step [4/8], train_loss: 637.1841\n",
            "Epoch [1947/2000], Step [6/8], train_loss: 171.2467\n",
            "Epoch [1947/2000], Step [8/8], train_loss: 283.3231\n",
            "Val Loss:  125.76793781916301\n",
            "------------------------------EPOCH: 1948 ------------------------\n",
            "Epoch [1948/2000], Step [2/8], train_loss: 324.8695\n",
            "Epoch [1948/2000], Step [4/8], train_loss: 621.6915\n",
            "Epoch [1948/2000], Step [6/8], train_loss: 173.4139\n",
            "Epoch [1948/2000], Step [8/8], train_loss: 282.9682\n",
            "Val Loss:  250.86196835835776\n",
            "------------------------------EPOCH: 1949 ------------------------\n",
            "Epoch [1949/2000], Step [2/8], train_loss: 321.0992\n",
            "Epoch [1949/2000], Step [4/8], train_loss: 632.0657\n",
            "Epoch [1949/2000], Step [6/8], train_loss: 172.8224\n",
            "Epoch [1949/2000], Step [8/8], train_loss: 283.2195\n",
            "Val Loss:  155.78153244654337\n",
            "------------------------------EPOCH: 1950 ------------------------\n",
            "Epoch [1950/2000], Step [2/8], train_loss: 323.4016\n",
            "Epoch [1950/2000], Step [4/8], train_loss: 626.3935\n",
            "Epoch [1950/2000], Step [6/8], train_loss: 174.0697\n",
            "Epoch [1950/2000], Step [8/8], train_loss: 282.9215\n",
            "Val Loss:  294.31205828984577\n",
            "------------------------------EPOCH: 1951 ------------------------\n",
            "Epoch [1951/2000], Step [2/8], train_loss: 321.1921\n",
            "Epoch [1951/2000], Step [4/8], train_loss: 633.0828\n",
            "Epoch [1951/2000], Step [6/8], train_loss: 171.5285\n",
            "Epoch [1951/2000], Step [8/8], train_loss: 283.2915\n",
            "Val Loss:  143.92012675603232\n",
            "------------------------------EPOCH: 1952 ------------------------\n",
            "Epoch [1952/2000], Step [2/8], train_loss: 324.9789\n",
            "Epoch [1952/2000], Step [4/8], train_loss: 623.1636\n",
            "Epoch [1952/2000], Step [6/8], train_loss: 171.1286\n",
            "Epoch [1952/2000], Step [8/8], train_loss: 282.9626\n",
            "Val Loss:  267.89572223027545\n",
            "------------------------------EPOCH: 1953 ------------------------\n",
            "Epoch [1953/2000], Step [2/8], train_loss: 321.4751\n",
            "Epoch [1953/2000], Step [4/8], train_loss: 636.8727\n",
            "Epoch [1953/2000], Step [6/8], train_loss: 171.4858\n",
            "Epoch [1953/2000], Step [8/8], train_loss: 283.1734\n",
            "Val Loss:  149.34071493148804\n",
            "------------------------------EPOCH: 1954 ------------------------\n",
            "Epoch [1954/2000], Step [2/8], train_loss: 323.5509\n",
            "Epoch [1954/2000], Step [4/8], train_loss: 623.5836\n",
            "Epoch [1954/2000], Step [6/8], train_loss: 173.4519\n",
            "Epoch [1954/2000], Step [8/8], train_loss: 282.8933\n",
            "Val Loss:  192.42945957183838\n",
            "------------------------------EPOCH: 1955 ------------------------\n",
            "Epoch [1955/2000], Step [2/8], train_loss: 320.5760\n",
            "Epoch [1955/2000], Step [4/8], train_loss: 630.0823\n",
            "Epoch [1955/2000], Step [6/8], train_loss: 172.6914\n",
            "Epoch [1955/2000], Step [8/8], train_loss: 283.1552\n",
            "Val Loss:  182.37646532058716\n",
            "------------------------------EPOCH: 1956 ------------------------\n",
            "Epoch [1956/2000], Step [2/8], train_loss: 322.9471\n",
            "Epoch [1956/2000], Step [4/8], train_loss: 627.3004\n",
            "Epoch [1956/2000], Step [6/8], train_loss: 174.3327\n",
            "Epoch [1956/2000], Step [8/8], train_loss: 282.7916\n",
            "Val Loss:  443.5945626894633\n",
            "------------------------------EPOCH: 1957 ------------------------\n",
            "Epoch [1957/2000], Step [2/8], train_loss: 320.2637\n",
            "Epoch [1957/2000], Step [4/8], train_loss: 633.5724\n",
            "Epoch [1957/2000], Step [6/8], train_loss: 171.7131\n",
            "Epoch [1957/2000], Step [8/8], train_loss: 283.1833\n",
            "Val Loss:  261.91360092163086\n",
            "------------------------------EPOCH: 1958 ------------------------\n",
            "Epoch [1958/2000], Step [2/8], train_loss: 324.0486\n",
            "Epoch [1958/2000], Step [4/8], train_loss: 623.7314\n",
            "Epoch [1958/2000], Step [6/8], train_loss: 171.3509\n",
            "Epoch [1958/2000], Step [8/8], train_loss: 282.8434\n",
            "Val Loss:  376.84663041432697\n",
            "------------------------------EPOCH: 1959 ------------------------\n",
            "Epoch [1959/2000], Step [2/8], train_loss: 321.0449\n",
            "Epoch [1959/2000], Step [4/8], train_loss: 637.1049\n",
            "Epoch [1959/2000], Step [6/8], train_loss: 171.3399\n",
            "Epoch [1959/2000], Step [8/8], train_loss: 283.0785\n",
            "Val Loss:  209.2347449461619\n",
            "------------------------------EPOCH: 1960 ------------------------\n",
            "Epoch [1960/2000], Step [2/8], train_loss: 323.4021\n",
            "Epoch [1960/2000], Step [4/8], train_loss: 623.1119\n",
            "Epoch [1960/2000], Step [6/8], train_loss: 173.2426\n",
            "Epoch [1960/2000], Step [8/8], train_loss: 282.7586\n",
            "Val Loss:  315.6077914237976\n",
            "------------------------------EPOCH: 1961 ------------------------\n",
            "Epoch [1961/2000], Step [2/8], train_loss: 319.9687\n",
            "Epoch [1961/2000], Step [4/8], train_loss: 630.3622\n",
            "Epoch [1961/2000], Step [6/8], train_loss: 172.3964\n",
            "Epoch [1961/2000], Step [8/8], train_loss: 283.0468\n",
            "Val Loss:  210.8457888762156\n",
            "------------------------------EPOCH: 1962 ------------------------\n",
            "Epoch [1962/2000], Step [2/8], train_loss: 322.6069\n",
            "Epoch [1962/2000], Step [4/8], train_loss: 627.0136\n",
            "Epoch [1962/2000], Step [6/8], train_loss: 174.0332\n",
            "Epoch [1962/2000], Step [8/8], train_loss: 282.6667\n",
            "Val Loss:  560.1114803949991\n",
            "------------------------------EPOCH: 1963 ------------------------\n",
            "Epoch [1963/2000], Step [2/8], train_loss: 319.7852\n",
            "Epoch [1963/2000], Step [4/8], train_loss: 633.9588\n",
            "Epoch [1963/2000], Step [6/8], train_loss: 171.5781\n",
            "Epoch [1963/2000], Step [8/8], train_loss: 283.0544\n",
            "Val Loss:  361.1092058817546\n",
            "------------------------------EPOCH: 1964 ------------------------\n",
            "Epoch [1964/2000], Step [2/8], train_loss: 323.6860\n",
            "Epoch [1964/2000], Step [4/8], train_loss: 622.5356\n",
            "Epoch [1964/2000], Step [6/8], train_loss: 171.0271\n",
            "Epoch [1964/2000], Step [8/8], train_loss: 282.7598\n",
            "Val Loss:  602.9070920944214\n",
            "------------------------------EPOCH: 1965 ------------------------\n",
            "Epoch [1965/2000], Step [2/8], train_loss: 320.9319\n",
            "Epoch [1965/2000], Step [4/8], train_loss: 636.9082\n",
            "Epoch [1965/2000], Step [6/8], train_loss: 170.9419\n",
            "Epoch [1965/2000], Step [8/8], train_loss: 282.9876\n",
            "Val Loss:  368.7639678319295\n",
            "------------------------------EPOCH: 1966 ------------------------\n",
            "Epoch [1966/2000], Step [2/8], train_loss: 322.9463\n",
            "Epoch [1966/2000], Step [4/8], train_loss: 621.8870\n",
            "Epoch [1966/2000], Step [6/8], train_loss: 173.0540\n",
            "Epoch [1966/2000], Step [8/8], train_loss: 282.5831\n",
            "Val Loss:  436.4483011563619\n",
            "------------------------------EPOCH: 1967 ------------------------\n",
            "Epoch [1967/2000], Step [2/8], train_loss: 319.4374\n",
            "Epoch [1967/2000], Step [4/8], train_loss: 631.6406\n",
            "Epoch [1967/2000], Step [6/8], train_loss: 172.4068\n",
            "Epoch [1967/2000], Step [8/8], train_loss: 282.9248\n",
            "Val Loss:  203.8869558175405\n",
            "------------------------------EPOCH: 1968 ------------------------\n",
            "Epoch [1968/2000], Step [2/8], train_loss: 322.6287\n",
            "Epoch [1968/2000], Step [4/8], train_loss: 625.6314\n",
            "Epoch [1968/2000], Step [6/8], train_loss: 173.8688\n",
            "Epoch [1968/2000], Step [8/8], train_loss: 282.5296\n",
            "Val Loss:  469.8853688240051\n",
            "------------------------------EPOCH: 1969 ------------------------\n",
            "Epoch [1969/2000], Step [2/8], train_loss: 319.4899\n",
            "Epoch [1969/2000], Step [4/8], train_loss: 635.4288\n",
            "Epoch [1969/2000], Step [6/8], train_loss: 171.1813\n",
            "Epoch [1969/2000], Step [8/8], train_loss: 282.8961\n",
            "Val Loss:  225.6535498301188\n",
            "------------------------------EPOCH: 1970 ------------------------\n",
            "Epoch [1970/2000], Step [2/8], train_loss: 323.2924\n",
            "Epoch [1970/2000], Step [4/8], train_loss: 623.0651\n",
            "Epoch [1970/2000], Step [6/8], train_loss: 170.5978\n",
            "Epoch [1970/2000], Step [8/8], train_loss: 282.6054\n",
            "Val Loss:  280.52041323979694\n",
            "------------------------------EPOCH: 1971 ------------------------\n",
            "Epoch [1971/2000], Step [2/8], train_loss: 320.3083\n",
            "Epoch [1971/2000], Step [4/8], train_loss: 636.1282\n",
            "Epoch [1971/2000], Step [6/8], train_loss: 170.9987\n",
            "Epoch [1971/2000], Step [8/8], train_loss: 282.8068\n",
            "Val Loss:  167.35661252339682\n",
            "------------------------------EPOCH: 1972 ------------------------\n",
            "Epoch [1972/2000], Step [2/8], train_loss: 322.5322\n",
            "Epoch [1972/2000], Step [4/8], train_loss: 623.1165\n",
            "Epoch [1972/2000], Step [6/8], train_loss: 173.0072\n",
            "Epoch [1972/2000], Step [8/8], train_loss: 282.4637\n",
            "Val Loss:  385.03342231114704\n",
            "------------------------------EPOCH: 1973 ------------------------\n",
            "Epoch [1973/2000], Step [2/8], train_loss: 319.0213\n",
            "Epoch [1973/2000], Step [4/8], train_loss: 630.8516\n",
            "Epoch [1973/2000], Step [6/8], train_loss: 172.3446\n",
            "Epoch [1973/2000], Step [8/8], train_loss: 282.7515\n",
            "Val Loss:  285.4137706756592\n",
            "------------------------------EPOCH: 1974 ------------------------\n",
            "Epoch [1974/2000], Step [2/8], train_loss: 321.2767\n",
            "Epoch [1974/2000], Step [4/8], train_loss: 625.4927\n",
            "Epoch [1974/2000], Step [6/8], train_loss: 173.7375\n",
            "Epoch [1974/2000], Step [8/8], train_loss: 282.4857\n",
            "Val Loss:  473.94940153757733\n",
            "------------------------------EPOCH: 1975 ------------------------\n",
            "Epoch [1975/2000], Step [2/8], train_loss: 319.1058\n",
            "Epoch [1975/2000], Step [4/8], train_loss: 633.4831\n",
            "Epoch [1975/2000], Step [6/8], train_loss: 171.4277\n",
            "Epoch [1975/2000], Step [8/8], train_loss: 282.7909\n",
            "Val Loss:  190.4583087762197\n",
            "------------------------------EPOCH: 1976 ------------------------\n",
            "Epoch [1976/2000], Step [2/8], train_loss: 322.3456\n",
            "Epoch [1976/2000], Step [4/8], train_loss: 623.2664\n",
            "Epoch [1976/2000], Step [6/8], train_loss: 171.1319\n",
            "Epoch [1976/2000], Step [8/8], train_loss: 282.4852\n",
            "Val Loss:  174.17759656906128\n",
            "------------------------------EPOCH: 1977 ------------------------\n",
            "Epoch [1977/2000], Step [2/8], train_loss: 319.4832\n",
            "Epoch [1977/2000], Step [4/8], train_loss: 635.0676\n",
            "Epoch [1977/2000], Step [6/8], train_loss: 171.2292\n",
            "Epoch [1977/2000], Step [8/8], train_loss: 282.7543\n",
            "Val Loss:  124.3636302947998\n",
            "------------------------------EPOCH: 1978 ------------------------\n",
            "Epoch [1978/2000], Step [2/8], train_loss: 321.7967\n",
            "Epoch [1978/2000], Step [4/8], train_loss: 622.4800\n",
            "Epoch [1978/2000], Step [6/8], train_loss: 173.1025\n",
            "Epoch [1978/2000], Step [8/8], train_loss: 282.4254\n",
            "Val Loss:  263.30915168921155\n",
            "------------------------------EPOCH: 1979 ------------------------\n",
            "Epoch [1979/2000], Step [2/8], train_loss: 318.7722\n",
            "Epoch [1979/2000], Step [4/8], train_loss: 629.2351\n",
            "Epoch [1979/2000], Step [6/8], train_loss: 172.3571\n",
            "Epoch [1979/2000], Step [8/8], train_loss: 282.6994\n",
            "Val Loss:  176.03051706155142\n",
            "------------------------------EPOCH: 1980 ------------------------\n",
            "Epoch [1980/2000], Step [2/8], train_loss: 321.1892\n",
            "Epoch [1980/2000], Step [4/8], train_loss: 626.9535\n",
            "Epoch [1980/2000], Step [6/8], train_loss: 173.8890\n",
            "Epoch [1980/2000], Step [8/8], train_loss: 282.3535\n",
            "Val Loss:  465.4891169865926\n",
            "------------------------------EPOCH: 1981 ------------------------\n",
            "Epoch [1981/2000], Step [2/8], train_loss: 318.6179\n",
            "Epoch [1981/2000], Step [4/8], train_loss: 633.7283\n",
            "Epoch [1981/2000], Step [6/8], train_loss: 171.0304\n",
            "Epoch [1981/2000], Step [8/8], train_loss: 282.7097\n",
            "Val Loss:  252.52293952306113\n",
            "------------------------------EPOCH: 1982 ------------------------\n",
            "Epoch [1982/2000], Step [2/8], train_loss: 322.6205\n",
            "Epoch [1982/2000], Step [4/8], train_loss: 622.5042\n",
            "Epoch [1982/2000], Step [6/8], train_loss: 170.3602\n",
            "Epoch [1982/2000], Step [8/8], train_loss: 282.3894\n",
            "Val Loss:  437.10672839482623\n",
            "------------------------------EPOCH: 1983 ------------------------\n",
            "Epoch [1983/2000], Step [2/8], train_loss: 319.2684\n",
            "Epoch [1983/2000], Step [4/8], train_loss: 635.7297\n",
            "Epoch [1983/2000], Step [6/8], train_loss: 170.9580\n",
            "Epoch [1983/2000], Step [8/8], train_loss: 282.6548\n",
            "Val Loss:  262.5395901997884\n",
            "------------------------------EPOCH: 1984 ------------------------\n",
            "Epoch [1984/2000], Step [2/8], train_loss: 321.3455\n",
            "Epoch [1984/2000], Step [4/8], train_loss: 621.3610\n",
            "Epoch [1984/2000], Step [6/8], train_loss: 173.2843\n",
            "Epoch [1984/2000], Step [8/8], train_loss: 282.3534\n",
            "Val Loss:  506.6409862836202\n",
            "------------------------------EPOCH: 1985 ------------------------\n",
            "Epoch [1985/2000], Step [2/8], train_loss: 318.1545\n",
            "Epoch [1985/2000], Step [4/8], train_loss: 630.0591\n",
            "Epoch [1985/2000], Step [6/8], train_loss: 172.5664\n",
            "Epoch [1985/2000], Step [8/8], train_loss: 282.6223\n",
            "Val Loss:  263.02333068847656\n",
            "------------------------------EPOCH: 1986 ------------------------\n",
            "Epoch [1986/2000], Step [2/8], train_loss: 320.6320\n",
            "Epoch [1986/2000], Step [4/8], train_loss: 625.9113\n",
            "Epoch [1986/2000], Step [6/8], train_loss: 173.9489\n",
            "Epoch [1986/2000], Step [8/8], train_loss: 282.2253\n",
            "Val Loss:  528.0161794026693\n",
            "------------------------------EPOCH: 1987 ------------------------\n",
            "Epoch [1987/2000], Step [2/8], train_loss: 318.0823\n",
            "Epoch [1987/2000], Step [4/8], train_loss: 633.9258\n",
            "Epoch [1987/2000], Step [6/8], train_loss: 170.9961\n",
            "Epoch [1987/2000], Step [8/8], train_loss: 282.6251\n",
            "Val Loss:  288.5712973276774\n",
            "------------------------------EPOCH: 1988 ------------------------\n",
            "Epoch [1988/2000], Step [2/8], train_loss: 322.0917\n",
            "Epoch [1988/2000], Step [4/8], train_loss: 622.6281\n",
            "Epoch [1988/2000], Step [6/8], train_loss: 170.3490\n",
            "Epoch [1988/2000], Step [8/8], train_loss: 282.2882\n",
            "Val Loss:  327.9507745107015\n",
            "------------------------------EPOCH: 1989 ------------------------\n",
            "Epoch [1989/2000], Step [2/8], train_loss: 318.5708\n",
            "Epoch [1989/2000], Step [4/8], train_loss: 636.1492\n",
            "Epoch [1989/2000], Step [6/8], train_loss: 170.8851\n",
            "Epoch [1989/2000], Step [8/8], train_loss: 282.5308\n",
            "Val Loss:  177.35739080111185\n",
            "------------------------------EPOCH: 1990 ------------------------\n",
            "Epoch [1990/2000], Step [2/8], train_loss: 320.7473\n",
            "Epoch [1990/2000], Step [4/8], train_loss: 622.8134\n",
            "Epoch [1990/2000], Step [6/8], train_loss: 173.1327\n",
            "Epoch [1990/2000], Step [8/8], train_loss: 282.2347\n",
            "Val Loss:  298.5404601097107\n",
            "------------------------------EPOCH: 1991 ------------------------\n",
            "Epoch [1991/2000], Step [2/8], train_loss: 317.6120\n",
            "Epoch [1991/2000], Step [4/8], train_loss: 629.0928\n",
            "Epoch [1991/2000], Step [6/8], train_loss: 172.4177\n",
            "Epoch [1991/2000], Step [8/8], train_loss: 282.5086\n",
            "Val Loss:  187.36525774002075\n",
            "------------------------------EPOCH: 1992 ------------------------\n",
            "Epoch [1992/2000], Step [2/8], train_loss: 320.0027\n",
            "Epoch [1992/2000], Step [4/8], train_loss: 626.0814\n",
            "Epoch [1992/2000], Step [6/8], train_loss: 173.9584\n",
            "Epoch [1992/2000], Step [8/8], train_loss: 282.1223\n",
            "Val Loss:  341.9033303260803\n",
            "------------------------------EPOCH: 1993 ------------------------\n",
            "Epoch [1993/2000], Step [2/8], train_loss: 317.0307\n",
            "Epoch [1993/2000], Step [4/8], train_loss: 632.6742\n",
            "Epoch [1993/2000], Step [6/8], train_loss: 171.7504\n",
            "Epoch [1993/2000], Step [8/8], train_loss: 282.5207\n",
            "Val Loss:  198.53107364972433\n",
            "------------------------------EPOCH: 1994 ------------------------\n",
            "Epoch [1994/2000], Step [2/8], train_loss: 321.0439\n",
            "Epoch [1994/2000], Step [4/8], train_loss: 620.7460\n",
            "Epoch [1994/2000], Step [6/8], train_loss: 171.5991\n",
            "Epoch [1994/2000], Step [8/8], train_loss: 282.2631\n",
            "Val Loss:  489.4235583941142\n",
            "------------------------------EPOCH: 1995 ------------------------\n",
            "Epoch [1995/2000], Step [2/8], train_loss: 318.7024\n",
            "Epoch [1995/2000], Step [4/8], train_loss: 634.1798\n",
            "Epoch [1995/2000], Step [6/8], train_loss: 170.7350\n",
            "Epoch [1995/2000], Step [8/8], train_loss: 282.5463\n",
            "Val Loss:  278.75117778778076\n",
            "------------------------------EPOCH: 1996 ------------------------\n",
            "Epoch [1996/2000], Step [2/8], train_loss: 321.3275\n",
            "Epoch [1996/2000], Step [4/8], train_loss: 622.3514\n",
            "Epoch [1996/2000], Step [6/8], train_loss: 172.0966\n",
            "Epoch [1996/2000], Step [8/8], train_loss: 282.2538\n",
            "Val Loss:  553.5445334116617\n",
            "------------------------------EPOCH: 1997 ------------------------\n",
            "Epoch [1997/2000], Step [2/8], train_loss: 318.2849\n",
            "Epoch [1997/2000], Step [4/8], train_loss: 629.0679\n",
            "Epoch [1997/2000], Step [6/8], train_loss: 171.4195\n",
            "Epoch [1997/2000], Step [8/8], train_loss: 282.5125\n",
            "Val Loss:  303.2243715922038\n",
            "------------------------------EPOCH: 1998 ------------------------\n",
            "Epoch [1998/2000], Step [2/8], train_loss: 319.9914\n",
            "Epoch [1998/2000], Step [4/8], train_loss: 628.8422\n",
            "Epoch [1998/2000], Step [6/8], train_loss: 172.3581\n",
            "Epoch [1998/2000], Step [8/8], train_loss: 282.0125\n",
            "Val Loss:  623.9758828481039\n",
            "------------------------------EPOCH: 1999 ------------------------\n",
            "Epoch [1999/2000], Step [2/8], train_loss: 316.5161\n",
            "Epoch [1999/2000], Step [4/8], train_loss: 634.2046\n",
            "Epoch [1999/2000], Step [6/8], train_loss: 171.4801\n",
            "Epoch [1999/2000], Step [8/8], train_loss: 282.3810\n",
            "Val Loss:  270.6414802869161\n",
            "------------------------------EPOCH: 2000 ------------------------\n",
            "Epoch [2000/2000], Step [2/8], train_loss: 320.7444\n",
            "Epoch [2000/2000], Step [4/8], train_loss: 618.7189\n",
            "Epoch [2000/2000], Step [6/8], train_loss: 172.1263\n",
            "Epoch [2000/2000], Step [8/8], train_loss: 282.1023\n",
            "Val Loss:  467.5515247980754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-24T15:42:36.816462Z",
          "start_time": "2021-11-24T15:42:36.714647Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "large-position",
        "outputId": "ce72e8fa-f3ad-478f-94d8-28076abaa293"
      },
      "source": [
        "plot_logs(train_log, val_log)"
      ],
      "id": "large-position",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p66lqCHLCd4C",
        "outputId": "74dca7c2-c989-4460-c87c-57e2781457b2"
      },
      "source": [
        "## Comparison with function\n",
        "mse = 0\n",
        "for _, (ages, weights) in enumerate(valloader):\n",
        "    y = 233.846*(1 - torch.exp(-1*0.006042*ages)) + 0.0001\n",
        "    mse += nn.functional.mse_loss(y, weights).item()/len(valloader)\n",
        "\n",
        "print(mse)"
      ],
      "id": "p66lqCHLCd4C",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105.65359600385031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYJzg5HWHcZW"
      },
      "source": [
        ""
      ],
      "id": "PYJzg5HWHcZW",
      "execution_count": null,
      "outputs": []
    }
  ]
}